diff --git a/third_party/tsl/third_party/eigen3/eigen.patch b/third_party/tsl/third_party/eigen3/eigen.patch
new file mode 100644
index 0000000000..8954b1d2e3
--- /dev/null
+++ b/third_party/tsl/third_party/eigen3/eigen.patch
@@ -0,0 +1,13 @@
+diff --git a/Eigen/src/Core/util/Macros.h b/Eigen/src/Core/util/Macros.h
+index 69bbf2e73..251053b55 100644
+--- a/Eigen/src/Core/util/Macros.h
++++ b/Eigen/src/Core/util/Macros.h
+@@ -686,7 +686,7 @@
+ // For instance, if compiling with gcc and -std=c++17, then EIGEN_COMP_CXXVER
+ // is defined to 17.
+ #if EIGEN_CPLUSPLUS >= 202002L
+-  #define EIGEN_COMP_CXXVER 20
++  #define EIGEN_COMP_CXXVER 17
+ #elif EIGEN_CPLUSPLUS >= 201703L
+   #define EIGEN_COMP_CXXVER 17
+ #elif EIGEN_CPLUSPLUS >= 201402L
diff --git a/third_party/tsl/third_party/eigen3/workspace.bzl b/third_party/tsl/third_party/eigen3/workspace.bzl
index d1d8d4ac48..be6848888f 100644
--- a/third_party/tsl/third_party/eigen3/workspace.bzl
+++ b/third_party/tsl/third_party/eigen3/workspace.bzl
@@ -14,6 +14,7 @@ def repo():
     tf_http_archive(
         name = "eigen_archive",
         build_file = "//third_party/eigen3:eigen_archive.BUILD",
+        patch_file = ["//third_party/eigen3:eigen.patch"],
         sha256 = EIGEN_SHA256,
         strip_prefix = "eigen-{commit}".format(commit = EIGEN_COMMIT),
         urls = tf_mirror_urls("https://gitlab.com/libeigen/eigen/-/archive/{commit}/eigen-{commit}.tar.gz".format(commit = EIGEN_COMMIT)),
diff --git a/third_party/tsl/third_party/grpc/upb_platform_fix.patch b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
index 6edd66067e..022c9d1557 100644
--- a/third_party/tsl/third_party/grpc/upb_platform_fix.patch
+++ b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
@@ -11,3 +11,12 @@ index ad85b202..2311b2e4 100644
  )
 
  config_setting(
+@@ -24,7 +24,7 @@ exports_files([
+
+ CPPOPTS = [
+     # copybara:strip_for_google3_begin
+-    "-Werror",
++    # "-Werror",
+     "-Wno-long-long",
+     # copybara:strip_end
+ ]
diff --git a/third_party/tsl/third_party/llvm/build.patch b/third_party/tsl/third_party/llvm/build.patch
index bbf8f587ac..ab0df933f5 100644
--- a/third_party/tsl/third_party/llvm/build.patch
+++ b/third_party/tsl/third_party/llvm/build.patch
@@ -2,6 +2,14 @@ diff --git a/utils/bazel/llvm-project-overlay/llvm/BUILD.bazel b/utils/bazel/llv
 index 2b88729d748b..e12d979b4908 100644
 --- a/utils/bazel/llvm-project-overlay/llvm/BUILD.bazel
 +++ b/utils/bazel/llvm-project-overlay/llvm/BUILD.bazel
+@@ -25,6 +25,7 @@ exports_files(["LICENSE.TXT"])
+ # widely available feature to enable unlimited stack frame instead of using
+ # this `Make` variable.
+ llvm_copts = [
++    "-fvisibility=hidden",
+     "$(STACK_FRAME_UNLIMITED)",
+ ]
+
 @@ -207,13 +207,15 @@ cc_library(
          "lib/Support/BLAKE3/llvm_blake3_prefix.h",
      ] + select({
@@ -44,3 +52,24 @@ index 2b88729d748b..e12d979b4908 100644
          "//conditions:default": [
              "BLAKE3_NO_AVX2",
              "BLAKE3_NO_AVX512",
+diff --git a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+index 177372c68046..40d49dc13b2f 100644
+--- a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
++++ b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+@@ -1549,6 +1549,8 @@ private:
+   const std::shared_ptr<llvm::SourceMgr> &bufferOwnerRef;
+ };
+
++// TODO: Disable clang opt since it crashes with clang-17 compiler.
++#pragma clang optimize off
+ LogicalResult BytecodeReader::Impl::read(
+     Block *block, llvm::function_ref<bool(Operation *)> lazyOpsCallback) {
+   EncodingReader reader(buffer.getBuffer(), fileLoc);
+@@ -1628,6 +1630,7 @@ LogicalResult BytecodeReader::Impl::read(
+   // Finally, process the IR section.
+   return parseIRSection(*sectionDatas[bytecode::Section::kIR], block);
+ }
++#pragma clang optimize on
+
+ LogicalResult BytecodeReader::Impl::parseVersion(EncodingReader &reader) {
+   if (failed(reader.parseVarInt(version)))
diff --git a/third_party/tsl/third_party/llvm/opt.patch b/third_party/tsl/third_party/llvm/opt.patch
new file mode 100644
index 0000000000..49e5b3b7fc
--- /dev/null
+++ b/third_party/tsl/third_party/llvm/opt.patch
@@ -0,0 +1,18 @@
+diff --git a/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp b/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp
+index d87f2fb59814..d2411cb33ecf 100644
+--- a/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp
++++ b/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp
+@@ -2019,9 +2019,10 @@ PreservedAnalyses MemCpyOptPass::run(Function &F, FunctionAnalysisManager &AM) {
+   auto *PDT = &AM.getResult<PostDominatorTreeAnalysis>(F);
+   auto *MSSA = &AM.getResult<MemorySSAAnalysis>(F);
+ 
+-  bool MadeChange = runImpl(F, &TLI, AA, AC, DT, PDT, &MSSA->getMSSA());
+-  if (!MadeChange)
+-    return PreservedAnalyses::all();
++  // FIXME: Disable MemCpyOptPass to avoid llvm.memcpy issue
++ // bool MadeChange = runImpl(F, &TLI, AA, AC, DT, PDT, &MSSA->getMSSA());
++ // if (!MadeChange)
++ //   return PreservedAnalyses::all();
+ 
+   PreservedAnalyses PA;
+   PA.preserveSet<CFGAnalyses>();
diff --git a/third_party/tsl/third_party/llvm/spirv.patch b/third_party/tsl/third_party/llvm/spirv.patch
new file mode 100644
index 0000000000..dd2ebb8e1e
--- /dev/null
+++ b/third_party/tsl/third_party/llvm/spirv.patch
@@ -0,0 +1,93 @@
+diff --git a/llvm/include/llvm/Analysis/TargetTransformInfoImpl.h b/llvm/include/llvm/Analysis/TargetTransformInfoImpl.h
+index c1ff314ae51c..a9d8b955629d 100644
+--- a/llvm/include/llvm/Analysis/TargetTransformInfoImpl.h
++++ b/llvm/include/llvm/Analysis/TargetTransformInfoImpl.h
+@@ -461,7 +461,11 @@ public:
+     return ElementCount::get(0, IsScalable);
+   }
+ 
+-  unsigned getMaximumVF(unsigned ElemWidth, unsigned Opcode) const { return 0; }
++  unsigned getMaximumVF(unsigned ElemWidth, unsigned Opcode) const {
++    if (Opcode == Instruction::Load || Opcode == Instruction::Store)
++        return 32 * 4 / ElemWidth;
++    return (ElemWidth == 16) ? 2 : 1;
++  }
+   unsigned getStoreMinimumVF(unsigned VF, Type *, Type *) const { return VF; }
+ 
+   bool shouldConsiderAddressTypePromotion(
+diff --git a/llvm/lib/Passes/PassBuilderPipelines.cpp b/llvm/lib/Passes/PassBuilderPipelines.cpp
+index 78e0e6353056..4f9f51164bfe 100644
+--- a/llvm/lib/Passes/PassBuilderPipelines.cpp
++++ b/llvm/lib/Passes/PassBuilderPipelines.cpp
+@@ -183,6 +183,10 @@ static cl::opt<bool> EnableGlobalAnalyses(
+     "enable-global-analyses", cl::init(true), cl::Hidden,
+     cl::desc("Enable inter-procedural analyses"));
+ 
++static cl::opt<bool>
++    SYCLOptimizationMode("sycl-opt", cl::init(false), cl::Hidden,
++                         cl::desc("Enable SYCL optimization mode."));
++
+ static cl::opt<bool>
+     RunPartialInlining("enable-partial-inlining", cl::init(false), cl::Hidden,
+                        cl::desc("Run Partial inlinining pass"));
+@@ -406,6 +410,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   // Add the primary loop simplification pipeline.
+@@ -477,7 +482,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -580,6 +585,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   if (EnableConstraintElimination)
+@@ -657,7 +663,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -715,6 +721,9 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+ 
+   invokeScalarOptimizerLateEPCallbacks(FPM, Level);
+ 
++  if (SYCLOptimizationMode)
++    FPM.addPass(SimplifyCFGPass());
++  else
+   FPM.addPass(SimplifyCFGPass(SimplifyCFGOptions()
+                                   .convertSwitchRangeToICmp(true)
+                                   .hoistCommonInsts(true)
+@@ -1385,6 +1394,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+ 
+   invokeVectorizerStartEPCallbacks(OptimizePM, Level);
+ 
++  if (!SYCLOptimizationMode) {
+   LoopPassManager LPM;
+   // First rotate loops that may have been un-rotated by prior passes.
+   // Disable header duplication at -Oz.
+@@ -1408,7 +1418,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+   OptimizePM.addPass(InjectTLIMappings());
+ 
+   addVectorPasses(Level, OptimizePM, /* IsFullLTO */ false);
+-
++  }
+   // LoopSink pass sinks instructions hoisted by LICM, which serves as a
+   // canonicalization pass that enables other optimizations. As a result,
+   // LoopSink pass needs to be a very late IR pass to avoid undoing LICM
diff --git a/third_party/tsl/third_party/llvm/workspace.bzl b/third_party/tsl/third_party/llvm/workspace.bzl
index 30e3e13fdf..131556f93d 100644
--- a/third_party/tsl/third_party/llvm/workspace.bzl
+++ b/third_party/tsl/third_party/llvm/workspace.bzl
@@ -17,6 +17,8 @@ def repo(name):
         ],
         build_file = "//third_party/llvm:llvm.BUILD",
         patch_file = [
+            "//third_party/llvm:spirv.patch",
+            "//third_party/llvm:opt.patch",
             "//third_party/llvm:generated.patch",  # Autogenerated, don't remove.
             "//third_party/llvm:build.patch",
             "//third_party/llvm:mathextras.patch",
diff --git a/third_party/tsl/tsl/framework/BUILD b/third_party/tsl/tsl/framework/BUILD
index e055a619f5..fa5e93af61 100644
--- a/third_party/tsl/tsl/framework/BUILD
+++ b/third_party/tsl/tsl/framework/BUILD
@@ -117,7 +117,8 @@ cc_library(
         "@com_google_absl//absl/types:optional",
     ] + if_static(
         extra_deps = [
-            ":allocator_registry_impl",
+            # Remove this to avoid duplicating compile cpu allocator part.
+            # ":allocator_registry_impl",
             "//tsl/lib/gtl:inlined_vector",
             "//tsl/platform:strcat",
             "//tsl/platform:stringprintf",
@@ -208,6 +209,7 @@ cc_library(
         "//tsl/profiler/lib:scoped_memory_debug_annotation",
         "//tsl/profiler/lib:traceme",
         "//tsl/protobuf:bfc_memory_map_proto_cc",
+        "@tsl//tsl/util:env_var",
         "@com_google_absl//absl/container:flat_hash_set",
         "@com_google_absl//absl/strings",
         "@com_google_absl//absl/types:optional",
diff --git a/third_party/tsl/tsl/framework/bfc_allocator.cc b/third_party/tsl/tsl/framework/bfc_allocator.cc
index 9e4447108a..18e5356be4 100644
--- a/third_party/tsl/tsl/framework/bfc_allocator.cc
+++ b/third_party/tsl/tsl/framework/bfc_allocator.cc
@@ -47,12 +47,13 @@ BFCAllocator::BFCAllocator(std::unique_ptr<SubAllocator> sub_allocator,
       sub_allocator_(std::move(sub_allocator)),
       name_(name),
       free_chunks_list_(kInvalidChunkHandle),
-      next_allocation_id_(1) {
-  if (opts.allow_growth) {
+      next_allocation_id_(1),
+      alloc_mode(AllocMode()) {
+  if (opts.allow_growth || alloc_mode == BFC_EXTEND_SMALL) {
     // 2MiB smallest initial allocation, unless total memory available
     // is less.
     curr_region_allocation_bytes_ =
-        RoundedBytes(std::min(total_memory, size_t{2 << 20}));
+        RoundedBytes(std::min(total_memory, kSmallSize));
   } else {
     curr_region_allocation_bytes_ = RoundedBytes(total_memory);
   }
@@ -111,6 +112,15 @@ const BFCAllocator::Chunk* BFCAllocator::ChunkFromHandle(ChunkHandle h) const {
   return &(chunks_[h]);
 }
 
+bool CheckIfUsingMachineWithLimitMem() {
+  const char* hardware_env = std::getenv("ITEX_USING_DATA_CENTER_GPU_MAX");
+  if (hardware_env) {
+    std::string env_value = absl::AsciiStrToLower(hardware_env);
+    return !(env_value == "1" || env_value == "true");
+  }
+  return true;
+}
+
 bool BFCAllocator::Extend(size_t alignment, size_t rounded_bytes) {
   size_t available_bytes = memory_limit_ - *stats_.pool_bytes;
   // Rounds available_bytes down to the nearest multiple of kMinAllocationSize.
@@ -122,17 +132,32 @@ bool BFCAllocator::Extend(size_t alignment, size_t rounded_bytes) {
     return false;
   }
 
-  // If curr_region_allocation_bytes_ is not enough to satisfy the
-  // allocation, keep multiplying by a power of two until that is
-  // sufficient.
   bool increased_allocation = false;
-  while (rounded_bytes > curr_region_allocation_bytes_) {
-    curr_region_allocation_bytes_ *= 2;
-    increased_allocation = true;
+  if (alloc_mode == BFC_EXTEND_LARGE) {
+    // If curr_region_allocation_bytes_ is not enough to satisfy the
+    // allocation, keep multiplying by a power of two until that is
+    // sufficient.
+    while (rounded_bytes > curr_region_allocation_bytes_) {
+      curr_region_allocation_bytes_ *= 2;
+      increased_allocation = true;
+    }
+  } else {
+      // Requested bytes              --- Allocated bytes
+      // (0, kSmallSize]              --- kSmallBuffer
+      // (kSmallSize, kMinLargeAlloc) --- kLargeBuffer
+      // [kMinLargeAlloc, max]        --- round up to multiple of kRoundLarge
+      curr_region_allocation_bytes_ =
+          (rounded_bytes <= kSmallSize)
+              ? kSmallBuffer
+              : ((rounded_bytes < kMinLargeAlloc)
+                     ? kLargeBuffer
+                     : (kRoundLarge *
+                        ((rounded_bytes + kRoundLarge - 1) / kRoundLarge)));
   }
-
   // Try allocating.
   size_t bytes = std::min(curr_region_allocation_bytes_, available_bytes);
+  bytes = std::min(bytes, GetLimitAlloc());
+
   size_t bytes_received;
   void* mem_addr = sub_allocator_->Alloc(alignment, bytes, &bytes_received);
   if (mem_addr == nullptr && !started_backpedal_) {
@@ -153,7 +178,7 @@ bool BFCAllocator::Extend(size_t alignment, size_t rounded_bytes) {
     return false;
   }
 
-  if (!increased_allocation) {
+  if ((alloc_mode == BFC_EXTEND_LARGE) && !increased_allocation) {
     // Increase the region size of the next required allocation.
     curr_region_allocation_bytes_ *= 2;
   }
@@ -1248,4 +1273,36 @@ AllocatorMemoryType BFCAllocator::GetMemoryType() const {
   return sub_allocator_->GetMemoryType();
 }
 
+int64 AllocModeFromEnv() {
+  int64 alloc_mode_env = 1;
+  Status status = ReadInt64FromEnvVar("ITEX_ALLOC_MODE", 1, &alloc_mode_env);
+  if (!status.ok()) {
+    LOG(ERROR) << "Failed to read environment variable ITEX_ALLOC_MODE!"; 
+  }
+
+  return alloc_mode_env;
+}
+
+int64 BFCAllocator::AllocMode() {
+  static int64 alloc_mode = AllocModeFromEnv();
+  return alloc_mode;
+}
+
+// This function set the upper bound of memory allocation size, the
+// actual allocation size is the minimal value of this limit size
+// and the size want to get from system.
+size_t BFCAllocator::GetLimitAlloc() {
+  // set default limit to 4GB
+  int64 limit_size = 4 * 1024 - 1;
+  if (!CheckIfUsingMachineWithLimitMem()) {
+    limit_size = memory_limit_ / 1024 / 1024 * 0.75;
+  }
+  Status status = ReadInt64FromEnvVar("ITEX_LIMIT_MEMORY_SIZE_IN_MB",
+                                        limit_size, &limit_size);
+  if (!status.ok()) {
+    LOG(ERROR) << "Failed to read environment variable ITEX_LIMIT_MEMORY_SIZE_IN_MB!"; 
+  }
+  
+  return limit_size * 1024 * 1024;
+}
 }  // namespace tsl
diff --git a/third_party/tsl/tsl/framework/bfc_allocator.h b/third_party/tsl/tsl/framework/bfc_allocator.h
index 47619856ab..bd31fa55ab 100644
--- a/third_party/tsl/tsl/framework/bfc_allocator.h
+++ b/third_party/tsl/tsl/framework/bfc_allocator.h
@@ -33,7 +33,10 @@ limitations under the License.
 #include "tsl/platform/strcat.h"
 #include "tsl/platform/thread_annotations.h"
 #include "tsl/platform/types.h"
+#include "tsl/util/env_var.h"
 
+#define BFC_EXTEND_LARGE 1
+#define BFC_EXTEND_SMALL 2
 namespace tensorflow {
 class MemoryDump;
 }
@@ -152,6 +155,15 @@ class BFCAllocator : public Allocator {
                   int64_t req_bytes, int64_t alloc_bytes)
       TF_EXCLUSIVE_LOCKS_REQUIRED(lock_);
 
+  int64 AllocMode();
+  int64 alloc_mode;
+  size_t GetLimitAlloc();
+  static constexpr size_t kSmallSize = 1048576;
+  static constexpr size_t kSmallBuffer = 2097152;
+  static constexpr size_t kLargeBuffer = 20971520;
+  static constexpr size_t kMinLargeAlloc = 10485760;
+  static constexpr size_t kRoundLarge = 2097152;
+  
   // A ChunkHandle is an index into the chunks_ vector in BFCAllocator
   // kInvalidChunkHandle means an invalid chunk
   typedef size_t ChunkHandle;
diff --git a/third_party/tsl/tsl/framework/contraction/BUILD b/third_party/tsl/tsl/framework/contraction/BUILD
index 220b751f4c..63f82cca50 100644
--- a/third_party/tsl/tsl/framework/contraction/BUILD
+++ b/third_party/tsl/tsl/framework/contraction/BUILD
@@ -121,7 +121,7 @@ cc_library(
         "//tsl:linux_ppc64le": [],
         "//tsl:linux_s390x": [],
         "//tsl:macos_arm64": [],
-        "//conditions:default": ["@onednn//:mkl_dnn"],
+        "//conditions:default": [],
     }),
 )
 
diff --git a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops.td b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops.td
index 3f4c95ed02..a070f3c222 100644
--- a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops.td
+++ b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops.td
@@ -322,6 +322,19 @@ def LHLOGPU_AllToAllDoneOp: LHLOGPU_Op<"all_to_all_done"> {
   let arguments = (ins MHLO_Token:$token);
 }
 
+
+def LHLOGPU_fusedQKVOp : LHLOGPU_Op<"fQKV"> {
+  let arguments = (ins
+    Arg<LHLO_Buffer, "", [MemRead]>:$input,
+    Arg<LHLO_Buffer, "", [MemRead]>:$weight,
+    Arg<LHLO_Buffer, "", [MemWrite]>:$bmm_output1,
+    Arg<LHLO_Buffer, "", [MemWrite]>:$bmm_output2,
+    Arg<LHLO_Buffer, "", [MemWrite]>:$bmm_output3,
+    MHLO_DotDimensionNumbers:$dot_dimension_numbers);
+}
+
+
+
 def LHLOGPU_fusedMHAOp : LHLOGPU_Op<"fMHA", [AttrSizedOperandSegments]> {
   let arguments = (ins
     Arg<LHLO_Buffer, "", [MemRead]>:$lhs_bmm1,
diff --git a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td
index d1c7c7ac54..601bcd6313 100644
--- a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td
+++ b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td
@@ -137,6 +137,7 @@ def FusedMhaDagSoftmaxDropout : I32EnumAttrCase<"SoftmaxDropout", 5>;
 def FusedMhaDagSoftmax : I32EnumAttrCase<"Softmax", 6>;
 def FusedMhaDagScaleBiasSoftmaxDropout : I32EnumAttrCase<"ScaleBiasSoftmaxDropout", 7>;
 def FusedMhaDagScaleBiasSoftmax : I32EnumAttrCase<"ScaleBiasSoftmax", 8>;
+def FusedMhaDagScaleSoftmax : I32EnumAttrCase<"ScaleSoftmax", 9>;
 
 def FusedMhaBackwardDagScaleBiasSoftmaxDropout : I32EnumAttrCase<"BackwardScaleBiasSoftmaxDropout", 0>;
 def FusedMhaBackwardDagScaleBiasSoftmax : I32EnumAttrCase<"BackwardScaleBiasSoftmax", 1>;
@@ -153,7 +154,8 @@ def FusedMhaDagSignature: I32EnumAttr<"FusedMhaDagSignature",
     FusedMhaDagSoftmaxDropout,
     FusedMhaDagSoftmax,
     FusedMhaDagScaleBiasSoftmaxDropout,
-    FusedMhaDagScaleBiasSoftmax]> {
+    FusedMhaDagScaleBiasSoftmax,
+    FusedMhaDagScaleSoftmax]> {
   let genSpecializedAttr = 0;
   let cppNamespace = "::mlir::lmhlo_gpu";
 }
diff --git a/xla/pjrt/event_pool.cc b/xla/pjrt/event_pool.cc
index 4a07a6ee75..b7f264c01b 100644
--- a/xla/pjrt/event_pool.cc
+++ b/xla/pjrt/event_pool.cc
@@ -53,8 +53,11 @@ StatusOr<EventPool::Handle> EventPool::AllocateEvent(
 }
 
 void EventPool::ThenRecordEvent(se::Stream* stream, EventPool::Handle& handle) {
-  absl::MutexLock lock(&mu_);
+  // We should not lock event pool mutex before submitting a sycl barrier to a
+  // stream, otherwise it may lead to a dead lock if there is a host task
+  // requiring this mutex in the same stream.
   stream->ThenRecordEvent(handle.event_.get());
+  absl::MutexLock lock(&mu_);
   handle.sequence_number_ = next_sequence_number_++;
 }
 
diff --git a/xla/pjrt/gpu/BUILD b/xla/pjrt/gpu/BUILD
index fd18d73bf5..93a5b61c4b 100644
--- a/xla/pjrt/gpu/BUILD
+++ b/xla/pjrt/gpu/BUILD
@@ -89,7 +89,7 @@ cc_library(
         "@tsl//tsl/platform:fingerprint",
         "@tsl//tsl/profiler/lib:connected_traceme",
         "@tsl//tsl/util:env_var",
-    ] + if_cuda_or_rocm([
+    ] + if_cuda([
         "//xla/service/gpu:gpu_compiler",
     ]) + if_cuda([
         ":nccl_id_store_cuda",
@@ -239,16 +239,16 @@ cc_library(
         "@tsl//tsl/platform:casts",
         "@tsl//tsl/platform:errors",
     ] + if_cuda([
+        "//xla/service/gpu:gpu_compiler",
+    ]) + if_cuda([
         ":nccl_id_store_cuda",
         "@local_config_cuda//cuda:cuda_headers",
         "//xla/stream_executor/cuda:cuda_activation_header",
         "//xla/stream_executor/gpu:gpu_cudamallocasync_allocator",
-        "//xla/service/gpu:gpu_compiler",
         "//xla/service/gpu:nvptx_compiler_impl",
     ]) + if_rocm([
         ":nccl_id_store_rocm",
         "@local_config_rocm//rocm:rocm_headers",
-        "//xla/service/gpu:gpu_compiler",
         "//xla/service/gpu:amdgpu_compiler_impl",
     ]),
     alwayslink = True,
diff --git a/xla/pjrt/gpu/gpu_helpers.cc b/xla/pjrt/gpu/gpu_helpers.cc
index b7064ac25e..dcb1efbd8b 100644
--- a/xla/pjrt/gpu/gpu_helpers.cc
+++ b/xla/pjrt/gpu/gpu_helpers.cc
@@ -37,11 +37,12 @@ namespace xla {
 StatusOr<LocalClient*> GetGpuXlaClient(
     const std::optional<std::string>& platform_name,
     const std::optional<std::set<int>>& allowed_devices) {
+  // SYCL: hardcode to xpu
   TF_ASSIGN_OR_RETURN(
       se::Platform * platform,
-      PlatformUtil::GetPlatform(platform_name ? *platform_name : "gpu"));
+      PlatformUtil::GetPlatform(platform_name ? *platform_name : "xpu"));
   if (platform->VisibleDeviceCount() <= 0) {
-    return FailedPrecondition("No visible GPU devices.");
+    return FailedPrecondition("No visible XPU devices.");
   }
   LocalClientOptions options;
   options.set_platform(platform);
@@ -113,7 +114,8 @@ StatusOr<std::unique_ptr<tsl::BFCAllocator>> CreateBFCAllocator(
   opts.allow_growth = !preallocate;
   return std::make_unique<tsl::BFCAllocator>(
       std::move(sub_allocator), allocator_memory,
-      absl::StrCat("GPU_", device_ordinal, "_bfc"), opts);
+      // SYCL: hardcode to xpu
+      absl::StrCat("XPU_", device_ordinal, "_bfc"), opts);
 }
 
 // Returns a GPU pinned host memory allocator to use when staging host->GPU
@@ -131,7 +133,8 @@ std::unique_ptr<tsl::BFCAllocator> GetGpuHostAllocator(
   opts.allow_growth = true;
   return std::make_unique<tsl::BFCAllocator>(std::move(sub_allocator),
                                              kGpuHostMemoryLimitBytes,
-                                             /*name=*/"xla_gpu_host_bfc", opts);
+                                             // SYCL: hardcode to xpu
+                                             /*name=*/"xla_xpu_host_bfc", opts);
 }
 
 }  // namespace xla
diff --git a/xla/pjrt/local_device_state.cc b/xla/pjrt/local_device_state.cc
index 69443f3263..6161ec8c74 100644
--- a/xla/pjrt/local_device_state.cc
+++ b/xla/pjrt/local_device_state.cc
@@ -146,6 +146,7 @@ Status LocalDeviceState::ThenMemcpyDeviceToDevice(
 void LocalDeviceState::ThenExecuteCallback(se::Stream* stream,
                                            std::function<void()> callback) {
   tsl::profiler::TraceMe traceme("ThenExecuteCallback");
+  se::Stream* other_stream = stream;
   if (callback_stream_map_.has_value()) {
     // Prevent concurrent updates to the callback stream map.
     absl::MutexLock lock(&mu_);
@@ -156,10 +157,14 @@ void LocalDeviceState::ThenExecuteCallback(se::Stream* stream,
       callback_stream =
           callback_stream_map_->insert({stream, std::move(new_stream)}).first;
     }
-    callback_stream->second->ThenWaitFor(stream);
-    stream = callback_stream->second.get();
+    other_stream = callback_stream->second.get();
   }
-  stream->ThenDoHostCallback([this, callback{std::move(callback)}]() mutable {
+  // We should release mutex before submit barrier or host task to callback
+  // stream, otherwise will lead to dead lock.
+  if (other_stream != stream) {
+    other_stream->ThenWaitFor(stream);
+  }
+  other_stream->ThenDoHostCallback([this, callback{std::move(callback)}]() mutable {
     callback_thread_->Schedule(std::move(callback));
   });
 }
diff --git a/xla/pjrt/pjrt_stream_executor_client.h b/xla/pjrt/pjrt_stream_executor_client.h
index b91269c3e3..151ab678d7 100644
--- a/xla/pjrt/pjrt_stream_executor_client.h
+++ b/xla/pjrt/pjrt_stream_executor_client.h
@@ -874,6 +874,10 @@ class PjRtStreamExecutorLoadedExecutable : public PjRtLoadedExecutable {
     return executables_;
   }
 
+  StatusOr<CompileOptions> GetCompileOptions() const override {
+    return compile_options_;
+  }
+
  protected:
   bool parameter_is_tupled_arguments() const {
     return parameter_is_tupled_arguments_;
diff --git a/xla/service/algebraic_simplifier.h b/xla/service/algebraic_simplifier.h
index 2320744973..8b304b6e2d 100644
--- a/xla/service/algebraic_simplifier.h
+++ b/xla/service/algebraic_simplifier.h
@@ -27,6 +27,7 @@ limitations under the License.
 #include <vector>
 
 #include "absl/container/inlined_vector.h"
+#include "tsl/util/env_var.h"
 #include "xla/hlo/ir/dfs_hlo_visitor_with_default.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_module.h"
@@ -432,7 +433,9 @@ class AlgebraicSimplifierVisitor : public DfsHloRewriteVisitor {
   virtual bool IsValidLayout(const Shape& shape) { return true; }
   // Allow backend targets to determine whether a layout is inefficient.
   virtual bool ShouldStrengthReduceDotToReduce(const HloInstruction* hlo) {
-    return true;
+    bool llm_flag = false;
+    tsl::ReadBoolFromEnvVar("LLM", false, &llm_flag);
+    return !llm_flag;
   }
 
  protected:
diff --git a/xla/service/dump.cc b/xla/service/dump.cc
index 65a09e9fcb..d3d90d242a 100644
--- a/xla/service/dump.cc
+++ b/xla/service/dump.cc
@@ -620,6 +620,8 @@ void DumpToFileInDirOrStdout(const HloModule& module, string_view file_prefix,
   if (opts.dumping_to_stdout()) return op->dump();
 
   mlir::OpPrintingFlags print_flags = mlir::OpPrintingFlags().useLocalScope();
+  // Avoid printing large constant weight.
+  print_flags.elideLargeElementsAttrs(8);
   // Enable debug info so that it is easier to see the corresponding HLO node.
   if (file_prefix == "lmhlo") {
     print_flags.enableDebugInfo(/*enable=*/true,
diff --git a/xla/service/float_normalization.cc b/xla/service/float_normalization.cc
index 267a92383a..dd966b883c 100644
--- a/xla/service/float_normalization.cc
+++ b/xla/service/float_normalization.cc
@@ -206,17 +206,21 @@ Status FloatNormalizationVisitor::ChangeOutputTypeThenInsertConvertBack(
                 HloInstruction::CreateConvert(original_subshape, leaf));
           }));
 
+  std::vector<HloInstruction*> simplify_conversions;
   for (auto* user : materialized_users) {
     // If the user is a low-precision -> high-precision convert, we can replace
     // it with `hlo`, which has its input changed to high-precision.
     if (user->opcode() == HloOpcode::kConvert &&
         user->shape().element_type() == to && to == HighPrecisionType() &&
         from == LowPrecisionType()) {
-      TF_RETURN_IF_ERROR(user->ReplaceAllUsesWith(hlo));
+      simplify_conversions.emplace_back(user);
     } else {
       TF_RETURN_IF_ERROR(hlo->ReplaceUseWithDifferentShape(user, new_hlo));
     }
   }
+  for (auto* convert : simplify_conversions) {
+    TF_RETURN_IF_ERROR(convert->ReplaceAllUsesWith(hlo));
+  }
   if (is_root) {
     computation->set_root_instruction(new_hlo, /*accept_different_shape=*/true);
   }
diff --git a/xla/service/float_normalization_test.cc b/xla/service/float_normalization_test.cc
index 3a41960bad..eab7cdcbc4 100644
--- a/xla/service/float_normalization_test.cc
+++ b/xla/service/float_normalization_test.cc
@@ -485,4 +485,33 @@ TEST_F(FloatNormalizationTest, ResolveIfUnsupportedF8e5m2) {
   EXPECT_EQ(mul1->operand(0)->opcode(), HloOpcode::kConvert);
 }
 
+TEST_F(FloatNormalizationTest, ConvertAfterOutput) {
+  auto builder = HloComputation::Builder(TestName());
+  Shape bf16_shape = ShapeUtil::MakeShape(BF16, {2, 4});
+  Shape f32_shape = ShapeUtil::MakeShape(F32, {2, 4});
+
+  HloInstruction* a = builder.AddInstruction(
+      HloInstruction::CreateParameter(0, bf16_shape, "a"));
+  HloInstruction* b =
+      builder.AddInstruction(HloInstruction::CreateParameter(1, bf16_shape, "b"));
+
+  HloInstruction* add = builder.AddInstruction(
+      HloInstruction::CreateBinary(bf16_shape, HloOpcode::kMultiply, a, b));
+
+  HloInstruction* convert = builder.AddInstruction(
+      HloInstruction::CreateConvert(f32_shape, add));
+
+  HloInstruction* tuple = builder.AddInstruction(
+      HloInstruction::CreateVariadic(ShapeUtil::MakeTupleShape({f32_shape, bf16_shape}), HloOpcode::kTuple, {convert, add}));
+
+  auto module = CreateNewVerifiedModule();
+  auto computation = module->AddEntryComputation(builder.Build());
+
+  EXPECT_TRUE(Normalize(module.get(), BF16));
+
+  EXPECT_EQ(computation->root_instruction()->opcode(), HloOpcode::kTuple);
+  EXPECT_EQ(computation->root_instruction()->operand(0)->shape().element_type(), F32);
+  EXPECT_EQ(computation->root_instruction()->shape().tuple_shapes(0).element_type(), F32);
+}
+
 }  // namespace xla
diff --git a/xla/service/gpu/cublas_cudnn.cc b/xla/service/gpu/cublas_cudnn.cc
index 2c6340c6c2..7b1fa7a64b 100644
--- a/xla/service/gpu/cublas_cudnn.cc
+++ b/xla/service/gpu/cublas_cudnn.cc
@@ -41,6 +41,7 @@ bool IsCublasLtMatmulF8(const HloInstruction& hlo) {
          hlo.custom_call_target() == kCublasLtMatmulF8CallTarget;
 }
 
+const absl::string_view kCudnnfQKVCallTarget = "__cudnn$fQKV";
 const absl::string_view kGemmCallTarget = "__cublas$gemm";
 const absl::string_view kCublasLtMatmulCallTarget = "__cublas$lt$matmul";
 const absl::string_view kCublasLtMatmulF8CallTarget = "__cublas$lt$matmul$f8";
@@ -69,6 +70,8 @@ const absl::string_view kCudnnfMHAScaleBiasMaskSoftmaxDropoutCallTarget =
     "__cudnn$fhmaScaleBiasMaskSoftmaxDropout";
 const absl::string_view kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget =
     "__cudnn$fhmaScaleBiasSoftmaxDropout";
+const absl::string_view kCudnnfMHAScaleSoftmaxCallTarget =
+    "__cudnn$fmhaScaleSoftmax";
 const absl::string_view kCudnnfMHAScaleBiasSoftmaxCallTarget =
     "__cudnn$fhmaScaleBiasSoftmax";
 const absl::string_view kCudnnfMHAScaleMaskSoftmaxCallTarget =
@@ -131,6 +134,7 @@ bool IsFwdCustomCallTofMHA(const HloInstruction& hlo) {
          target == kCudnnfMHAScaleBiasMaskSoftmaxDropoutCallTarget ||
          target == kCudnnfMHAScaleMaskSoftmaxCallTarget ||
          target == kCudnnfMHAScaleMaskSoftmaxDropoutCallTarget ||
+         target == kCudnnfMHAScaleSoftmaxCallTarget ||target == kCudnnfMHAScaleSoftmaxCallTarget ||
          target == kCudnnfMHASoftmaxDropoutCallTarget ||
          target == kCudnnfMHAScaleBiasSoftmaxCallTarget ||
          target == kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget;
@@ -163,6 +167,11 @@ bool MHACallHasDropout(const absl::string_view fmha_call_name) {
          fmha_call_name == kCudnnfMHAScaleMaskSoftmaxDropoutBackwardCallTarget;
 }
 
+bool IsCustomCallTofQKV(const HloInstruction& hlo) {
+  const auto& target = hlo.custom_call_target();
+  return target == kCudnnfQKVCallTarget;
+}
+
 bool IsCustomCallTofMHA(const HloInstruction& hlo) {
   return (IsFwdCustomCallTofMHA(hlo) || IsBwdCustomCallTofMHA(hlo));
 }
@@ -215,6 +224,8 @@ StatusOr<CudnnfMHAKind> GetCudnnfMHAKind(
     return CudnnfMHAKind::kScaleMaskSoftmax;
   if (target == kCudnnfMHAScaleMaskSoftmaxDropoutCallTarget)
     return CudnnfMHAKind::kScaleMaskSoftmaxDropout;
+  if (target == kCudnnfMHAScaleSoftmaxCallTarget)
+    return CudnnfMHAKind::kScaleSoftmax;
   if (target == kCudnnfMHASoftmaxDropoutCallTarget)
     return CudnnfMHAKind::kSoftmaxDropout;
   if (target == kCudnnfMHASoftmaxCallTarget) return CudnnfMHAKind::kSoftmax;
@@ -264,6 +275,8 @@ std::string CudnnfMHAKindToString(CudnnfMHAKind kind) {
       return "fmha_bias_softmax_with_dropout";
     case CudnnfMHAKind::kScaleBiasSoftmax:
       return "fmha_bias_softmax";
+    case CudnnfMHAKind::kScaleSoftmax:
+       return "fmha_scale_softmax";
     // backward
     case CudnnfMHAKind::kBackwardBmmBmm:
       return "fused_batched_matmuls_backward";
diff --git a/xla/service/gpu/cublas_cudnn.h b/xla/service/gpu/cublas_cudnn.h
index 4feaf28bb1..1b532b74ad 100644
--- a/xla/service/gpu/cublas_cudnn.h
+++ b/xla/service/gpu/cublas_cudnn.h
@@ -56,6 +56,7 @@ enum class CudnnfMHAKind {
   kScaleMaskSoftmaxDropout,
   kSoftmaxDropout,
   kSoftmax,
+  kScaleSoftmax,
   kScaleBiasSoftmax,
   kScaleBiasSoftmaxDropout,
   kBackwardBmmBmm,
@@ -166,6 +167,7 @@ bool IsCudnnConvolutionReorder(const HloInstruction& hlo);
 // 7. BMM1 - Softmax - BMM2
 // 8. BMM1 - scale - Bias - Softmax - BMM2
 // Forward calls
+extern const absl::string_view kCudnnfQKVCallTarget;
 extern const absl::string_view kCudnnfMHABmmBmmCallTarget;
 extern const absl::string_view kCudnnfMHASoftmaxCallTarget;
 extern const absl::string_view kCudnnfMHAScaleBiasMaskSoftmaxCallTarget;
@@ -175,6 +177,7 @@ extern const absl::string_view kCudnnfMHAScaleMaskSoftmaxDropoutCallTarget;
 extern const absl::string_view kCudnnfMHASoftmaxDropoutCallTarget;
 extern const absl::string_view kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget;
 extern const absl::string_view kCudnnfMHAScaleBiasSoftmaxCallTarget;
+extern const absl::string_view kCudnnfMHAScaleSoftmaxCallTarget;
 // Backward calls
 extern const absl::string_view kCudnnfMHABmmBmmBackwardCallTarget;
 extern const absl::string_view kCudnnfMHASoftmaxBackwardCallTarget;
@@ -192,6 +195,8 @@ extern const absl::string_view kCudnnfMHAScaleBiasSoftmaxBackwardCallTarget;
 bool IsFwdCustomCallTofMHA(const HloInstruction& hlo);
 bool IsBwdCustomCallTofMHA(const HloInstruction& hlo);
 bool IsCustomCallTofMHA(const HloInstruction& hlo);
+bool IsCustomCallTofQKV(const HloInstruction& hlo);
+
 
 StatusOr<CudnnfMHAKind> GetCudnnfMHAKind(const HloCustomCallInstruction* instr);
 
diff --git a/xla/service/gpu/gpu_conv_rewriter.cc b/xla/service/gpu/gpu_conv_rewriter.cc
index 838de80674..0bed9501c7 100644
--- a/xla/service/gpu/gpu_conv_rewriter.cc
+++ b/xla/service/gpu/gpu_conv_rewriter.cc
@@ -1,4 +1,4 @@
-/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+/* Copyright 2018 The OpenXLA Authors.
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
@@ -43,6 +43,30 @@ namespace {
 using ConvolutionMatch = std::optional<
     std::tuple<Window, ConvolutionDimensionNumbers, HloInstruction*>>;
 
+// Determine whether conv2d is equal to conv1d.
+bool MaybeConv1dToConv2d(HloInstruction* conv) {
+  if (conv->window().dimensions().size() != 2) {
+    return false;
+  }
+  if (conv->operand(1)->opcode() != HloOpcode::kReshape) {
+    return false;
+  }
+  auto filter = conv->operand(1);
+  std::optional<ShapeUtil::ShapeEqualityDescriptor> reshape_degenerate =
+      filter->ReshapeMerelyInsertsOrDeletes1SizedDimensions();
+  if (reshape_degenerate.has_value() &&
+      reshape_degenerate->deleted_dimensions.empty() &&
+      reshape_degenerate->inserted_dimensions.size() == 1) {
+    auto dnums = conv->convolution_dimension_numbers();
+    for (auto dim : dnums.kernel_spatial_dimensions()) {
+      if (dim == reshape_degenerate->inserted_dimensions[0]) {
+        return true;
+      }
+    }
+  }
+  return false;
+}
+
 bool CanImplementAsGpuForwardConv(HloInstruction* conv) {
   const ConvolutionDimensionNumbers& dnums =
       conv->convolution_dimension_numbers();
@@ -145,10 +169,18 @@ ConvolutionMatch MatchBackwardFilter(HloInstruction* conv) {
   // convolutions have very small kernel dimensions, while in the backward pass
   // "kernel dimensions" are large. If kernel dimensions are smaller than the
   // output dimensions, return foward conv; otherwise proceed with backward
-  // filter conv.
-  if ((kernel_spatial_dims.empty() ||
-       conv->operand(1)->shape().dimensions(kernel_spatial_dims[0]) <=
-           conv->shape().dimensions(output_spatial_dims[0])) &&
+  // filter conv. But for conv1d, it is not same. Due to conv1d always reshape
+  // 1D-filter to 2D-filter, even backward or forward will exist one small
+  // kernel dimension. We should handle this special case.
+  int small_kernel_dimension_num = 0;
+  for (int i = 0; i < kernel_spatial_dims.size(); ++i) {
+    if (conv->operand(1)->shape().dimensions(kernel_spatial_dims[i]) <=
+        conv->shape().dimensions(output_spatial_dims[i])) {
+      small_kernel_dimension_num += 1;
+    }
+  }
+  if ((kernel_spatial_dims.empty() || small_kernel_dimension_num > 1 ||
+       (!MaybeConv1dToConv2d(conv) && small_kernel_dimension_num == 1)) &&
       !window_util::HasWindowDilation(conv->window())) {
     VLOG(1) << conv->ToString()
             << " is a regular forward convolution. No need "
@@ -309,10 +341,18 @@ ConvolutionMatch MatchBackwardInput(HloInstruction* conv) {
       reverse_filter->opcode() == HloOpcode::kReverse &&
       absl::c_is_permutation(dnums.kernel_spatial_dimensions(),
                              reverse_filter->dimensions());
+  // For conv1d which reshape to conv2d, filter reverse pattern is
+  // reshape(reverse(filter)). It seems we can reuse conv2d backward input
+  // pattern matcher, but after algsimp pass, this pattern will change to
+  // reverse(reshape(filter)) and fail to match. So matching conv1d backward
+  // input need different processing logic.
+  bool is_reversed_conv1d_filter =
+      MaybeConv1dToConv2d(conv) &&
+      reverse_filter->operand(0)->opcode() == HloOpcode::kReverse;
   bool is_1x1_filter =
       absl::c_all_of(conv->window().dimensions(),
                      [](const WindowDimension& d) { return d.size() == 1; });
-  if (!is_reversed_filter &&
+  if (!is_reversed_filter && !is_reversed_conv1d_filter &&
       !(window_util::HasBaseDilation(conv->window()) &&
         (reverse_filter->IsConstant() || is_1x1_filter))) {
     VLOG(1) << "Can't match to backwards convolution. Either filter is not "
@@ -484,6 +524,10 @@ ConvolutionMatch MatchBackwardInput(HloInstruction* conv) {
   // One reverse is subsumed by the cuDNN call.
   if (rhs->opcode() == HloOpcode::kReverse) {
     rhs = rhs->mutable_operand(0);
+  } else if (is_reversed_conv1d_filter) {
+    auto src = rhs->mutable_operand(0)->mutable_operand(0);
+    rhs = conv->parent()->AddInstruction(
+        HloInstruction::CreateReshape(rhs->shape(), src));
   }
   if (conv->feature_group_count() == 1) {
     return std::make_tuple(new_window, dnums, rhs);
@@ -662,7 +706,8 @@ CudnnConvBackendConfig GetDefaultBackendConfig() {
 
 // Helper function to create a custom_call instruction to replace the given
 // conv instruction
-static StatusOr<HloInstruction*> CreateCustomCallHelper(HloInstruction* conv) {
+static absl::StatusOr<HloInstruction*> CreateCustomCallHelper(
+    HloInstruction* conv) {
   if (ConvolutionMatch m = MatchBackwardInput(conv)) {
     auto& [window, dnums, rhs] = *m;
     return CreateGpuConv(kCudnnConvBackwardInputCallTarget, conv->shape(),
@@ -696,7 +741,7 @@ static StatusOr<HloInstruction*> CreateCustomCallHelper(HloInstruction* conv) {
 }
 
 // Tries to rewrite a single convolution into a call to cudnn/miopen.
-StatusOr<bool> RunOnInstruction(HloInstruction* conv) {
+absl::StatusOr<bool> RunOnInstruction(HloInstruction* conv) {
   CHECK_EQ(conv->opcode(), HloOpcode::kConvolution);
 
   TF_ASSIGN_OR_RETURN(HloInstruction * custom_call,
@@ -722,7 +767,7 @@ StatusOr<bool> RunOnInstruction(HloInstruction* conv) {
 // Rewrites the convolutions in the given computation into calls to
 // cudnn/miopen.
 // Returns true if it made any changes.
-StatusOr<bool> RunOnComputation(HloComputation* computation) {
+absl::StatusOr<bool> RunOnComputation(HloComputation* computation) {
   std::vector<HloInstruction*> convs;
   for (auto* hlo : computation->instructions()) {
     if (hlo->opcode() == HloOpcode::kConvolution) {
@@ -739,7 +784,7 @@ StatusOr<bool> RunOnComputation(HloComputation* computation) {
 }
 }  // namespace
 
-StatusOr<bool> GpuConvRewriter::Run(
+absl::StatusOr<bool> GpuConvRewriter::Run(
     HloModule* module,
     const absl::flat_hash_set<absl::string_view>& execution_threads) {
   XLA_VLOG_LINES(2, "GpuConvRewriter::Run(), before:\n" + module->ToString());
diff --git a/xla/service/gpu/gpu_conv_rewriter_test.cc b/xla/service/gpu/gpu_conv_rewriter_test.cc
index 4f7a59b8c5..f161a3012d 100644
--- a/xla/service/gpu/gpu_conv_rewriter_test.cc
+++ b/xla/service/gpu/gpu_conv_rewriter_test.cc
@@ -1,4 +1,4 @@
-/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+/* Copyright 2018 The OpenXLA Authors.
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
@@ -642,7 +642,9 @@ TEST_F(GpuConvRewriterTest, BackwardInputConvolveConstantFilter) {
                   0)));
 }
 
-TEST_F(GpuConvRewriterTest, TestBackwardFilterPattern) {
+TEST_F(GpuConvRewriterTest, TestBackwardFilterPatternMatch) {
+  // All filter dimensions are larger than the corresponding output dimensions.
+  // This must be a backward filter convolution.
   const std::string module_str = absl::StrFormat(R"(
     HloModule Test
 
@@ -662,6 +664,74 @@ TEST_F(GpuConvRewriterTest, TestBackwardFilterPattern) {
                   0)));
 }
 
+TEST_F(GpuConvRewriterTest, TestBackwardFilterPatternNoMatch) {
+  // At least one filter dimension is smaller than the corresponding output
+  // dimension. This must be a forward convolution.
+  const std::string module_str = absl::StrFormat(R"(
+    HloModule Test
+
+    ENTRY Test {
+      input = f32[8,128,2,32] parameter(0)
+      filter = f32[3,3,128,128] parameter(1)
+
+      ROOT conv = f32[8,128,2,32] convolution(input, filter), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01
+    })");
+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));
+
+  EXPECT_TRUE(RunPass(m.get()));
+  EXPECT_THAT(m->entry_computation()->root_instruction(),
+              GmockMatch(m::GetTupleElement(
+                  m::CustomCall({kCudnnConvForwardCallTarget}, m::Parameter(0),
+                                m::Parameter(1)),
+                  0)));
+}
+
+TEST_F(GpuConvRewriterTest, TestConv1dBackwardFilterPatternMatch) {
+  // There exist one kernel dimension equal to output dimension, regard
+  // it as backward filter if conv is 1d.
+  const std::string module_str = absl::StrFormat(R"(
+    HloModule Test
+
+    ENTRY Test {
+      input = f32[8,256,128] parameter(0)
+      filter = f32[8,254,128] parameter(1)
+      reshape.1 = f32[8,1,256,128] reshape(input)
+      reshape.2 = f32[8,1,254,128] reshape(filter)
+      ROOT conv = f32[1,3,128,128] convolution(reshape.1, reshape.2), window={size=1x254}, dim_labels=f01b_i01o->01bf
+    })");
+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));
+
+  EXPECT_TRUE(RunPass(m.get()));
+  EXPECT_THAT(m->entry_computation()->root_instruction(),
+              GmockMatch(m::GetTupleElement(
+                  m::CustomCall({kCudnnConvBackwardFilterCallTarget},
+                                m::Reshape(), m::Reshape()),
+                  0)));
+}
+
+TEST_F(GpuConvRewriterTest, TestConv1dBackwardInputPatternMatch) {
+  // For conv1d backward input, filter may reverse first and then reshape.
+  const std::string module_str = absl::StrFormat(R"(
+    HloModule Test
+
+    ENTRY Test {
+      input = f32[8,254,128] parameter(0)
+      filter = f32[3,128,128] parameter(1)
+      reverse = f32[3,128,128] reverse(filter), dimensions={0}
+      reshape.1 = f32[8,1,254,128] reshape(input)
+      reshape.2 = f32[1,3,128,128] reshape(reverse)
+      ROOT conv = f32[8,1,256,128] convolution(reshape.1, reshape.2), window={size=1x3 pad=0_0x2_2}, dim_labels=b01f_01oi->b01f
+    })");
+  TF_ASSERT_OK_AND_ASSIGN(auto m, ParseAndReturnVerifiedModule(module_str));
+
+  EXPECT_TRUE(RunPass(m.get()));
+  EXPECT_THAT(m->entry_computation()->root_instruction(),
+              GmockMatch(m::GetTupleElement(
+                  m::CustomCall({kCudnnConvBackwardInputCallTarget},
+                                m::Reshape(), m::Reshape()),
+                  0)));
+}
+
 }  // anonymous namespace
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/gpu_layout_assignment.cc b/xla/service/gpu/gpu_layout_assignment.cc
index ad2d7ffa62..8b176910af 100644
--- a/xla/service/gpu/gpu_layout_assignment.cc
+++ b/xla/service/gpu/gpu_layout_assignment.cc
@@ -69,7 +69,7 @@ HeuristicLayoutAssignment(const HloInstruction* instr,
       std::make_tuple(DataLayout::kBatchDepthYX4, FilterLayout::kOutputInputYX4,
                       DataLayout::kBatchDepthYX4);
   constexpr auto kAllNHWC =
-      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kOutputYXInput,
+      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kYXInputOutput,
                       DataLayout::kBatchYXDepth);
 
   // Integer convolution must use NHWC or NCHW_VECT_C.
@@ -111,8 +111,7 @@ HeuristicLayoutAssignment(const HloInstruction* instr,
   // If we're not Volta or not fp16/bfloat16, or not conv2D, the decision is
   // easy: Use NCHW.
   const bool isFloat16 = (input_ty == F16) || (input_ty == BF16);
-  if (!isFloat16 ||
-      !stream_executor->GetDeviceDescription()
+  if (!stream_executor->GetDeviceDescription()
            .cuda_compute_capability()
            .IsAtLeast(se::CudaComputeCapability::VOLTA) ||
       instr->shape().tuple_shapes(0).dimensions_size() != 4) {
diff --git a/xla/service/gpu/hlo_fusion_analysis.cc b/xla/service/gpu/hlo_fusion_analysis.cc
index 36006855dd..b7d3222f48 100644
--- a/xla/service/gpu/hlo_fusion_analysis.cc
+++ b/xla/service/gpu/hlo_fusion_analysis.cc
@@ -761,13 +761,8 @@ int HloFusionAnalysis::CalculateVirtualThreadScalingFactorForReduction(
   int64_t dimx = reduction_dimensions.dimensions[kDimX];
   if (reduction_dimensions.is_row_reduction && dimx <= 128) {
     int rows_per_warp = RowReductionGetRowsPerWarp(dimx);
-    const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(
-        &device_info_->gpu_compute_capability());
-    if (cuda_cc != nullptr &&
-        cuda_cc->IsAtLeast(se::CudaComputeCapability::AMPERE)) {
-      return rows_per_warp * 3;
-    }
-    return rows_per_warp * 5;
+    // SYCL: larger thread number causes hang.
+    return rows_per_warp;
   }
   return 1;
 }
diff --git a/xla/service/gpu/ir_emission_utils.cc b/xla/service/gpu/ir_emission_utils.cc
index 12264198a0..1d6713b500 100644
--- a/xla/service/gpu/ir_emission_utils.cc
+++ b/xla/service/gpu/ir_emission_utils.cc
@@ -80,11 +80,11 @@ bool IsMatrixMultiplication(const HloInstruction& dot) {
   const DotDimensionNumbers& dim_numbers = dot.dot_dimension_numbers();
 
   PrimitiveType output_primitive_type = dot.shape().element_type();
+  // Disable F64, C64, C128
   bool type_is_allowed =
       (output_primitive_type == F8E4M3FN || output_primitive_type == F8E5M2 ||
        output_primitive_type == F16 || output_primitive_type == BF16 ||
-       output_primitive_type == F32 || output_primitive_type == F64 ||
-       output_primitive_type == C64 || output_primitive_type == C128) ||
+       output_primitive_type == F32) ||
       (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&
        rhs_shape.element_type() == S8);
   bool shapes_are_valid =
@@ -232,6 +232,31 @@ llvm::Value* EmitNVPTXShflDown(llvm::Value* value, llvm::Value* offset,
       intrinsic, {b->getInt32(-1), value, offset, b->getInt32(WarpSize() - 1)});
 }
 
+// Helper function to emit call to NVPTX shfl_down intrinsic.
+llvm::Value* EmitSPIRShflDown(llvm::Value* value, llvm::Value* offset,
+                              llvm::IRBuilder<>* b) {
+  llvm::Module* module = b->GetInsertBlock()->getModule();
+  llvm::Intrinsic::ID llvm_intrinsic_id;
+  CHECK_EQ(value->getType()->getPrimitiveSizeInBits(), 32);
+  if (value->getType()->isFloatTy()) {
+    return EmitDeviceFunctionCall(
+        "_Z32__spirv_SubgroupShuffleDownINTELffj",
+        {value, value, offset}, {F32, F32, U32}, F32,
+        llvm::AttrBuilder(b->getContext())
+            .addAttribute(llvm::Attribute::NoUnwind)
+            .addAttribute(llvm::Attribute::Convergent),
+        b);
+  } else {
+    return EmitDeviceFunctionCall(
+        "_Z32__spirv_SubgroupShuffleDownINTELjjj",
+        {value, value, offset}, {U32, U32, U32}, U32,
+        llvm::AttrBuilder(b->getContext())
+            .addAttribute(llvm::Attribute::NoUnwind)
+            .addAttribute(llvm::Attribute::Convergent),
+        b);
+  }
+}
+
 llvm::Value* EmitFullWarpShuffleDown(llvm::Value* value, llvm::Value* offset,
                                      llvm::IRBuilder<>* builder) {
   int bit_width = value->getType()->getPrimitiveSizeInBits();
@@ -244,6 +269,8 @@ llvm::Value* EmitFullWarpShuffleDown(llvm::Value* value, llvm::Value* offset,
       return EmitNVPTXShflDown(value, offset, builder);
     } else if (target_triple.getArch() == llvm::Triple::amdgcn) {
       return EmitAMDGPUShflDown(value, offset, builder);
+    } else if (target_triple.isSPIR()) {
+      return EmitSPIRShflDown(value, offset, builder);
     } else {
       LOG(FATAL) << "Invalid triple " << target_triple.str();
     }
@@ -265,6 +292,9 @@ llvm::Value* EmitFullWarpShuffleDown(llvm::Value* value, llvm::Value* offset,
     } else if (target_triple.getArch() == llvm::Triple::amdgcn) {
       insert_val = EmitAMDGPUShflDown(builder->CreateExtractElement(x, i),
                                       offset, builder);
+    } else if (target_triple.isSPIR()) {
+      insert_val = EmitSPIRShflDown(builder->CreateExtractElement(x, i), offset,
+                                    builder);
     } else {
       LOG(FATAL) << "Invalid triple " << target_triple.str();
     }
diff --git a/xla/service/gpu/matmul_utils.h b/xla/service/gpu/matmul_utils.h
index 78c11c5bcc..44a30e52d0 100644
--- a/xla/service/gpu/matmul_utils.h
+++ b/xla/service/gpu/matmul_utils.h
@@ -44,6 +44,23 @@ limitations under the License.
 #endif  // TF_HIPBLASLT
 #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
 
+namespace stream_executor {
+namespace cuda {
+namespace BlasLt {
+enum class Epilogue {
+  kDefault = 1,                   // No special postprocessing
+  kReLU = 2,                      // Apply point-wise ReLU function
+  kBias = 4,                      // Add broadcasted bias vector
+  kBiasThenReLU = kBias | kReLU,  // Apply bias and then ReLU transform
+  kGELU = 32,                // Apply GELU point-wise transform to the results
+  kGELUWithAux = 32 | 1024,  // Apply GELU with auxiliary output.
+  kBiasThenGELU = kBias | kGELU,  // Apply bias and then approximate GELU.
+  kBiasThenGELUWithAux = kBiasThenGELU | 1024,
+};
+}
+}  // namespace cuda
+}  // namespace stream_executor
+
 namespace xla {
 namespace gpu {
 
@@ -175,6 +192,7 @@ struct GemmConfig {
   double beta;
   std::optional<int64_t> algorithm;
   int64_t compute_precision;
+  se::cuda::BlasLt::Epilogue epilogue;
 };
 
 StatusOr<se::blas::ComputationType> GetBlasComputationType(
diff --git a/xla/service/gpu/parallel_loop_emitter.cc b/xla/service/gpu/parallel_loop_emitter.cc
index 7577f166f6..857f3cf4ef 100644
--- a/xla/service/gpu/parallel_loop_emitter.cc
+++ b/xla/service/gpu/parallel_loop_emitter.cc
@@ -178,9 +178,7 @@ ParallelLoopEmitter::EmitIndexAndSetExitBasicBlock(absl::string_view loop_name,
   // for that dimensions.  This helps LLVM generate vectorized codes
   // in that cases.
   llvm::Value* row_index = nullptr;
-  if (!launch_config_.row_vectorized) {
-    array_indices.emplace_back(linear_index_base, shape_, b_);
-  } else {
+  if (launch_config_.row_vectorized) {
     // Simpler index for row computation.
     // This will allow LLVM to vectorize.
     row_index = b_->CreateMul(
@@ -192,19 +190,19 @@ ParallelLoopEmitter::EmitIndexAndSetExitBasicBlock(absl::string_view loop_name,
     array_indices.emplace_back(linear_index_base, multidim, shape_, b_);
   }
 
-  for (int i = 1; i < launch_config_.unroll_factor; ++i) {
+  for (int i = 0; i < launch_config_.unroll_factor; ++i) {
     llvm::Value* linear_index =
         b_->CreateAdd(linear_index_base, llvm::ConstantInt::get(index_type, i),
                       absl::StrCat("linear_index", i),
                       /*HasNUW=*/true, /*HasNSW=*/true);
-    if (!launch_config_.row_vectorized) {
-      array_indices.emplace_back(linear_index, shape_, b_);
-    } else {
+    if (launch_config_.row_vectorized && i > 0) {
       std::vector<llvm::Value*> multidim(shape_.rank(), nullptr);
       multidim.back() = b_->CreateAdd(
           row_index, llvm::ConstantInt::get(index_type, i),
           absl::StrCat("row_index_plus", i), /*HasNUW=*/true, /*HasNSW=*/true);
       array_indices.emplace_back(linear_index, multidim, shape_, b_);
+    } else {
+      array_indices.emplace_back(linear_index, shape_, b_);
     }
   }
 
diff --git a/xla/service/gpu/stream_executor_util.cc b/xla/service/gpu/stream_executor_util.cc
index 7904e6d5e8..1d8305c044 100644
--- a/xla/service/gpu/stream_executor_util.cc
+++ b/xla/service/gpu/stream_executor_util.cc
@@ -139,6 +139,13 @@ StreamExecutorConvLayoutsToXlaLayouts(const ConvolutionDimensionNumbers& dnums,
                            dnums.kernel_spatial_dimensions().end());
       filter_layout.push_back(dnums.kernel_input_feature_dimension());
       break;
+    case FilterLayout::kYXInputOutput:  // HWIO
+      filter_layout.insert(filter_layout.end(),
+                           dnums.kernel_spatial_dimensions().begin(),
+                           dnums.kernel_spatial_dimensions().end());
+      filter_layout.push_back(dnums.kernel_input_feature_dimension());
+      filter_layout.push_back(dnums.kernel_output_feature_dimension());
+      break;
     default:
       return InternalError("Invalid filter layout %s for conv with dnums %s,",
                            FilterLayoutString(filter),
@@ -176,7 +183,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
   Layout nhwc_input, nhwc_filter, nhwc_output;
   std::tie(nhwc_input, nhwc_filter, nhwc_output) =
       StreamExecutorConvLayoutsToXlaLayouts(dnums, DataLayout::kBatchYXDepth,
-                                            FilterLayout::kOutputYXInput,
+                                            FilterLayout::kYXInputOutput,
                                             DataLayout::kBatchYXDepth)
           .value();
 
@@ -225,7 +232,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
           ConvolutionDimensionNumbersToString(dnums), vect_size);
     }
   } else if (LayoutUtil::Equal(filter.layout(), nhwc_filter)) {
-    filter_layout = FilterLayout::kOutputYXInput;
+    filter_layout = FilterLayout::kYXInputOutput;
   } else {
     return InternalError(
         "Invalid filter layout %s for conv with dnums %s, expected one of (%s, "
@@ -328,7 +335,8 @@ StatusOr<std::unique_ptr<se::KernelBase>> CreateKernel(
 
   if (!cubin_data.empty()) {
     loader_spec.AddCudaCubinInMemory(
-        reinterpret_cast<const char*>(cubin_data.data()), kernel_name);
+        reinterpret_cast<const char*>(cubin_data.data()), cubin_data.size(),
+        kernel_name);
   }
 
   auto kernel_base = std::make_unique<se::KernelBase>(stream_exec);
diff --git a/xla/service/gpu/stream_executor_util.h b/xla/service/gpu/stream_executor_util.h
index 4e291b5e9a..89455d18f1 100644
--- a/xla/service/gpu/stream_executor_util.h
+++ b/xla/service/gpu/stream_executor_util.h
@@ -106,6 +106,8 @@ StatusOr<se::dnn::FusedMHAKind> GetDNNFusedMHAKindFromCudnnfMHAKind(
     CudnnfMHAKind kind);
 
 StatusOr<se::dnn::DataType> GetDNNDataTypeFromPrimitiveType(PrimitiveType type);
+StatusOr<se::dnn::FusedMHAKind> GetDNNFusedMHAKindFromCudnnfMHAKind(
+    CudnnfMHAKind kind);
 
 // Returns result with the smallest time which has not failed.
 // If deterministic output is requested, returns first (not failing) result.
diff --git a/xla/service/gpu/target_constants.h b/xla/service/gpu/target_constants.h
index 92f31a6c13..ff12c19d90 100644
--- a/xla/service/gpu/target_constants.h
+++ b/xla/service/gpu/target_constants.h
@@ -54,6 +54,28 @@ inline const char* DataLayout() {
 
 }  // namespace amdgpu
 
+namespace spir {
+// SYCL: The triple that represents our target on SPIR backend.
+inline const char* TargetTriple() {
+  static constexpr char kTargetTriple[] = "spir64-unknown-unknown";
+  return kTargetTriple;
+}
+
+// The data layout of the emitted module.
+inline const char* DataLayout() {
+  // Specifies the address space as global address space
+  // A1: Specifies the address space of objects created by ‘alloca’.
+  // P1: Specifies the address space that corresponds to program memory.
+  // G1: Specifies the address space of global variables.
+  static constexpr char kDataLayout[] =
+      "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:"
+      "32:32-f64:64:64-v16:16:16-v24:32:32-v32:32:32-v48:64:64-v64:64:64-v96:"
+      "128:128-v128:128:128-v192:256:256-v256:256:256-v512:512:512-v1024:1024:"
+      "1024";
+  return kDataLayout;
+}
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/target_util.cc b/xla/service/gpu/target_util.cc
index 12b12b3f19..951c8ac259 100644
--- a/xla/service/gpu/target_util.cc
+++ b/xla/service/gpu/target_util.cc
@@ -23,12 +23,12 @@ limitations under the License.
 #include "llvm/IR/IntrinsicsAMDGPU.h"
 #include "llvm/IR/IntrinsicsNVPTX.h"
 #include "llvm/IR/MDBuilder.h"
+#include "tsl/platform/logging.h"
 #include "xla/hlo/ir/hlo_opcode.h"
 #include "xla/primitive_util.h"
 #include "xla/service/llvm_ir/llvm_type_conversion_util.h"
 #include "xla/service/llvm_ir/llvm_util.h"
 #include "xla/status.h"
-#include "tsl/platform/logging.h"
 
 namespace xla {
 namespace gpu {
@@ -45,6 +45,10 @@ struct TargetIntrinsics {
   std::variant<llvm::Intrinsic::ID,
                std::function<llvm::CallInst*(llvm::IRBuilder<>*)>>
       amdgpu_intrinsic_or_function;
+  // SYCL: Target for SPIRV.
+  absl::variant<llvm::Intrinsic::ID,
+                std::function<llvm::CallInst*(llvm::IRBuilder<>*)>>
+      spir_intrinsic_or_function;
 };
 
 // Gets the llvm intrinsic ids on different platforms (NVPTX, AMDGPU)
@@ -52,32 +56,79 @@ struct TargetIntrinsics {
 struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
   switch (intrin) {
     case TargetIntrinsicID::kThreadIdx: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_tid_x,
-              llvm::Intrinsic::amdgcn_workitem_id_x};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_tid_x,
+          llvm::Intrinsic::amdgcn_workitem_id_x,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("__builtin_IB_get_local_id_x", {}, {},
+                                          U32, {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kThreadIdy: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_tid_y,
-              llvm::Intrinsic::amdgcn_workitem_id_y};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_tid_y,
+          llvm::Intrinsic::amdgcn_workitem_id_y,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("__builtin_IB_get_local_id_y", {}, {},
+                                          U32, {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kThreadIdz: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_tid_z,
-              llvm::Intrinsic::amdgcn_workitem_id_z};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_tid_z,
+          llvm::Intrinsic::amdgcn_workitem_id_z,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("__builtin_IB_get_local_id_z", {}, {},
+                                          U32, {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBlockIdx: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_x,
-              llvm::Intrinsic::amdgcn_workgroup_id_x};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_x,
+          llvm::Intrinsic::amdgcn_workgroup_id_x,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("__builtin_IB_get_group_id",
+                                          {b_->getInt32(0)}, {U32}, U32,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBlockIdy: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_y,
-              llvm::Intrinsic::amdgcn_workgroup_id_y};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_y,
+          llvm::Intrinsic::amdgcn_workgroup_id_y,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("__builtin_IB_get_group_id",
+                                          {b_->getInt32(1)}, {U32}, U32,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBlockIdz: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_z,
-              llvm::Intrinsic::amdgcn_workgroup_id_z};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_z,
+          llvm::Intrinsic::amdgcn_workgroup_id_z,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("__builtin_IB_get_group_id",
+                                          {b_->getInt32(2)}, {U32}, U32,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBarrierId: {
-      return {llvm::Intrinsic::nvvm_barrier0,
-              llvm::Intrinsic::amdgcn_s_barrier};
+      return {llvm::Intrinsic::nvvm_barrier0, llvm::Intrinsic::amdgcn_s_barrier,
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall(
+                    "_Z22__spirv_ControlBarrierjjj",
+                    {b_->getInt32(2), b_->getInt32(2), b_->getInt32(272)},
+                    {U32, U32, U32}, VOID,
+                    llvm::AttrBuilder(b_->getContext())
+                        .addAttribute(llvm::Attribute::Convergent),
+                    b_);
+              }};
     }
     case TargetIntrinsicID::kBlockDimx: {
       return {llvm::Intrinsic::nvvm_read_ptx_sreg_ntid_x,
@@ -85,6 +136,11 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
                 return EmitDeviceFunctionCall("__ockl_get_local_size",
                                               {b_->getInt32(0)}, {U32}, U64,
                                               {b_->getContext()}, b_);
+              },
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall("__builtin_IB_get_local_size",
+                                              {b_->getInt32(0)}, {U32}, U32,
+                                              {b_->getContext()}, b_);
               }};
     }
     case TargetIntrinsicID::kBlockDimy: {
@@ -93,6 +149,11 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
                 return EmitDeviceFunctionCall("__ockl_get_local_size",
                                               {b_->getInt32(1)}, {U32}, U64,
                                               {b_->getContext()}, b_);
+              },
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall("__builtin_IB_get_local_size",
+                                              {b_->getInt32(1)}, {U32}, U32,
+                                              {b_->getContext()}, b_);
               }};
     }
     case TargetIntrinsicID::kBlockDimz: {
@@ -101,11 +162,26 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
                 return EmitDeviceFunctionCall("__ockl_get_local_size",
                                               {b_->getInt32(2)}, {U32}, U64,
                                               {b_->getContext()}, b_);
+              },
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall("__builtin_IB_get_local_size",
+                                              {b_->getInt32(2)}, {U32}, U32,
+                                              {b_->getContext()}, b_);
               }};
     }
     case TargetIntrinsicID::kGroupBarrierId: {
       return {llvm::Intrinsic::nvvm_bar_warp_sync,
-              llvm::Intrinsic::amdgcn_wave_barrier};
+              llvm::Intrinsic::amdgcn_wave_barrier,
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                // TODO: Fix it.
+                return EmitDeviceFunctionCall(
+                    "_Z22__spirv_ControlBarrierjjj",
+                    {b_->getInt32(2), b_->getInt32(2), b_->getInt32(272)},
+                    {U32, U32, U32}, VOID,
+                    llvm::AttrBuilder(b_->getContext())
+                        .addAttribute(llvm::Attribute::Convergent),
+                    b_);
+              }};
     }
   }
 }
@@ -114,6 +190,7 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
 struct TargetDeviceFunction {
   const std::string nvptx_root;
   const std::string amdgpu_root;
+  const std::string spir_root;
 };
 
 // Gets the device function name on different platforms (NVPTX, AMDGPU)
@@ -122,49 +199,49 @@ struct TargetDeviceFunction GetDeviceFunctionRoot(
     TargetDeviceFunctionID func_id) {
   switch (func_id) {
     case TargetDeviceFunctionID::kAtan2: {
-      return {"__nv_atan2", "__ocml_atan2"};
+      return {"__nv_atan2", "__ocml_atan2", "_Z17__spirv_ocl_atan2"};
     }
     case TargetDeviceFunctionID::kCos: {
-      return {"__nv_cos", "__ocml_cos"};
+      return {"__nv_cos", "__ocml_cos", "_Z15__spirv_ocl_cos"};
     }
     case TargetDeviceFunctionID::kExp: {
-      return {"__nv_exp", "__ocml_exp"};
+      return {"__nv_exp", "__ocml_exp", "_Z15__spirv_ocl_exp"};
     }
     case TargetDeviceFunctionID::kExpm1: {
-      return {"__nv_expm1", "__ocml_expm1"};
+      return {"__nv_expm1", "__ocml_expm1", "_Z17__spirv_ocl_expm1"};
     }
     case TargetDeviceFunctionID::kFmod: {
-      return {"__nv_fmod", "__ocml_fmod"};
+      return {"__nv_fmod", "__ocml_fmod", "_Z16__spirv_ocl_fmod"};
     }
     case TargetDeviceFunctionID::kHypot: {
-      return {"__nv_hypot", "__ocml_hypot"};
+      return {"__nv_hypot", "__ocml_hypot", "_Z17__spirv_ocl_hypot"};
     }
     case TargetDeviceFunctionID::kLog: {
-      return {"__nv_log", "__ocml_log"};
+      return {"__nv_log", "__ocml_log", "_Z15__spirv_ocl_log"};
     }
     case TargetDeviceFunctionID::kLog1p: {
-      return {"__nv_log1p", "__ocml_log1p"};
+      return {"__nv_log1p", "__ocml_log1p", "_Z17__spirv_ocl_log1p"};
     }
     case TargetDeviceFunctionID::kPow: {
-      return {"__nv_pow", "__ocml_pow"};
+      return {"__nv_pow", "__ocml_pow", "_Z15__spirv_ocl_pow"};
     }
     case TargetDeviceFunctionID::kRsqrt: {
-      return {"__nv_rsqrt", "__ocml_rsqrt"};
+      return {"__nv_rsqrt", "__ocml_rsqrt", "_Z17__spirv_ocl_rsqrt"};
     }
     case TargetDeviceFunctionID::kSin: {
-      return {"__nv_sin", "__ocml_sin"};
+      return {"__nv_sin", "__ocml_sin", "_Z15__spirv_ocl_sin"};
     }
     case TargetDeviceFunctionID::kSqrt: {
-      return {"__nv_sqrt", "__ocml_sqrt"};
+      return {"__nv_sqrt", "__ocml_sqrt", "_Z16__spirv_ocl_sqrt"};
     }
     case TargetDeviceFunctionID::kTan: {
-      return {"__nv_tan", "__ocml_tan"};
+      return {"__nv_tan", "__ocml_tan", "_Z15__spirv_ocl_tan"};
     }
     case TargetDeviceFunctionID::kTanh: {
-      return {"__nv_tanh", "__ocml_tanh"};
+      return {"__nv_tanh", "__ocml_tanh", "_Z16__spirv_ocl_tanh"};
     }
     case TargetDeviceFunctionID::kCbrt: {
-      return {"__nv_cbrt", "__ocml_cbrt"};
+      return {"__nv_cbrt", "__ocml_cbrt", "_Z16__spirv_ocl_cbrt"};
     }
   }
 }
@@ -231,6 +308,24 @@ std::string ObtainDeviceFunctionName(TargetDeviceFunctionID func_id,
     } else {
       LOG(FATAL) << "Unexpected type while getting device function name.";
     }
+  } else if (target_triple.isSPIR()) {
+    if (output_type == F32) {
+      if (gpu_root_names.spir_root == "_Z17__spirv_ocl_hypot" ||
+          gpu_root_names.spir_root == "_Z15__spirv_ocl_pow" ||
+          gpu_root_names.spir_root == "_Z17__spirv_ocl_atan2" ||
+          gpu_root_names.spir_root == "_Z16__spirv_ocl_fmod")
+        return StrCat(gpu_root_names.spir_root, "ff");
+      return StrCat(gpu_root_names.spir_root, "f");
+    } else if (output_type == F64) {
+      if (gpu_root_names.spir_root == "_Z17__spirv_ocl_hypot" ||
+          gpu_root_names.spir_root == "_Z15__spirv_ocl_pow" ||
+          gpu_root_names.spir_root == "_Z17__spirv_ocl_atan2" ||
+          gpu_root_names.spir_root == "_Z16__spirv_ocl_fmod")
+        return StrCat(gpu_root_names.spir_root, "dd");
+      return StrCat(gpu_root_names.spir_root, "d");
+    } else {
+      LOG(FATAL) << "Unexpected type while getting device function name.";
+    }
   } else {
     LOG(FATAL) << "Invalid triple " << target_triple.str();
   }
@@ -243,6 +338,7 @@ llvm::CallInst* EmitDeviceFunctionCall(
     absl::string_view name) {
   std::vector<llvm::Type*> ir_input_types;
   llvm::Module* module = b->GetInsertBlock()->getModule();
+  llvm::Triple target_triple = llvm::Triple(module->getTargetTriple());
   for (PrimitiveType input_type : input_types) {
     ir_input_types.push_back(
         llvm_ir::PrimitiveTypeToIrType(input_type, module));
@@ -260,6 +356,9 @@ llvm::CallInst* EmitDeviceFunctionCall(
           .getCallee());
 
   callee->addFnAttrs(attributes);
+  // SYCL: SPIR function
+  if (target_triple.isSPIR())
+    callee->setCallingConv(llvm::CallingConv::SPIR_FUNC);
 
   return b->CreateCall(callee, llvm_ir::AsArrayRef(operands), name.data());
 }
@@ -285,6 +384,18 @@ llvm::CallInst* EmitCallToTargetIntrinsic(
               &gpu_intrinsic_id.amdgpu_intrinsic_or_function);
       return (*builder_func)(b);
     }
+  } else if (target_triple.isSPIR()) {
+    llvm::Intrinsic::ID* llvm_intrinsic_id_ptr =
+        absl::get_if<llvm::Intrinsic::ID>(
+            &gpu_intrinsic_id.spir_intrinsic_or_function);
+    if (llvm_intrinsic_id_ptr) {
+      llvm_intrinsic_id = *llvm_intrinsic_id_ptr;
+    } else {
+      std::function<llvm::CallInst*(llvm::IRBuilder<>*)>* builder_func =
+          absl::get_if<std::function<llvm::CallInst*(llvm::IRBuilder<>*)>>(
+              &gpu_intrinsic_id.spir_intrinsic_or_function);
+      return (*builder_func)(b);
+    }
   } else {
     LOG(FATAL) << "Invalid triple " << target_triple.str();
   }
@@ -312,6 +423,8 @@ void AnnotateFunctionAsGpuKernel(llvm::Module* module, llvm::Function* func,
     // Attach information so AMDGPU can recognize function as a AMDGPU kernel.
     func->setCallingConv(llvm::CallingConv::AMDGPU_KERNEL);
     func->addFnAttr("amdgpu-flat-work-group-size", "1, 1024");
+  } else if (target_triple.isSPIR()) {
+    // Do nothing
   } else {
     LOG(FATAL) << "Invalid triple " << target_triple.str();
   }
diff --git a/xla/service/gpu/thunk.cc b/xla/service/gpu/thunk.cc
index 0613debc4f..421dcd1211 100644
--- a/xla/service/gpu/thunk.cc
+++ b/xla/service/gpu/thunk.cc
@@ -78,6 +78,7 @@ Thunk::ExecuteParams::ExecuteParams(
     CASE(kTriangularSolve);
     CASE(kWhile);
     CASE(kFusedMHA);
+    CASE(kFusedQKV);
   }
 }
 
diff --git a/xla/service/gpu/thunk.h b/xla/service/gpu/thunk.h
index 2bea9efc00..19b0f5a943 100644
--- a/xla/service/gpu/thunk.h
+++ b/xla/service/gpu/thunk.h
@@ -96,7 +96,8 @@ class Thunk {
     kSequential,
     kTriangularSolve,
     kWhile,
-    kFusedMHA
+    kFusedMHA,
+    kFusedQKV
   };
 
   struct ThunkInfo {
diff --git a/xla/service/layout_normalization_test.cc b/xla/service/layout_normalization_test.cc
index 7c972ebef4..b5c34de315 100644
--- a/xla/service/layout_normalization_test.cc
+++ b/xla/service/layout_normalization_test.cc
@@ -580,6 +580,31 @@ ENTRY main {
   )");
 }
 
+TEST_F(LayoutNormalizationTest, ConstantAvoidRevisitOfUser) {
+  const char* hlo = R"(
+HloModule module
+
+ENTRY main {
+  c = f32[5,4]{0,1} constant({...})
+  s = f32[5,4]{0,1} sine(c)
+  t = f32[5,4]{0,1} tanh(s)
+  ROOT o = f32[5,4]{0,1} add(s, t)
+}
+)";
+  // If we allowed visiting the normalized user 's' of the constant, we would
+  // run into a CHECK failure, because the constant was normalized in-place and
+  // therefore would not be revisited.
+  CheckLayoutNormalization(hlo, R"(
+// CHECK: [[constant_2:%[^ ]+]] = f32[4,5]{1,0} constant({...})
+// CHECK-NEXT: [[sine:%[^ ]+]] = f32[4,5]{1,0} sine([[constant_2]])
+// CHECK-NEXT: [[bitcast_1:%[^ ]+]] = f32[5,4]{0,1} bitcast([[sine]])
+// CHECK-NEXT: [[bitcast_2:%[^ ]+]] = f32[4,5]{1,0} bitcast([[bitcast_1]])
+// CHECK-NEXT: [[tanh:%[^ ]+]] = f32[4,5]{1,0} tanh([[bitcast_2]])
+// CHECK-NEXT: [[add_3:%[^ ]+]] = f32[4,5]{1,0} add([[bitcast_2]], [[tanh]])
+// CHECK-NEXT: ROOT [[bitcast_3_4:%[^ ]+]] = f32[5,4]{0,1} bitcast([[add_3]])
+  )");
+}
+
 TEST_F(LayoutNormalizationTest, Slice) {
   const char* hlo = R"(
 HloModule module
diff --git a/xla/service/llvm_ir/fused_ir_emitter.cc b/xla/service/llvm_ir/fused_ir_emitter.cc
index 006020e41c..0add8fbe3d 100644
--- a/xla/service/llvm_ir/fused_ir_emitter.cc
+++ b/xla/service/llvm_ir/fused_ir_emitter.cc
@@ -104,7 +104,7 @@ FusedIrEmitter::IndexedGenerator FusedIrEmitter::HandleConstant(
       /*Initializer=*/initializer,
       /*Name=*/"", /*InsertBefore=*/nullptr,
       /*TLMode=*/llvm::GlobalValue::NotThreadLocal,
-      /*AddressSpace=*/0,
+      /*AddressSpace=*/1,  // SYCL: Hardcode to global addrspace
       /*isExternallyInitialized=*/false);
   global->setUnnamedAddr(llvm::GlobalVariable::UnnamedAddr::Global);
 
diff --git a/xla/service/llvm_ir/ir_array.cc b/xla/service/llvm_ir/ir_array.cc
index 36794b4b75..4c255f7627 100644
--- a/xla/service/llvm_ir/ir_array.cc
+++ b/xla/service/llvm_ir/ir_array.cc
@@ -24,6 +24,7 @@ limitations under the License.
 #include "llvm/IR/Constants.h"
 #include "llvm/IR/Instructions.h"
 #include "llvm/IR/Value.h"
+#include "tsl/platform/logging.h"
 #include "xla/layout_util.h"
 #include "xla/permutation_util.h"
 #include "xla/service/llvm_ir/llvm_type_conversion_util.h"
@@ -32,7 +33,6 @@ limitations under the License.
 #include "xla/statusor.h"
 #include "xla/util.h"
 #include "xla/xla_data.pb.h"
-#include "tsl/platform/logging.h"
 
 namespace xla {
 namespace llvm_ir {
@@ -496,9 +496,24 @@ llvm::Value* IrArray::EmitArrayElementAddress(const IrArray::Index& index,
   if (use_linear_index && index.LinearValidOnShape(shape_)) {
     llvm::Module* module = b->GetInsertBlock()->getParent()->getParent();
     llvm::Type* type = PrimitiveTypeToIrType(shape_.element_type(), module);
-    return b->CreateInBoundsGEP(
-        type, b->CreateBitCast(base_ptr_, type->getPointerTo()), index.linear(),
-        llvm_ir::AsStringRef(name));
+    
+    auto linear_index = llvm::dyn_cast<llvm::BinaryOperator>(index.linear());
+    // only separate const offset when having add
+    if (linear_index && (linear_index->getOpcode() == llvm::Instruction::Add)) {
+      llvm::Value* index_operand_0 = linear_index->getOperand(0);
+      // constant index
+      llvm::Value* index_operand_1 = linear_index->getOperand(1);
+      llvm::Value* ptr_address =
+          b->CreateGEP(type, b->CreateBitCast(base_ptr_, type->getPointerTo()),
+                       index_operand_0, "");
+
+      return b->CreateInBoundsGEP(type, ptr_address, index_operand_1,
+                                  llvm_ir::AsStringRef(name));
+    } else {
+      return b->CreateInBoundsGEP(
+          type, b->CreateBitCast(base_ptr_, type->getPointerTo()),
+          index.linear(), llvm_ir::AsStringRef(name));
+    }
   }
 
   std::vector<llvm::Value*> actual_index;
diff --git a/xla/service/llvm_ir/llvm_util.cc b/xla/service/llvm_ir/llvm_util.cc
index 676c9c3790..5e07bee28e 100644
--- a/xla/service/llvm_ir/llvm_util.cc
+++ b/xla/service/llvm_ir/llvm_util.cc
@@ -263,6 +263,8 @@ llvm::Type* PrimitiveTypeToIrType(PrimitiveType element_type,
       // Tokens do not have a physical representation, but the compiler needs
       // some placeholder type, so use int8_t*.
       return llvm::Type::getInt8PtrTy(module->getContext());
+    case VOID:
+      return llvm::Type::getVoidTy(module->getContext());
     default:
       LOG(FATAL) << "unsupported type " << element_type;
   }
@@ -345,8 +347,9 @@ llvm::AllocaInst* EmitAllocaAtFunctionEntryWithCount(llvm::Type* type,
   llvm::Function* function = b->GetInsertBlock()->getParent();
   b->SetInsertPoint(&function->getEntryBlock(),
                     function->getEntryBlock().getFirstInsertionPt());
+  // SYCL: Fix atomic issue by allocating on private addrspace
   llvm::AllocaInst* alloca =
-      b->CreateAlloca(type, element_count, AsStringRef(name));
+      b->CreateAlloca(type, /*addrspace*/5, element_count, AsStringRef(name));
   if (alignment != 0) {
     alloca->setAlignment(llvm::Align(alignment));
   }
@@ -465,6 +468,7 @@ void SetDereferenceableMetadataForLoad(llvm::LoadInst* load,
 
 llvm::Instruction* AddRangeMetadata(int32_t lower, int32_t upper,
                                     llvm::Instruction* inst) {
+  /* SYCL: This range is for NVPTX target only.
   llvm::LLVMContext& context = inst->getParent()->getContext();
   llvm::IntegerType* i32 = llvm::Type::getInt32Ty(context);
   inst->setMetadata(
@@ -473,6 +477,7 @@ llvm::Instruction* AddRangeMetadata(int32_t lower, int32_t upper,
           context,
           {llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(i32, lower)),
            llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(i32, upper))}));
+  */
   return inst;
 }
 
diff --git a/xla/stream_executor/build_defs.bzl b/xla/stream_executor/build_defs.bzl
index 8e75a55324..38e2f5b7a1 100644
--- a/xla/stream_executor/build_defs.bzl
+++ b/xla/stream_executor/build_defs.bzl
@@ -19,10 +19,12 @@ def tf_additional_cudnn_plugin_copts():
 
 # Returns whether any GPU backend is configuered.
 def if_gpu_is_configured(x):
-    return if_cuda_is_configured(x) + if_rocm_is_configured(x)
+    # hardcode to enable sycl
+    return if_cuda_is_configured(x) + if_rocm_is_configured(x) + x
 
 def if_cuda_or_rocm(x):
-    return if_gpu_is_configured(x)
+    # hardcode to enable sycl
+    return if_gpu_is_configured(x) + x
 
 # nvlink is not available via the pip wheels, disable it since it will create
 # unnecessary dependency
diff --git a/xla/stream_executor/cuda/cuda_driver.cc b/xla/stream_executor/cuda/cuda_driver.cc
index dcb6a3c8c4..981a66a82c 100644
--- a/xla/stream_executor/cuda/cuda_driver.cc
+++ b/xla/stream_executor/cuda/cuda_driver.cc
@@ -892,6 +892,14 @@ GpuDriver::GraphNodeGetType(CUgraphNode node) {
       "Feature not supported on CUDA platform (LoadHsaco)");
 }
 
+/* static */ tsl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  CUmodule* module) {
+  LOG(ERROR) << "Feature not supported on CUDA platform (LoadLevelzero)";
+  return tsl::errors::Internal("Not Implemented");
+}
+
 /* static */ tsl::Status GpuDriver::SynchronousMemsetUint8(GpuContext* context,
                                                            CUdeviceptr location,
                                                            uint8_t value,
diff --git a/xla/stream_executor/device_memory.h b/xla/stream_executor/device_memory.h
index f69fe8148c..6df3b4dc52 100644
--- a/xla/stream_executor/device_memory.h
+++ b/xla/stream_executor/device_memory.h
@@ -85,9 +85,6 @@ class DeviceMemoryBase {
     return opaque() == other.opaque() && size() == other.size();
   }
 
- protected:
-  friend class StreamExecutor;
-
   // Resets the internal values of the opaque pointer and number of bytes in the
   // memory region, just as in the constructor.
   void Reset(void *opaque, uint64_t bytes) {
@@ -95,6 +92,10 @@ class DeviceMemoryBase {
     size_ = bytes;
   }
 
+ protected:
+  friend class StreamExecutor;
+
+
  private:
   void *opaque_;  // Platform-dependent value representing allocated memory.
   uint64_t size_;         // Size in bytes of this allocation.
diff --git a/xla/stream_executor/gpu/BUILD b/xla/stream_executor/gpu/BUILD
index c8cd4f5587..9947a718ad 100644
--- a/xla/stream_executor/gpu/BUILD
+++ b/xla/stream_executor/gpu/BUILD
@@ -344,7 +344,7 @@ cc_library(
     name = "asm_compiler",
     srcs = if_gpu_is_configured(["asm_compiler.cc"]),
     hdrs = if_gpu_is_configured(["asm_compiler.h"]),
-    copts = tsl_copts(),
+    copts = tsl_copts(allow_exceptions=True),
     local_defines = if_cuda_is_configured(["GOOGLE_CUDA=1"]),
     visibility = set_external_visibility([
         "//third_party/py/jax:__subpackages__",
diff --git a/xla/stream_executor/gpu/gpu_driver.h b/xla/stream_executor/gpu/gpu_driver.h
index 292e9081a0..42a75948c6 100644
--- a/xla/stream_executor/gpu/gpu_driver.h
+++ b/xla/stream_executor/gpu/gpu_driver.h
@@ -457,6 +457,9 @@ class GpuDriver {
   // (supported on ROCm only)
   static tsl::Status LoadHsaco(GpuContext* context, const char* hsaco_contents,
                                GpuModuleHandle* module);
+  static tsl::Status LoadLevelzero(GpuContext* context,
+                                   const char* spir_contents, const size_t size,
+                                   GpuModuleHandle* module);
 
   // Retrieves a named kernel from a loaded module, and places the resulting
   // handle into function (outparam) on success. Neither kernel_name nor
diff --git a/xla/stream_executor/gpu/gpu_executor.h b/xla/stream_executor/gpu/gpu_executor.h
index c93073ffd9..cd33a406e5 100644
--- a/xla/stream_executor/gpu/gpu_executor.h
+++ b/xla/stream_executor/gpu/gpu_executor.h
@@ -317,6 +317,11 @@ class GpuExecutor : public internal::StreamExecutorInterface {
   tsl::Status LoadModuleFromHsaco(const char* hsaco, GpuModuleHandle* module)
       TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
 
+  // (supported on SYCL only)
+  tsl::Status LoadModuleFromSpir(const char* spir, const size_t size,
+                                 GpuModuleHandle* module)
+      TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
+
   bool UnloadGpuBinary(const void* gpu_binary)
       TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
 
diff --git a/xla/stream_executor/gpu/gpu_types.h b/xla/stream_executor/gpu/gpu_types.h
index dea81d66a1..f73bef66f9 100644
--- a/xla/stream_executor/gpu/gpu_types.h
+++ b/xla/stream_executor/gpu/gpu_types.h
@@ -18,7 +18,20 @@ limitations under the License.
 #ifndef XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 #define XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 
-#if TENSORFLOW_USE_ROCM
+#define TENSORFLOW_USE_SYCL 1
+
+#if TENSORFLOW_USE_SYCL
+
+#if __has_include(<sycl/sycl.hpp>)
+#include <sycl/sycl.hpp>
+#elif __has_include(<CL/sycl.hpp>)
+#include <CL/sycl.hpp>
+#else
+#error "Unsupported compiler"
+#endif
+#include <level_zero/ze_api.h>
+
+#elif TENSORFLOW_USE_ROCM
 
 #define __HIP_DISABLE_CPP_FUNCTIONS__
 
@@ -36,7 +49,30 @@ limitations under the License.
 namespace stream_executor {
 namespace gpu {
 
-#if TENSORFLOW_USE_ROCM
+#if TENSORFLOW_USE_SYCL
+
+using GpuContextHandle = const void*;
+using GpuStreamHandle = ::sycl::queue*;
+using GpuEventHandle = ::sycl::event*;
+using GpuFunctionHandle = ::sycl::kernel*;
+using GpuFunctionAttribute = const void*;
+using GpuDeviceHandle = ::sycl::device*;
+using GpuDevicePtr = void*;
+using GpuDeviceAttribute = const void*;
+using GpuDeviceProperty = const void*;
+using GpuModuleHandle = ze_module_handle_t;
+using GpuStatus = const void*;
+using GpuFuncCachePreference = const void*;
+using GpuSharedMemConfig = const void*;
+using GpuComplexType = const void*;
+using GpuDoubleComplexType = const void*;
+using GpuRngHandle = const void*;
+using GpuGraphHandle = const void*;
+using GpuGraphExecHandle = const void*;
+using GpuGraphNodeHandle = const void*;
+
+
+#elif TENSORFLOW_USE_ROCM
 
 using GpuContextHandle = hipCtx_t;
 using GpuStreamHandle = hipStream_t;
diff --git a/xla/stream_executor/kernel_spec.cc b/xla/stream_executor/kernel_spec.cc
index f0fabf44d5..e98b5c0bdc 100644
--- a/xla/stream_executor/kernel_spec.cc
+++ b/xla/stream_executor/kernel_spec.cc
@@ -43,9 +43,9 @@ CudaCubinOnDisk::CudaCubinOnDisk(absl::string_view filename,
                                  absl::string_view kernel_name)
     : OnDiskKernelLoaderSpec(filename, kernel_name) {}
 
-CudaCubinInMemory::CudaCubinInMemory(const char *bytes,
+CudaCubinInMemory::CudaCubinInMemory(const char *bytes, int size, 
                                      absl::string_view kernel_name)
-    : KernelLoaderSpec(kernel_name), bytes_(bytes) {}
+    : KernelLoaderSpec(kernel_name), size_(size),bytes_(bytes) {}
 
 bool CompareComputeCapability(const std::tuple<int, int> &lhs,
                               const std::tuple<int, int> &rhs) {
@@ -174,9 +174,9 @@ MultiKernelLoaderSpec *MultiKernelLoaderSpec::AddCudaPtxOnDisk(
 }
 
 MultiKernelLoaderSpec *MultiKernelLoaderSpec::AddCudaCubinInMemory(
-    const char *bytes, absl::string_view kernel_name) {
+    const char *bytes, int size, absl::string_view kernel_name) {
   CHECK(cuda_cubin_in_memory_ == nullptr);
-  cuda_cubin_in_memory_.reset(new CudaCubinInMemory{bytes, kernel_name});
+  cuda_cubin_in_memory_.reset(new CudaCubinInMemory{bytes, size, kernel_name});
   return this;
 }
 
diff --git a/xla/stream_executor/kernel_spec.h b/xla/stream_executor/kernel_spec.h
index 68f45a0ee0..bc585d06bf 100644
--- a/xla/stream_executor/kernel_spec.h
+++ b/xla/stream_executor/kernel_spec.h
@@ -224,13 +224,15 @@ class CudaPtxInMemory : public KernelLoaderSpec {
 // Kernel loader specification for a CUBIN blob that resides in memory.
 class CudaCubinInMemory : public KernelLoaderSpec {
  public:
-  CudaCubinInMemory(const char *bytes, absl::string_view kernel_name);
+  CudaCubinInMemory(const char *bytes,int size,  absl::string_view kernel_name);
   ~CudaCubinInMemory() override {}
 
   const char *bytes() const { return bytes_; }
-
+  const int size() const { return size_; }
  private:
   const char *bytes_;
+  // SYCL: this is needed only for SPIRV
+  int size_;
 
   CudaCubinInMemory(const CudaCubinInMemory &) = delete;
   void operator=(const CudaCubinInMemory &) = delete;
@@ -282,7 +284,7 @@ class MultiKernelLoaderSpec {
                                           absl::string_view kernel_name);
   MultiKernelLoaderSpec *AddCudaCubinOnDisk(absl::string_view filename,
                                             absl::string_view kernel_name);
-  MultiKernelLoaderSpec *AddCudaCubinInMemory(const char *cubin_bytes,
+  MultiKernelLoaderSpec *AddCudaCubinInMemory(const char *cubin_bytes, int size, 
                                               absl::string_view kernel_name);
   MultiKernelLoaderSpec *AddCudaPtxInMemory(absl::string_view ptx,
                                             absl::string_view kernel_name);
diff --git a/xla/stream_executor/rocm/rocm_driver.cc b/xla/stream_executor/rocm/rocm_driver.cc
index d48ed7e381..68d147e87a 100644
--- a/xla/stream_executor/rocm/rocm_driver.cc
+++ b/xla/stream_executor/rocm/rocm_driver.cc
@@ -771,6 +771,14 @@ GpuDriver::GraphNodeGetType(hipGraphNode_t node) {
   return ret;
 }
 
+/* static */ tsl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  hipModule_t* module) {
+  LOG(ERROR) << "Feature not supported on ROCm platform (LoadLevelzero)";
+  return tsl::errors::Internal("Not Implemented");
+}
+
 /* static */ tsl::Status GpuDriver::SynchronousMemsetUint8(
     GpuContext* context, hipDeviceptr_t location, uint8 value, size_t size) {
   ScopedActivateContext activation{context};
diff --git a/xla/stream_executor/rocm/rocm_gpu_executor.cc b/xla/stream_executor/rocm/rocm_gpu_executor.cc
index 51a3a79166..9696e777a2 100644
--- a/xla/stream_executor/rocm/rocm_gpu_executor.cc
+++ b/xla/stream_executor/rocm/rocm_gpu_executor.cc
@@ -389,6 +389,11 @@ tsl::Status GpuExecutor::LoadModuleFromHsaco(const char* hsaco,
   return tsl::OkStatus();
 }
 
+tsl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                            hipModule_t* module) {
+  LOG(FATAL) << "Feature not supported on ROCM platform (LoadModuleFromSpir)";
+}
+
 // This is a non-essential operation; if there's a failure, proceed without
 // logging an error. It's nearly certain that in case of failures, we'd never
 // get here in the first place; these are very low-impact routines.
diff --git a/xla/stream_executor/stream_executor_pimpl.h b/xla/stream_executor/stream_executor_pimpl.h
index f8903f1c94..83f8e5701f 100644
--- a/xla/stream_executor/stream_executor_pimpl.h
+++ b/xla/stream_executor/stream_executor_pimpl.h
@@ -694,7 +694,8 @@ StreamExecutor::CreateTypedKernel(absl::string_view kernel_name,
 
   if (!cubin_data.empty()) {
     loader_spec.AddCudaCubinInMemory(
-        reinterpret_cast<const char*>(cubin_data.data()), kernel_name);
+        reinterpret_cast<const char*>(cubin_data.data()), cubin_data.size(),
+        kernel_name);
   }
 
   TF_RETURN_IF_ERROR(GetKernel(loader_spec, kernel_base.get()));
diff --git a/xla/stream_executor/tf_allocator_adapter.cc b/xla/stream_executor/tf_allocator_adapter.cc
index 3fcc6b6635..926430c347 100644
--- a/xla/stream_executor/tf_allocator_adapter.cc
+++ b/xla/stream_executor/tf_allocator_adapter.cc
@@ -51,6 +51,24 @@ tsl::StatusOr<OwningDeviceMemory> TfAllocatorAdapter::Allocate(
   return OwningDeviceMemory(DeviceMemoryBase(data, size), device_ordinal, this);
 }
 
+void* TfAllocatorAdapter::AllocateRaw(
+    int device_ordinal, uint64_t size, bool retry_on_failure,
+    int64_t memory_space) {
+  CHECK_EQ(memory_space, 0);
+  tsl::AllocationAttributes attrs;
+  attrs.retry_on_failure = retry_on_failure;
+  void *data = nullptr;
+  if (size != 0) {
+    data =
+        wrapped_->AllocateRaw(tsl::Allocator::kAllocatorAlignment, size, attrs);
+   // if (data == nullptr) {
+   //   return tsl::errors::ResourceExhausted(
+   //       "Out of memory while trying to allocate ", size, " bytes.");
+   // }
+  }
+  return data;
+}
+
 tsl::Status TfAllocatorAdapter::Deallocate(int device_ordinal,
                                            DeviceMemoryBase mem) {
   wrapped_->DeallocateRaw(mem.opaque());
diff --git a/xla/stream_executor/tf_allocator_adapter.h b/xla/stream_executor/tf_allocator_adapter.h
index 5293e4639d..6810585d8a 100644
--- a/xla/stream_executor/tf_allocator_adapter.h
+++ b/xla/stream_executor/tf_allocator_adapter.h
@@ -50,6 +50,10 @@ class TfAllocatorAdapter : public DeviceMemoryAllocator {
                                              bool retry_on_failure,
                                              int64_t memory_space) override;
 
+  void* AllocateRaw(int device_ordinal, uint64_t size,
+                                             bool retry_on_failure,
+                                             int64_t memory_space);
+
   tsl::Status Deallocate(int device_ordinal, DeviceMemoryBase mem) override;
 
   // The Tensorflow BFC allocator used on GPU allows host-side deallocation
@@ -127,6 +131,19 @@ class MultiDeviceAdapter : public DeviceMemoryAllocator {
                                                               mem);
   }
 
+  void* AllocateRaw(int device_ordinal, uint64_t size,
+                    bool retry_on_failure,
+                    int64_t memory_space) {
+    CHECK_LT(device_ordinal, per_device_allocators_.size());
+    return per_device_allocators_[device_ordinal]->AllocateRaw(
+        device_ordinal, size, retry_on_failure, memory_space);
+  }
+  
+  tsl::Status DeallocateRaw(int device_ordinal, void* mem_opaque) {
+    DeviceMemoryBase device_mem(mem_opaque);
+    return this->Deallocate(device_ordinal, device_mem);
+  }
+
   // The Tensorflow BFC allocator used on GPU allows host-side deallocation
   // before GPU execution takes place. Tensorflow uses the ordering of the main
   // compute stream to enforce a happens-before relationship between a memory
diff --git a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
index c46135e023..485aaa9753 100644
--- a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
+++ b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
@@ -565,6 +565,10 @@ tsl::StatusOr<mlir::Operation*> LhloDialectEmitter::EmitCustomCallOp(
     return EmitDnnfMHABackward(custom_call_instr);
   }
 
+  if (xla::gpu::IsCustomCallTofQKV(*instr)) {
+    return EmitDnnfQKV(custom_call_instr);
+  }
+
   // For custom call, if there are any token operands or results, they will not
   // be represented in LHLO so we need to remember the mapping. First create
   // operands where each token is replaced with a null Value.
@@ -735,6 +739,8 @@ tsl::StatusOr<lmhlo_gpu::FusedMhaDagSignature> AsLhloFusedMhaDagSignature(
       return lmhlo_gpu::FusedMhaDagSignature::SoftmaxDropout;
     case xla::gpu::CudnnfMHAKind::kSoftmax:
       return lmhlo_gpu::FusedMhaDagSignature::Softmax;
+    case xla::gpu::CudnnfMHAKind::kScaleSoftmax:
+      return lmhlo_gpu::FusedMhaDagSignature::ScaleSoftmax;
     case xla::gpu::CudnnfMHAKind::kScaleBiasSoftmax:
       return lmhlo_gpu::FusedMhaDagSignature::ScaleBiasSoftmax;
     case xla::gpu::CudnnfMHAKind::kScaleBiasSoftmaxDropout:
@@ -1212,12 +1218,18 @@ tsl::StatusOr<Operation*> LhloDialectEmitter::EmitDnnfMHA(
       TF_RETURN_IF_ERROR(GetOrCreateView(custom_call, &operands));
       auto fmha =
           CreateOpWithoutAttrs<lmhlo_gpu::fusedMHAOp>(custom_call, operands);
+
+      fmha.setFmhaScaleAttr(
+          builder_.getF64FloatAttr(config.fmha_scale()));
       return set_common_fmha_attributes(fmha);
     }
     case xla::gpu::CudnnfMHAKind::kSoftmaxDropout: {
       TF_RETURN_IF_ERROR(GetOrCreateView(custom_call, &operands));
       auto fmha =
           CreateOpWithoutAttrs<lmhlo_gpu::fusedMHAOp>(custom_call, operands);
+      fmha.setFmhaScaleAttr(
+          builder_.getF64FloatAttr(config.fmha_scale()));
+
       fmha.setDropoutRateAttr(builder_.getF64FloatAttr(config.dropout_rate()));
       fmha.setSeedAttr(builder_.getI64IntegerAttr(config.seed()));
       return set_common_fmha_attributes(fmha);
@@ -1280,11 +1292,45 @@ tsl::StatusOr<Operation*> LhloDialectEmitter::EmitDnnfMHA(
       has_bias = true;
       return set_common_fmha_attributes(fmha);
     }
+    case xla::gpu::CudnnfMHAKind::kScaleSoftmax: {
+      TF_RETURN_IF_ERROR(GetOrCreateView(custom_call, &operands));
+      auto fmha_softmax =
+          CreateOpWithoutAttrs<lmhlo_gpu::fusedMHAOp>(custom_call, operands);
+      fmha_softmax.setFmhaScaleAttr(
+          builder_.getF64FloatAttr(config.fmha_scale()));
+      return set_common_fmha_attributes(fmha_softmax);
+    }
+
     default:
       return xla::InternalError("Unknown forward fused MHA call.");
   }
 }
 
+
+tsl::StatusOr<Operation*> LhloDialectEmitter::EmitDnnfQKV(
+    const HloCustomCallInstruction* custom_call) {
+
+  TF_ASSIGN_OR_RETURN(
+      auto const config,
+      custom_call->backend_config<xla::gpu::GemmBackendConfig>());
+
+  if (custom_call->operand_count() != 2) {
+    return xla::InvalidArgument("GEMM custom call should have 2 operands");
+  }
+
+  // QKV GEMM have two operands.
+  TF_ASSIGN_OR_RETURN(
+      lmhlo_gpu::fusedQKVOp op,
+      CreateOpWithoutAttrs<lmhlo_gpu::fusedQKVOp>(custom_call,
+                                              /*num_operands=*/2));
+  op.setDotDimensionNumbersAttr(
+      GetDotDimensionNumbersAttr(builder_, config.dot_dimension_numbers()));
+
+  return op.getOperation();
+}
+
+
+
 tsl::StatusOr<Operation*> LhloDialectEmitter::EmitDnnfMHABackward(
     const HloCustomCallInstruction* custom_call) {
   TF_ASSIGN_OR_RETURN(
diff --git a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.h b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.h
index c355cb132f..aacee29c87 100644
--- a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.h
+++ b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.h
@@ -90,6 +90,8 @@ class LhloDialectEmitter : public xla::ConstDfsHloVisitorWithDefault {
       const xla::HloCustomCallInstruction* custom_call);
   xla::StatusOr<Operation*> EmitDnnfMHA(
       const xla::HloCustomCallInstruction* custom_call);
+  xla::StatusOr<Operation*> EmitDnnfQKV(
+      const xla::HloCustomCallInstruction* custom_call);
   xla::StatusOr<Operation*> EmitDnnfMHABackward(
       const xla::HloCustomCallInstruction* custom_call);
   tsl::StatusOr<memref::GetGlobalOp> EmitConstant(
diff --git a/xla/xla_data.proto b/xla/xla_data.proto
index ba43906c94..498a41d0b4 100644
--- a/xla/xla_data.proto
+++ b/xla/xla_data.proto
@@ -28,6 +28,7 @@ enum PrimitiveType {
   // Invalid primitive type to serve as default.
   PRIMITIVE_TYPE_INVALID = 0;
 
+  VOID = 100;
   // Predicates are two-state booleans.
   PRED = 1;
 
