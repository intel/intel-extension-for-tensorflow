diff --git a/Eigen/Core b/Eigen/Core
index f44b77831..9721cc60d 100644
--- a/Eigen/Core
+++ b/Eigen/Core
@@ -29,7 +29,6 @@
   #include <hip/hip_runtime.h>
 #endif
 
-
 #ifdef EIGEN_EXCEPTIONS
   #include <new>
 #endif
@@ -106,13 +105,21 @@
   #include <intrin.h>
 #endif
 
-#if defined(EIGEN_USE_SYCL)
+#if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
   #undef min
   #undef max
   #undef isnan
   #undef isinf
   #undef isfinite
+
+  #if __has_include(<sycl/sycl.hpp>)
+  #include <sycl/sycl.hpp>
+  #elif __has_include(<CL/sycl.hpp>)
   #include <CL/sycl.hpp>
+  #else
+  #error "Unsupported compiler"
+  #endif
+  
   #include <map>
   #include <memory>
   #include <utility>
@@ -166,8 +173,13 @@ using std::ptrdiff_t;
 #include "src/Core/MathFunctionsImpl.h"
 #include "src/Core/arch/Default/ConjHelper.h"
 // Generic half float support
+#if defined(DPCPP_DEVICE_ONLY)
+#include "src/Core/arch/DPCPP/Half.h"
+#include "src/Core/arch/DPCPP/BFloat16.h"
+#else
 #include "src/Core/arch/Default/Half.h"
 #include "src/Core/arch/Default/BFloat16.h"
+#endif // defined(DPCPP_DEVICE_ONLY)
 #include "src/Core/arch/Default/TypeCasting.h"
 #include "src/Core/arch/Default/GenericPacketMathFunctionsFwd.h"
 
@@ -218,6 +230,19 @@ using std::ptrdiff_t;
   #include "src/Core/arch/MSA/Complex.h"
 #endif
 
+#include "src/Core/arch/Default/Settings.h"
+// This file provides generic implementations valid for scalar as well
+#include "src/Core/arch/Default/GenericPacketMathFunctions.h"
+
+#include "src/Core/functors/TernaryFunctors.h"
+#include "src/Core/functors/BinaryFunctors.h"
+#include "src/Core/functors/UnaryFunctors.h"
+#include "src/Core/functors/NullaryFunctors.h"
+#include "src/Core/functors/StlFunctors.h"
+#include "src/Core/functors/AssignmentFunctors.h"
+
+#include <unsupported/Eigen/SpecialFunctions>
+
 #if defined EIGEN_VECTORIZE_GPU
   #include "src/Core/arch/GPU/PacketMath.h"
   #include "src/Core/arch/GPU/MathFunctions.h"
@@ -234,16 +259,15 @@ using std::ptrdiff_t;
 #endif
 #endif
 
-#include "src/Core/arch/Default/Settings.h"
-// This file provides generic implementations valid for scalar as well
-#include "src/Core/arch/Default/GenericPacketMathFunctions.h"
-
-#include "src/Core/functors/TernaryFunctors.h"
-#include "src/Core/functors/BinaryFunctors.h"
-#include "src/Core/functors/UnaryFunctors.h"
-#include "src/Core/functors/NullaryFunctors.h"
-#include "src/Core/functors/StlFunctors.h"
-#include "src/Core/functors/AssignmentFunctors.h"
+#if defined(EIGEN_USE_DPCPP)
+  #include "src/Core/arch/DPCPP/InteropHeaders.h"
+  #include "src/Core/arch/DPCPP/TypeCasting.h"
+  #include "itex_gpu_runtime.h"
+#if !defined(EIGEN_DONT_VECTORIZE_SYCL)
+  #include "src/Core/arch/DPCPP/MathFunctions.h"
+  #include "src/Core/arch/DPCPP/PacketMath.h"
+#endif
+#endif
 
 // Specialized functors to enable the processing of complex numbers
 // on CUDA devices
diff --git a/Eigen/src/Core/GenericPacketMath.h b/Eigen/src/Core/GenericPacketMath.h
index ebae1a34f..fe7332db4 100644
--- a/Eigen/src/Core/GenericPacketMath.h
+++ b/Eigen/src/Core/GenericPacketMath.h
@@ -107,6 +107,7 @@ template<typename T> struct packet_traits : default_packet_traits
 {
   typedef T type;
   typedef T half;
+  static const bool vectorized = false;
   enum {
     Vectorizable = 0,
     size = 1,
@@ -193,6 +194,11 @@ padd(const bool& a, const bool& b) { return a || b; }
 template<typename Packet> EIGEN_DEVICE_FUNC inline Packet
 psub(const Packet& a, const Packet& b) { return a-b; }
 
+template<typename Packet> EIGEN_DEVICE_FUNC inline Packet
+psign(const Packet& a) {
+  return (numext::isnan)(a) ? a : Packet( (a>Packet(0)) - (a<Packet(0)) );
+}
+
 /** \internal \returns -a (coeff-wise) */
 template<typename Packet> EIGEN_DEVICE_FUNC inline Packet
 pnegate(const Packet& a) { return -a; }
@@ -223,6 +229,15 @@ pmin(const Packet& a, const Packet& b) { return numext::mini(a, b); }
 template<typename Packet> EIGEN_DEVICE_FUNC inline Packet
 pmax(const Packet& a, const Packet& b) { return numext::maxi(a, b); }
 
+template<typename OutPacket, typename InPacket> EIGEN_DEVICE_FUNC inline OutPacket 
+pisnan(const InPacket& a) { using numext::isnan; return isnan(a); }
+
+template<typename OutPacket, typename InPacket> EIGEN_DEVICE_FUNC inline OutPacket 
+pisfinite(const InPacket& a) { using numext::isfinite; return isfinite(a); }
+
+template<typename OutPacket, typename InPacket> EIGEN_DEVICE_FUNC inline OutPacket 
+pisinf(const InPacket& a) { using numext::isinf; return isinf(a); }
+
 /** \internal \returns the absolute value of \a a */
 template<typename Packet> EIGEN_DEVICE_FUNC inline Packet
 pabs(const Packet& a) { using std::abs; return abs(a); }
@@ -293,7 +308,14 @@ template<> EIGEN_DEVICE_FUNC inline double pzero<double>(const double& a) {
 
 /** \internal \returns one bits */
 template<typename Packet> EIGEN_DEVICE_FUNC inline Packet
-ptrue(const Packet& /*a*/) { Packet b; memset((void*)&b, 0xff, sizeof(b)); return b;}
+ptrue(const Packet& /*a*/) {
+  Packet b;
+  unsigned char* byte_ptr = reinterpret_cast<unsigned char*>(&b);
+  EIGEN_UNROLL_LOOP
+  for (auto i = 0; i < sizeof(b); ++i)
+    byte_ptr[i] = 0xff;
+  return b;
+}
 
 template <typename RealScalar>
 EIGEN_DEVICE_FUNC inline std::complex<RealScalar> ptrue(const std::complex<RealScalar>& /*a*/) {
@@ -318,6 +340,9 @@ pxor(const Packet& a, const Packet& b) { return a ^ b; }
 template<typename Packet> EIGEN_DEVICE_FUNC inline Packet
 pandnot(const Packet& a, const Packet& b) { return pand(a, pxor(ptrue(b), b)); }
 
+template<typename Packet> EIGEN_DEVICE_FUNC inline Packet
+plogical_not(const Packet& a) { return !a; }
+
 /** \internal \returns a <= b as a bit mask */
 template<typename Packet> EIGEN_DEVICE_FUNC inline Packet
 pcmp_le(const Packet& a, const Packet& b)  { return a<=b ? ptrue(a) : pzero(a); }
@@ -334,6 +359,24 @@ pcmp_eq(const Packet& a, const Packet& b) { return a==b ? ptrue(a) : pzero(a); }
 template<typename Packet> EIGEN_DEVICE_FUNC inline Packet
 pcmp_lt_or_nan(const Packet& a, const Packet& b) { return a>=b ? pzero(a) : ptrue(a); }
 
+template<typename OutPacket, typename InPacket> EIGEN_DEVICE_FUNC inline OutPacket
+boolean_pcmp_le(const InPacket& a, const InPacket& b) {
+  OutPacket tmp; 
+  return a<=b ? ptrue(tmp) : pzero(tmp); 
+}
+
+template<typename OutPacket, typename InPacket> EIGEN_DEVICE_FUNC inline OutPacket
+boolean_pcmp_lt(const InPacket& a, const InPacket& b) { 
+  OutPacket tmp;
+  return a<b ? ptrue(tmp) : pzero(tmp); 
+}
+
+template<typename OutPacket, typename InPacket> EIGEN_DEVICE_FUNC inline OutPacket
+boolean_pcmp_eq(const InPacket& a, const InPacket& b) {
+  OutPacket tmp; 
+  return a==b ? ptrue(tmp) : pzero(tmp); 
+}
+
 /** \internal \returns \a or \b for each field in packet according to \mask */
 template<typename Packet> EIGEN_DEVICE_FUNC inline Packet
 pselect(const Packet& mask, const Packet& a, const Packet& b) {
diff --git a/Eigen/src/Core/MathFunctions.h b/Eigen/src/Core/MathFunctions.h
index 96cb24fcb..1aea7f4ff 100644
--- a/Eigen/src/Core/MathFunctions.h
+++ b/Eigen/src/Core/MathFunctions.h
@@ -12,7 +12,7 @@
 
 // source: http://www.geom.uiuc.edu/~huberty/math5337/groupe/digits.html
 // TODO this should better be moved to NumTraits
-#define EIGEN_PI 3.141592653589793238462643383279502884197169399375105820974944592307816406L
+#define EIGEN_PI 3.141592653589793238462643383279502884197169399375105820974944592307816406
 
 namespace Eigen {
 
@@ -1049,25 +1049,25 @@ EIGEN_ALWAYS_INLINE long double maxi(const long double& x, const long double& y)
 
 
 #define SYCL_SPECIALIZE_SIGNED_INTEGER_TYPES_BINARY(NAME, FUNC) \
-  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_char)   \
-  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_short)  \
-  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_int)    \
-  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_long)
+  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, sycl::cl_char)   \
+  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, sycl::cl_short)  \
+  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, sycl::cl_int)    \
+  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, sycl::cl_long)
 #define SYCL_SPECIALIZE_SIGNED_INTEGER_TYPES_UNARY(NAME, FUNC) \
-  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_char)   \
-  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_short)  \
-  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_int)    \
-  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_long)
+  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, sycl::cl_char)   \
+  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, sycl::cl_short)  \
+  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, sycl::cl_int)    \
+  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, sycl::cl_long)
 #define SYCL_SPECIALIZE_UNSIGNED_INTEGER_TYPES_BINARY(NAME, FUNC) \
-  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_uchar)  \
-  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_ushort) \
-  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_uint)   \
-  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_ulong)
+  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, sycl::cl_uchar)  \
+  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, sycl::cl_ushort) \
+  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, sycl::cl_uint)   \
+  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, sycl::cl_ulong)
 #define SYCL_SPECIALIZE_UNSIGNED_INTEGER_TYPES_UNARY(NAME, FUNC) \
-  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_uchar)  \
-  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_ushort) \
-  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_uint)   \
-  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_ulong)
+  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, sycl::cl_uchar)  \
+  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, sycl::cl_ushort) \
+  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, sycl::cl_uint)   \
+  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, sycl::cl_ulong)
 #define SYCL_SPECIALIZE_INTEGER_TYPES_BINARY(NAME, FUNC) \
   SYCL_SPECIALIZE_SIGNED_INTEGER_TYPES_BINARY(NAME, FUNC) \
   SYCL_SPECIALIZE_UNSIGNED_INTEGER_TYPES_BINARY(NAME, FUNC)
@@ -1075,20 +1075,20 @@ EIGEN_ALWAYS_INLINE long double maxi(const long double& x, const long double& y)
   SYCL_SPECIALIZE_SIGNED_INTEGER_TYPES_UNARY(NAME, FUNC) \
   SYCL_SPECIALIZE_UNSIGNED_INTEGER_TYPES_UNARY(NAME, FUNC)
 #define SYCL_SPECIALIZE_FLOATING_TYPES_BINARY(NAME, FUNC) \
-  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_float) \
-  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC,cl::sycl::cl_double)
+  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, sycl::cl_float) \
+  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC,sycl::cl_double)
 #define SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(NAME, FUNC) \
-  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_float) \
-  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC,cl::sycl::cl_double)
+  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, sycl::cl_float) \
+  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC,sycl::cl_double)
 #define SYCL_SPECIALIZE_FLOATING_TYPES_UNARY_FUNC_RET_TYPE(NAME, FUNC, RET_TYPE) \
-  SYCL_SPECIALIZE_GEN_UNARY_FUNC(NAME, FUNC, RET_TYPE, cl::sycl::cl_float) \
-  SYCL_SPECIALIZE_GEN_UNARY_FUNC(NAME, FUNC, RET_TYPE, cl::sycl::cl_double)
+  SYCL_SPECIALIZE_GEN_UNARY_FUNC(NAME, FUNC, RET_TYPE, sycl::cl_float) \
+  SYCL_SPECIALIZE_GEN_UNARY_FUNC(NAME, FUNC, RET_TYPE, sycl::cl_double)
 
 #define SYCL_SPECIALIZE_GEN_UNARY_FUNC(NAME, FUNC, RET_TYPE, ARG_TYPE) \
 template<>                                               \
   EIGEN_DEVICE_FUNC                                      \
   EIGEN_ALWAYS_INLINE RET_TYPE NAME(const ARG_TYPE& x) { \
-    return cl::sycl::FUNC(x);                            \
+    return sycl::FUNC(x);                            \
   }
 
 #define SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, TYPE) \
@@ -1098,7 +1098,7 @@ template<>                                               \
   template<>                                                                  \
   EIGEN_DEVICE_FUNC                                                           \
   EIGEN_ALWAYS_INLINE RET_TYPE NAME(const ARG_TYPE1& x, const ARG_TYPE2& y) { \
-    return cl::sycl::FUNC(x, y);                                              \
+    return sycl::FUNC(x, y);                                              \
   }
 
 #define SYCL_SPECIALIZE_GEN2_BINARY_FUNC(NAME, FUNC, RET_TYPE, ARG_TYPE) \
diff --git a/Eigen/src/Core/NumTraits.h b/Eigen/src/Core/NumTraits.h
index 9ab55534f..89237db7c 100644
--- a/Eigen/src/Core/NumTraits.h
+++ b/Eigen/src/Core/NumTraits.h
@@ -77,6 +77,27 @@ struct default_digits_impl<T,false,true> // Integer
 
 } // end namespace internal
 
+namespace numext {
+/** \internal bit-wise cast without changing the underlying bit representation. */
+
+// TODO: Replace by std::bit_cast (available in C++20)
+template <typename Tgt, typename Src>
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Tgt bit_cast(const Src& src) {
+#if EIGEN_HAS_TYPE_TRAITS
+  // The behaviour of memcpy is not specified for non-trivially copyable types
+  EIGEN_STATIC_ASSERT(std::is_trivially_copyable<Src>::value, THIS_TYPE_IS_NOT_SUPPORTED);
+  EIGEN_STATIC_ASSERT(std::is_trivially_copyable<Tgt>::value && std::is_default_constructible<Tgt>::value,
+                      THIS_TYPE_IS_NOT_SUPPORTED);
+#endif
+
+  EIGEN_STATIC_ASSERT(sizeof(Src) == sizeof(Tgt), THIS_TYPE_IS_NOT_SUPPORTED);
+  Tgt tgt;
+  EIGEN_USING_STD(memcpy)
+  memcpy(&tgt, &src, sizeof(Tgt));
+  return tgt;
+}
+}  // namespace numext
+
 /** \class NumTraits
   * \ingroup Core_Module
   *
diff --git a/Eigen/src/Core/arch/AVX/PacketMath.h b/Eigen/src/Core/arch/AVX/PacketMath.h
index cf7146cbc..4dc4f6931 100644
--- a/Eigen/src/Core/arch/AVX/PacketMath.h
+++ b/Eigen/src/Core/arch/AVX/PacketMath.h
@@ -1093,12 +1093,7 @@ EIGEN_STRONG_INLINE Packet8f Bf16ToF32(const Packet8bf& a) {
 EIGEN_STRONG_INLINE Packet8bf F32ToBf16(const Packet8f& a) {
   Packet8bf r;
 
-  // Flush input denormals value to zero with hardware capability.
-  _MM_SET_DENORMALS_ZERO_MODE(_MM_DENORMALS_ZERO_ON);
-  __m256 flush = _mm256_and_ps(a, a);
-  _MM_SET_DENORMALS_ZERO_MODE(_MM_DENORMALS_ZERO_OFF);
-
-  __m256i input = _mm256_castps_si256(flush);
+  __m256i input = _mm256_castps_si256(a);
 
 #ifdef EIGEN_VECTORIZE_AVX2
   // uint32_t lsb = (input >> 16);
@@ -1112,7 +1107,7 @@ EIGEN_STRONG_INLINE Packet8bf F32ToBf16(const Packet8f& a) {
   // input = input >> 16;
   t = _mm256_srli_epi32(t, 16);
   // Check NaN before converting back to bf16
-  __m256 mask = _mm256_cmp_ps(flush, flush, _CMP_ORD_Q);
+  __m256 mask = _mm256_cmp_ps(a, a, _CMP_ORD_Q);
   __m256i nan = _mm256_set1_epi32(0x7fc0);
   t = _mm256_blendv_epi8(nan, t, _mm256_castps_si256(mask));
   // output.value = static_cast<uint16_t>(input);
@@ -1135,7 +1130,7 @@ EIGEN_STRONG_INLINE Packet8bf F32ToBf16(const Packet8f& a) {
   lo = _mm_srli_epi32(lo, 16);
   hi = _mm_srli_epi32(hi, 16);
   // Check NaN before converting back to bf16
-  __m256 mask = _mm256_cmp_ps(flush, flush, _CMP_ORD_Q);
+  __m256 mask = _mm256_cmp_ps(a, a, _CMP_ORD_Q);
   __m128i nan = _mm_set1_epi32(0x7fc0);
   lo = _mm_blendv_epi8(nan, lo, _mm_castps_si128(_mm256_castps256_ps128(mask)));
   hi = _mm_blendv_epi8(nan, hi, _mm_castps_si128(_mm256_extractf128_ps(mask, 1)));
diff --git a/Eigen/src/Core/arch/AVX512/PacketMath.h b/Eigen/src/Core/arch/AVX512/PacketMath.h
index 76f3366d7..e4cbafc05 100644
--- a/Eigen/src/Core/arch/AVX512/PacketMath.h
+++ b/Eigen/src/Core/arch/AVX512/PacketMath.h
@@ -1731,23 +1731,14 @@ EIGEN_STRONG_INLINE Packet16f Bf16ToF32(const Packet16bf& a) {
 EIGEN_STRONG_INLINE Packet16bf F32ToBf16(const Packet16f& a) {
   Packet16bf r;
 
-  // Flush input denormals value to zero with hardware capability.
-  _MM_SET_DENORMALS_ZERO_MODE(_MM_DENORMALS_ZERO_ON);
-#if defined(EIGEN_VECTORIZE_AVX512DQ)
-  __m512 flush = _mm512_and_ps(a, a);
-#else
-  __m512 flush = _mm512_max_ps(a, a);
-#endif // EIGEN_VECTORIZE_AVX512DQ
-  _MM_SET_DENORMALS_ZERO_MODE(_MM_DENORMALS_ZERO_OFF);
-
 #if defined(EIGEN_VECTORIZE_AVX512BF16) && EIGEN_GNUC_AT_LEAST(10, 1)
   // Since GCC 10.1 supports avx512bf16 and C style explicit cast
   // (C++ static_cast is not supported yet), do converion via intrinsic
   // and register path for performance.
-  r = (__m256i)(_mm512_cvtneps_pbh(flush));
+  r = (__m256i)(_mm512_cvtneps_pbh(a));
 #else
   __m512i t;
-  __m512i input = _mm512_castps_si512(flush);
+  __m512i input = _mm512_castps_si512(a);
   __m512i nan = _mm512_set1_epi32(0x7fc0);
 
   // uint32_t lsb = (input >> 16) & 1;
@@ -1760,7 +1751,7 @@ EIGEN_STRONG_INLINE Packet16bf F32ToBf16(const Packet16f& a) {
   t = _mm512_srli_epi32(t, 16);
 
   // Check NaN before converting back to bf16
-  __mmask16 mask = _mm512_cmp_ps_mask(flush, flush, _CMP_ORD_Q);
+  __mmask16 mask = _mm512_cmp_ps_mask(a, a, _CMP_ORD_Q);
   t = _mm512_mask_blend_epi32(mask, nan, t);
 
   // output.value = static_cast<uint16_t>(input);
diff --git a/Eigen/src/Core/arch/DPCPP/BFloat16.h b/Eigen/src/Core/arch/DPCPP/BFloat16.h
new file mode 100644
index 000000000..f790d8ce6
--- /dev/null
+++ b/Eigen/src/Core/arch/DPCPP/BFloat16.h
@@ -0,0 +1,694 @@
+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef EIGEN_BFLOAT16_H
+#define EIGEN_BFLOAT16_H
+
+#ifdef DPCPP_DEVICE_ONLY
+
+namespace Eigen {
+
+struct bfloat16;
+
+namespace bfloat16_impl {
+
+// Make our own __bfloat16_raw definition.
+struct __bfloat16_raw {
+  EIGEN_DEVICE_FUNC __bfloat16_raw() : value(0) {}
+  explicit EIGEN_DEVICE_FUNC __bfloat16_raw(unsigned short raw) : value(raw) {}
+  unsigned short value;
+};
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw raw_uint16_to_bfloat16(unsigned short value);
+template <bool AssumeArgumentIsNormalOrInfinityOrZero>
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw float_to_bfloat16_rtne(float ff);
+// Forward declarations of template specializations, to avoid Visual C++ 2019 errors, saying:
+// > error C2908: explicit specialization; 'float_to_bfloat16_rtne' has already been instantiated
+template <>
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw float_to_bfloat16_rtne<false>(float ff);
+template <>
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw float_to_bfloat16_rtne<true>(float ff);
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC float bfloat16_to_float(__bfloat16_raw h);
+
+struct bfloat16_base : public __bfloat16_raw {
+  EIGEN_DEVICE_FUNC bfloat16_base() {}
+  EIGEN_DEVICE_FUNC bfloat16_base(const __bfloat16_raw& h) : __bfloat16_raw(h) {}
+};
+
+} // namespace bfloat16_impl
+
+// Class definition.
+struct bfloat16 : public bfloat16_impl::bfloat16_base {
+
+  typedef bfloat16_impl::__bfloat16_raw __bfloat16_raw;
+
+  EIGEN_DEVICE_FUNC bfloat16() {}
+
+  EIGEN_DEVICE_FUNC bfloat16(const __bfloat16_raw& h) : bfloat16_impl::bfloat16_base(h) {}
+
+  explicit EIGEN_DEVICE_FUNC bfloat16(bool b)
+      : bfloat16_impl::bfloat16_base(bfloat16_impl::raw_uint16_to_bfloat16(b ? 0x3f80 : 0)) {}
+
+  template<class T>
+  explicit EIGEN_DEVICE_FUNC bfloat16(const T& val)
+      : bfloat16_impl::bfloat16_base(bfloat16_impl::float_to_bfloat16_rtne<internal::is_integral<T>::value>(static_cast<float>(val))) {}
+  
+  explicit EIGEN_DEVICE_FUNC bfloat16(float f)
+      : bfloat16_impl::bfloat16_base(bfloat16_impl::float_to_bfloat16_rtne<false>(f)) {}
+
+  // Following the convention of numpy, converting between complex and
+  // float will lead to loss of imag value.
+  template<typename RealScalar>
+  explicit EIGEN_DEVICE_FUNC bfloat16(const std::complex<RealScalar>& val)
+      : bfloat16_impl::bfloat16_base(bfloat16_impl::float_to_bfloat16_rtne<false>(static_cast<float>(val.real()))) {}
+
+  EIGEN_DEVICE_FUNC operator float() const {  // NOLINT: Allow implicit conversion to float, because it is lossless.
+    return bfloat16_impl::bfloat16_to_float(*this);
+  }
+
+#if EIGEN_HAS_CXX11
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(bool) const {
+    // +0.0 and -0.0 become false, everything else becomes true.
+    return (value & 0x7fff) != 0;
+  }
+#endif 
+
+};
+} // namespace Eigen
+
+namespace std {
+template<>
+struct numeric_limits<Eigen::bfloat16> {
+  static const bool is_specialized = true;
+  static const bool is_signed = true;
+  static const bool is_integer = false;
+  static const bool is_exact = false;
+  static const bool has_infinity = true;
+  static const bool has_quiet_NaN = true;
+  static const bool has_signaling_NaN = true;
+  static const float_denorm_style has_denorm = numeric_limits<float>::has_denorm;
+  static const bool has_denorm_loss = numeric_limits<float>::has_denorm_loss;
+  static const std::float_round_style round_style = numeric_limits<float>::round_style;
+  static const bool is_iec559 = false;
+  static const bool is_bounded = true;
+  static const bool is_modulo = false;
+  static const int digits = 8;
+  static const int digits10 = 2;
+  static const int max_digits10 = 4;
+  static const int radix = 2;
+  static const int min_exponent = numeric_limits<float>::min_exponent;
+  static const int min_exponent10 = numeric_limits<float>::min_exponent10;
+  static const int max_exponent = numeric_limits<float>::max_exponent;
+  static const int max_exponent10 = numeric_limits<float>::max_exponent10;
+  static const bool traps = numeric_limits<float>::traps;
+  static const bool tinyness_before = numeric_limits<float>::tinyness_before;
+
+  static Eigen::bfloat16 (min)() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x0080); }
+  static Eigen::bfloat16 lowest() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0xff7f); }
+  static Eigen::bfloat16 (max)() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x7f7f); }
+  static Eigen::bfloat16 epsilon() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x3c00); }
+  static Eigen::bfloat16 round_error() { return Eigen::bfloat16(0x3f00); }
+  static Eigen::bfloat16 infinity() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x7f80); }
+  static Eigen::bfloat16 quiet_NaN() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x7fc0); }
+  static Eigen::bfloat16 signaling_NaN() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x7f81); }
+  static Eigen::bfloat16 denorm_min() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x0001); }
+};
+
+// If std::numeric_limits<T> is specialized, should also specialize
+// std::numeric_limits<const T>, std::numeric_limits<volatile T>, and
+// std::numeric_limits<const volatile T>
+// https://stackoverflow.com/a/16519653/
+template<>
+struct numeric_limits<const Eigen::bfloat16> : numeric_limits<Eigen::bfloat16> {};
+template<>
+struct numeric_limits<volatile Eigen::bfloat16> : numeric_limits<Eigen::bfloat16> {};
+template<>
+struct numeric_limits<const volatile Eigen::bfloat16> : numeric_limits<Eigen::bfloat16> {};
+} // namespace std
+
+namespace Eigen {
+
+namespace bfloat16_impl {
+
+#if !defined(EIGEN_HAS_NATIVE_BF16)
+
+// Definitions for CPUs, mostly working through conversion
+// to/from fp32.
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator + (const bfloat16& a, const bfloat16& b) {
+  return bfloat16(float(a) + float(b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator + (const bfloat16& a, const int& b) {
+  return bfloat16(float(a) + static_cast<float>(b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator + (const int& a, const bfloat16& b) {
+  return bfloat16(static_cast<float>(a) + float(b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator * (const bfloat16& a, const bfloat16& b) {
+  return bfloat16(float(a) * float(b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator - (const bfloat16& a, const bfloat16& b) {
+  return bfloat16(float(a) - float(b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator / (const bfloat16& a, const bfloat16& b) {
+  return bfloat16(float(a) / float(b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator - (const bfloat16& a) {
+  bfloat16 result;
+  result.value = a.value ^ 0x8000;
+  return result;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16& operator += (bfloat16& a, const bfloat16& b) {
+  a = bfloat16(float(a) + float(b));
+  return a;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16& operator *= (bfloat16& a, const bfloat16& b) {
+  a = bfloat16(float(a) * float(b));
+  return a;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16& operator -= (bfloat16& a, const bfloat16& b) {
+  a = bfloat16(float(a) - float(b));
+  return a;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16& operator /= (bfloat16& a, const bfloat16& b) {
+  a = bfloat16(float(a) / float(b));
+  return a;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator++(bfloat16& a) {
+  a += bfloat16(1);
+  return a;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator--(bfloat16& a) {
+  a -= bfloat16(1);
+  return a;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator++(bfloat16& a, int) {
+  bfloat16 original_value = a;
+  ++a;
+  return original_value;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator--(bfloat16& a, int) {
+  bfloat16 original_value = a;
+  --a;
+  return original_value;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator == (const bfloat16& a, const bfloat16& b) {
+  return numext::equal_strict(float(a),float(b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator != (const bfloat16& a, const bfloat16& b) {
+  return numext::not_equal_strict(float(a), float(b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator < (const bfloat16& a, const bfloat16& b) {
+  return float(a) < float(b);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator <= (const bfloat16& a, const bfloat16& b) {
+  return float(a) <= float(b);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator > (const bfloat16& a, const bfloat16& b) {
+  return float(a) > float(b);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator >= (const bfloat16& a, const bfloat16& b) {
+  return float(a) >= float(b);
+}
+
+#endif  // Emulate support for bfloat16 floats
+
+// Division by an index. Do it in full float precision to avoid accuracy
+// issues in converting the denominator to bfloat16.
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator / (const bfloat16& a, Index b) {
+  return bfloat16(static_cast<float>(a) / static_cast<float>(b));
+}
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw truncate_to_bfloat16(const float v) {
+  __bfloat16_raw output;
+  if (Eigen::numext::isnan EIGEN_NOT_A_MACRO(v)) {
+    output.value = 0x7FC0;
+    return output;
+  } else if (sycl::fabs(v) < std::numeric_limits<float>::min EIGEN_NOT_A_MACRO()) {
+    // Flush denormal to +/- 0.
+    output.value = sycl::signbit(v) ? 0x8000 : 0;
+    return output;
+  }
+  const uint16_t* p = reinterpret_cast<const uint16_t*>(&v);
+#if defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
+  output.value = p[0];
+#else
+  output.value = p[1];
+#endif
+  return output;
+}
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw raw_uint16_to_bfloat16(unsigned short value) {
+  __bfloat16_raw h;
+  h.value = value;
+  return h;
+}
+
+// float_to_bfloat16_rtne template specialization that does not make any
+// assumption about the value of its function argument (ff).
+template <>
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw float_to_bfloat16_rtne<false>(float ff) {
+  __bfloat16_raw output;
+
+  if (Eigen::numext::isnan EIGEN_NOT_A_MACRO(ff)) {
+    // If the value is a NaN, squash it to a qNaN with msb of fraction set,
+    // this makes sure after truncation we don't end up with an inf.
+    //
+    // qNaN magic: All exponent bits set + most significant bit of fraction
+    // set.
+    output.value = 0x7fc0;
+  } else if (sycl::fabs(ff) < std::numeric_limits<float>::min EIGEN_NOT_A_MACRO()) {
+    // Flush denormal to +/- 0.0
+    output.value = sycl::signbit(ff) ? 0x8000 : 0;
+  } else {
+    // Fast rounding algorithm that rounds a half value to nearest even. This
+    // reduces expected error when we convert a large number of floats. Here
+    // is how it works:
+    //
+    // Definitions:
+    // To convert a float 32 to bfloat16, a float 32 can be viewed as 32 bits
+    // with the following tags:
+    //
+    // Sign |  Exp (8 bits) | Frac (23 bits)
+    //  S     EEEEEEEE         FFFFFFLRTTTTTTTTTTTTTTT
+    //
+    //  S: Sign bit.
+    //  E: Exponent bits.
+    //  F: First 6 bits of fraction.
+    //  L: Least significant bit of resulting bfloat16 if we truncate away the
+    //  rest of the float32. This is also the 7th bit of fraction
+    //  R: Rounding bit, 8th bit of fraction.
+    //  T: Sticky bits, rest of fraction, 15 bits.
+    //
+    // To round half to nearest even, there are 3 cases where we want to round
+    // down (simply truncate the result of the bits away, which consists of
+    // rounding bit and sticky bits) and two cases where we want to round up
+    // (truncate then add one to the result).
+    //
+    // The fast converting algorithm simply adds lsb (L) to 0x7fff (15 bits of
+    // 1s) as the rounding bias, adds the rounding bias to the input, then
+    // truncates the last 16 bits away.
+    //
+    // To understand how it works, we can analyze this algorithm case by case:
+    //
+    // 1. L = 0, R = 0:
+    //   Expect: round down, this is less than half value.
+    //
+    //   Algorithm:
+    //   - Rounding bias: 0x7fff + 0 = 0x7fff
+    //   - Adding rounding bias to input may create any carry, depending on
+    //   whether there is any value set to 1 in T bits.
+    //   - R may be set to 1 if there is a carry.
+    //   - L remains 0.
+    //   - Note that this case also handles Inf and -Inf, where all fraction
+    //   bits, including L, R and Ts are all 0. The output remains Inf after
+    //   this algorithm.
+    //
+    // 2. L = 1, R = 0:
+    //   Expect: round down, this is less than half value.
+    //
+    //   Algorithm:
+    //   - Rounding bias: 0x7fff + 1 = 0x8000
+    //   - Adding rounding bias to input doesn't change sticky bits but
+    //   adds 1 to rounding bit.
+    //   - L remains 1.
+    //
+    // 3. L = 0, R = 1, all of T are 0:
+    //   Expect: round down, this is exactly at half, the result is already
+    //   even (L=0).
+    //
+    //   Algorithm:
+    //   - Rounding bias: 0x7fff + 0 = 0x7fff
+    //   - Adding rounding bias to input sets all sticky bits to 1, but
+    //   doesn't create a carry.
+    //   - R remains 1.
+    //   - L remains 0.
+    //
+    // 4. L = 1, R = 1:
+    //   Expect: round up, this is exactly at half, the result needs to be
+    //   round to the next even number.
+    //
+    //   Algorithm:
+    //   - Rounding bias: 0x7fff + 1 = 0x8000
+    //   - Adding rounding bias to input doesn't change sticky bits, but
+    //   creates a carry from rounding bit.
+    //   - The carry sets L to 0, creates another carry bit and propagate
+    //   forward to F bits.
+    //   - If all the F bits are 1, a carry then propagates to the exponent
+    //   bits, which then creates the minimum value with the next exponent
+    //   value. Note that we won't have the case where exponents are all 1,
+    //   since that's either a NaN (handled in the other if condition) or inf
+    //   (handled in case 1).
+    //
+    // 5. L = 0, R = 1, any of T is 1:
+    //   Expect: round up, this is greater than half.
+    //
+    //   Algorithm:
+    //   - Rounding bias: 0x7fff + 0 = 0x7fff
+    //   - Adding rounding bias to input creates a carry from sticky bits,
+    //   sets rounding bit to 0, then create another carry.
+    //   - The second carry sets L to 1.
+    //
+    // Examples:
+    //
+    //  Exact half value that is already even:
+    //    Input:
+    //    Sign |  Exp (8 bit)     | Frac (first 7 bit) | Frac (last 16 bit)
+    //     S     E E E E E E E E      F F F F F F L     RTTTTTTTTTTTTTTT
+    //     0     0 0 0 0 0 0 0 0      0 0 0 0 0 1 0     1000000000000000
+    //
+    //     This falls into case 3. We truncate the rest of 16 bits and no
+    //     carry is created into F and L:
+    //
+    //    Output:
+    //    Sign |  Exp (8 bit)     | Frac (first 7 bit)
+    //     S     E E E E E E E E      F F F F F F L
+    //     0     0 0 0 0 0 0 0 0      0 0 0 0 0 1 0
+    //
+    //  Exact half value, round to next even number:
+    //    Input:
+    //    Sign |  Exp (8 bit)     | Frac (first 7 bit) | Frac (last 16 bit)
+    //     S     E E E E E E E E      F F F F F F L     RTTTTTTTTTTTTTTT
+    //     0     0 0 0 0 0 0 0 0      0 0 0 0 0 0 1     1000000000000000
+    //
+    //     This falls into case 4. We create a carry from R and T,
+    //     which then propagates into L and F:
+    //
+    //    Output:
+    //    Sign |  Exp (8 bit)     | Frac (first 7 bit)
+    //     S     E E E E E E E E      F F F F F F L
+    //     0     0 0 0 0 0 0 0 0      0 0 0 0 0 1 0
+    //
+    //
+    //  Max denormal value round to min normal value:
+    //    Input:
+    //    Sign |  Exp (8 bit)     | Frac (first 7 bit) | Frac (last 16 bit)
+    //     S     E E E E E E E E      F F F F F F L     RTTTTTTTTTTTTTTT
+    //     0     0 0 0 0 0 0 0 0      1 1 1 1 1 1 1     1111111111111111
+    //
+    //     This falls into case 4. We create a carry from R and T,
+    //     propagate into L and F, which then propagates into exponent
+    //     bits:
+    //
+    //    Output:
+    //    Sign |  Exp (8 bit)     | Frac (first 7 bit)
+    //     S     E E E E E E E E      F F F F F F L
+    //     0     0 0 0 0 0 0 0 1      0 0 0 0 0 0 0
+    //
+    //  Max normal value round to Inf:
+    //    Input:
+    //    Sign |  Exp (8 bit)     | Frac (first 7 bit) | Frac (last 16 bit)
+    //     S     E E E E E E E E      F F F F F F L     RTTTTTTTTTTTTTTT
+    //     0     1 1 1 1 1 1 1 0      1 1 1 1 1 1 1     1111111111111111
+    //
+    //     This falls into case 4. We create a carry from R and T,
+    //     propagate into L and F, which then propagates into exponent
+    //     bits:
+    //
+    //    Sign |  Exp (8 bit)     | Frac (first 7 bit)
+    //     S     E E E E E E E E      F F F F F F L
+    //     0     1 1 1 1 1 1 1 1      0 0 0 0 0 0 0
+
+    // At this point, ff must be either a normal float, or +/-infinity.
+    output = float_to_bfloat16_rtne<true>(ff);
+  }
+  return output;
+}
+
+// float_to_bfloat16_rtne template specialization that assumes that its function
+// argument (ff) is either a normal floating point number, or +/-infinity, or
+// zero. Used to improve the runtime performance of conversion from an integer
+// type to bfloat16.  
+template <>
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw float_to_bfloat16_rtne<true>(float ff) {
+    unsigned int input = numext::as_uint(ff);
+    __bfloat16_raw output;
+
+    // Least significant bit of resulting bfloat.
+    unsigned int lsb = (input >> 16) & 1;
+    unsigned int rounding_bias = 0x7fff + lsb;
+    input += rounding_bias;
+    output.value = static_cast<unsigned short>(input >> 16);
+    return output;
+}
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC float bfloat16_to_float(__bfloat16_raw h) {
+    float result = 0;
+    unsigned short* q = reinterpret_cast<unsigned short*>(&result);
+#if defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
+    q[0] = h.value;
+#else
+    q[1] = h.value;
+#endif
+    return result;
+}
+
+#ifndef EIGEN_DONT_VECTORIZE_SYCL
+
+template <int PACKET_SIZE>
+EIGEN_ALWAYS_INLINE EIGEN_DEVICE_FUNC sycl::vec<unsigned short, PACKET_SIZE> F32ToBf16(const sycl::vec<float, PACKET_SIZE>& a) {
+  sycl::vec<unsigned short, PACKET_SIZE> nan(0x7fc0); 
+  sycl::vec<short, PACKET_SIZE> isnan = sycl::isnan(a).template convert<short, sycl::rounding_mode::automatic>();
+  sycl::vec<unsigned int, PACKET_SIZE> input = *reinterpret_cast<const sycl::vec<unsigned int, PACKET_SIZE>*>(&a);
+  input = (input + ((input >> 16) & 1) + 0x7fff) >> 16;
+  return sycl::select(input.template convert<unsigned short, sycl::rounding_mode::automatic>(), nan, isnan);
+}
+
+template <int PACKET_SIZE>
+EIGEN_ALWAYS_INLINE EIGEN_DEVICE_FUNC sycl::vec<float, PACKET_SIZE> Bf16ToF32(const sycl::vec<unsigned short, PACKET_SIZE>& a) {
+  sycl::vec<unsigned int, PACKET_SIZE> r;
+  r = a.template convert<uint32_t, sycl::rounding_mode::automatic>() << 16;
+  return *reinterpret_cast<sycl::vec<float, PACKET_SIZE>*>(&r);
+}
+
+#endif // EIGEN_DONT_VECTORIZE_SYCL
+// --- standard functions ---
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool (isinf)(const bfloat16& a) {
+  return sycl::isinf EIGEN_NOT_A_MACRO(float(a));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool (isnan)(const bfloat16& a) {
+  return sycl::isnan EIGEN_NOT_A_MACRO(float(a));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool (isfinite)(const bfloat16& a) {
+  return !(isinf EIGEN_NOT_A_MACRO (a)) && !(isnan EIGEN_NOT_A_MACRO (a));
+}
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 abs(const bfloat16& a) {
+  bfloat16 result;
+  result.value = a.value & 0x7FFF;
+  return result;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 exp(const bfloat16& a) {
+   return bfloat16(sycl::exp(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 expm1(const bfloat16& a) {
+  return bfloat16(sycl::expm1(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 log(const bfloat16& a) {
+  return bfloat16(sycl::log(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 log1p(const bfloat16& a) {
+  return bfloat16(sycl::log1p(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 log10(const bfloat16& a) {
+  return bfloat16(sycl::log10(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 sqrt(const bfloat16& a) {
+    return bfloat16(sycl::sqrt(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 rint(const bfloat16& a) {
+    return bfloat16(sycl::rint(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 rsqrt(const bfloat16& a) {
+    return bfloat16(sycl::rsqrt(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 round(const bfloat16& a) {
+    return bfloat16(sycl::round(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 pow(const bfloat16& a, const bfloat16& b) {
+  return bfloat16(sycl::pow(float(a), float(b)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 sin(const bfloat16& a) {
+  return bfloat16(sycl::sin(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 cos(const bfloat16& a) {
+  return bfloat16(sycl::cos(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 tan(const bfloat16& a) {
+  return bfloat16(sycl::tan(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 asin(const bfloat16& a) {
+  return bfloat16(sycl::asin(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 acos(const bfloat16& a) {
+  return bfloat16(sycl::acos(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 atan(const bfloat16& a) {
+  return bfloat16(sycl::atan(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 sinh(const bfloat16& a) {
+  return bfloat16(sycl::sinh(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 cosh(const bfloat16& a) {
+  return bfloat16(sycl::cosh(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 tanh(const bfloat16& a) {
+  return bfloat16(sycl::tanh(float(a)));
+}
+#if EIGEN_HAS_CXX11_MATH
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 asinh(const bfloat16& a) {
+  return bfloat16(sycl::asinh(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 acosh(const bfloat16& a) {
+  return bfloat16(sycl::acosh(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 atanh(const bfloat16& a) {
+  return bfloat16(sycl::atanh(float(a)));
+}
+#endif
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 floor(const bfloat16& a) {
+  return bfloat16(sycl::floor(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 ceil(const bfloat16& a) {
+  return bfloat16(sycl::ceil(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 erfc(const bfloat16& a) {
+  return bfloat16(sycl::erfc(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 erf(const bfloat16& a) {
+  return bfloat16(sycl::erf(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 lgamma(const bfloat16& a) {
+  return bfloat16(sycl::lgamma(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 fmod(const bfloat16& a, const bfloat16& b) {
+  return bfloat16(sycl::fmod(float(a), float(b)));
+}
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 (min)(const bfloat16& a, const bfloat16& b) {
+  const float f1 = static_cast<float>(a);
+  const float f2 = static_cast<float>(b);
+  return f2 < f1 ? b : a;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 (max)(const bfloat16& a, const bfloat16& b) {
+  const float f1 = static_cast<float>(a);
+  const float f2 = static_cast<float>(b);
+  return f1 < f2 ? b : a;
+}
+
+#ifndef EIGEN_NO_IO
+EIGEN_ALWAYS_INLINE std::ostream& operator << (std::ostream& os, const bfloat16& v) {
+  os << static_cast<float>(v);
+  return os;
+}
+#endif
+
+} // namespace bfloat16_impl
+
+namespace internal {
+
+template<>
+struct random_default_impl<bfloat16, false, false>
+{
+  static inline bfloat16 run(const bfloat16& x, const bfloat16& y)
+  {
+    return x + (y-x) * bfloat16(float(std::rand()) / float(RAND_MAX));
+  }
+  static inline bfloat16 run()
+  {
+    return run(bfloat16(-1.f), bfloat16(1.f));
+  }
+};
+
+template<> struct is_arithmetic<bfloat16> { enum { value = true }; };
+
+} // namespace internal
+
+template<> struct NumTraits<Eigen::bfloat16>
+    : GenericNumTraits<Eigen::bfloat16>
+{
+  enum {
+    IsSigned = true,
+    IsInteger = false,
+    IsComplex = false,
+    RequireInitialization = false
+  };
+
+  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Eigen::bfloat16 epsilon() {
+    return bfloat16_impl::raw_uint16_to_bfloat16(0x3c00);
+  }
+  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Eigen::bfloat16 dummy_precision() { return Eigen::bfloat16(5e-2f); }
+  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Eigen::bfloat16 highest() {
+    return bfloat16_impl::raw_uint16_to_bfloat16(0x7F7F);
+  }
+  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Eigen::bfloat16 lowest() {
+    return bfloat16_impl::raw_uint16_to_bfloat16(0xFF7F);
+  }
+  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Eigen::bfloat16 infinity() {
+    return bfloat16_impl::raw_uint16_to_bfloat16(0x7f80);
+  }
+  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Eigen::bfloat16 quiet_NaN() {
+    return bfloat16_impl::raw_uint16_to_bfloat16(0x7fc0);
+  }
+};
+
+} // namespace Eigen
+
+namespace std {
+
+#if __cplusplus > 199711L
+template <>
+struct hash<Eigen::bfloat16> {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE std::size_t operator()(const Eigen::bfloat16& a) const {
+    return hash<float>()(static_cast<float>(a));
+  }
+};
+#endif
+
+} // namespace std
+
+
+namespace Eigen {
+namespace numext {
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
+bool (isnan)(const Eigen::bfloat16& h) {
+  return (bfloat16_impl::isnan)(h);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
+bool (isinf)(const Eigen::bfloat16& h) {
+  return (bfloat16_impl::isinf)(h);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
+bool (isfinite)(const Eigen::bfloat16& h) {
+  return (bfloat16_impl::isfinite)(h);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
+Eigen::bfloat16 (rint)(const Eigen::bfloat16& h) {
+  return (bfloat16_impl::rint)(h);
+}
+} // namespace numext
+}  // namespace Eigen
+
+#endif // DPCPP_DEVICE_ONLY
+#endif // EIGEN_BFLOAT16_H
diff --git a/Eigen/src/Core/arch/DPCPP/Half.h b/Eigen/src/Core/arch/DPCPP/Half.h
new file mode 100644
index 000000000..135b1b9c8
--- /dev/null
+++ b/Eigen/src/Core/arch/DPCPP/Half.h
@@ -0,0 +1,709 @@
+// This file is part of Eigen, a lightweight C++ template library
+// for linear algebra.
+//
+// This Source Code Form is subject to the terms of the Mozilla
+// Public License v. 2.0. If a copy of the MPL was not distributed
+// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
+//
+// The conversion routines are Copyright (c) Fabian Giesen, 2016.
+// The original license follows:
+//
+// Copyright (c) Fabian Giesen, 2016
+// All rights reserved.
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted.
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+// HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+
+// Standard 16-bit float type, mostly useful for GPUs. Defines a new
+// type Eigen::half (inheriting either from CUDA's or HIP's __half struct) with
+// operator overloads such that it behaves basically as an arithmetic
+// type. It will be quite slow on CPUs (so it is recommended to stay
+// in fp32 for CPUs, except for simple parameter conversions, I/O
+// to disk and the likes), but fast on GPUs.
+
+
+#ifndef EIGEN_HALF_DPCPP_H
+#define EIGEN_HALF_DPCPP_H
+
+#ifdef DPCPP_DEVICE_ONLY
+
+#if __cplusplus > 199711L
+#define EIGEN_EXPLICIT_CAST(tgt_type) explicit operator tgt_type()
+#else
+#define EIGEN_EXPLICIT_CAST(tgt_type) operator tgt_type()
+#endif
+
+
+namespace Eigen {
+
+struct half;
+
+typedef union _u16_to_half {
+  unsigned short u;
+  sycl::half h;
+} u16_to_sycl_half;
+
+namespace half_impl {
+
+struct __half_raw {
+  EIGEN_DEVICE_FUNC __half_raw() : x(0) {}
+  explicit EIGEN_DEVICE_FUNC __half_raw(sycl::half raw) : x(raw) {}
+  explicit EIGEN_DEVICE_FUNC __half_raw(float ff) : x(ff) {}
+  // ushort inintializer may be used elsewhere,
+  // so keep it for potential compatibility
+  explicit EIGEN_DEVICE_FUNC __half_raw(unsigned short raw) {
+    u16_to_sycl_half u2h;
+    u2h.u = raw;
+    x = u2h.u;
+  }
+  sycl::half x;
+};
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __half_raw raw_uint16_to_half(unsigned short x);
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __half_raw float_to_half_rtne(float ff);
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC float half_to_float(__half_raw h);
+
+struct half_base : public __half_raw {
+  EIGEN_DEVICE_FUNC half_base() {}
+  EIGEN_DEVICE_FUNC half_base(const __half_raw& h) : __half_raw(h) {}
+};
+
+} // namespace half_impl
+
+// Class definition.
+struct half : public half_impl::half_base {
+
+  EIGEN_DEVICE_FUNC half() {}
+
+  EIGEN_DEVICE_FUNC half(const __half_raw& h) : half_impl::half_base(h) {}
+
+  explicit EIGEN_DEVICE_FUNC half(float f) {
+    x = static_cast<sycl::half>(f);
+  }
+  // sycl c
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(bool) const {
+    // +0.0 and -0.0 become false, everything else becomes true.
+    return static_cast<bool>(x);
+  }
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(signed char) const {
+    return static_cast<signed char>(x);
+  }
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(unsigned char) const {
+    return static_cast<unsigned char>(x);
+  }
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(short) const {
+    return static_cast<short>(x);
+  }
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(unsigned short) const {
+    return static_cast<unsigned short>(x);
+  }
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(int) const {
+    return static_cast<int>(x);
+  }
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(unsigned int) const {
+    return static_cast<unsigned int>(x);
+  }
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(long) const {
+    return static_cast<long>(x);
+  }
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(unsigned long) const {
+    return static_cast<unsigned long>(x);
+  }
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(long long) const {
+    return static_cast<long long>(x);
+  }
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(unsigned long long) const {
+    return static_cast<unsigned long long>(x);
+  }
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(float) const {
+    return static_cast<float>(x);
+  }
+  EIGEN_DEVICE_FUNC EIGEN_EXPLICIT_CAST(double) const {
+    return static_cast<double>(x);
+  }
+
+  EIGEN_DEVICE_FUNC half& operator=(const half& other) {
+    x = other.x;
+    return *this;
+  }
+
+  EIGEN_DEVICE_FUNC half& operator=(const sycl::half& other) {
+    x = other;
+    return *this;
+  }
+
+};
+
+} // end namespace Eigen
+
+namespace std {
+template<>
+struct numeric_limits<Eigen::half> {
+  static const bool is_specialized = true;
+  static const bool is_signed = true;
+  static const bool is_integer = false;
+  static const bool is_exact = false;
+  static const bool has_infinity = true;
+  static const bool has_quiet_NaN = true;
+  static const bool has_signaling_NaN = true;
+  static const float_denorm_style has_denorm = denorm_present;
+  static const bool has_denorm_loss = false;
+  static const std::float_round_style round_style = std::round_to_nearest;
+  static const bool is_iec559 = false;
+  static const bool is_bounded = false;
+  static const bool is_modulo = false;
+  static const int digits = 11;
+  static const int digits10 = 3;      // according to http://half.sourceforge.net/structstd_1_1numeric__limits_3_01half__float_1_1half_01_4.html
+  static const int max_digits10 = 5;  // according to http://half.sourceforge.net/structstd_1_1numeric__limits_3_01half__float_1_1half_01_4.html
+  static const int radix = 2;
+  static const int min_exponent = -13;
+  static const int min_exponent10 = -4;
+  static const int max_exponent = 16;
+  static const int max_exponent10 = 4;
+  static const bool traps = true;
+  static const bool tinyness_before = false;
+
+  static Eigen::half (min)() { return Eigen::half_impl::raw_uint16_to_half(0x400); }
+  static Eigen::half lowest() { return Eigen::half_impl::raw_uint16_to_half(0xfbff); }
+  static Eigen::half (max)() { return Eigen::half_impl::raw_uint16_to_half(0x7bff); }
+  static Eigen::half epsilon() { return Eigen::half_impl::raw_uint16_to_half(0x0800); }
+  static Eigen::half round_error() { return Eigen::half(0.5); }
+  static Eigen::half infinity() { return Eigen::half_impl::raw_uint16_to_half(0x7c00); }
+  static Eigen::half quiet_NaN() { return Eigen::half_impl::raw_uint16_to_half(0x7e00); }
+  static Eigen::half signaling_NaN() { return Eigen::half_impl::raw_uint16_to_half(0x7e00); }
+  static Eigen::half denorm_min() { return Eigen::half_impl::raw_uint16_to_half(0x1); }
+
+};
+
+// If std::numeric_limits<T> is specialized, should also specialize
+// std::numeric_limits<const T>, std::numeric_limits<volatile T>, and
+// std::numeric_limits<const volatile T>
+// https://stackoverflow.com/a/16519653/
+template<>
+struct numeric_limits<const Eigen::half> : numeric_limits<Eigen::half> {};
+template<>
+struct numeric_limits<volatile Eigen::half> : numeric_limits<Eigen::half> {};
+template<>
+struct numeric_limits<const volatile Eigen::half> : numeric_limits<Eigen::half> {};
+} // end namespace std
+
+namespace Eigen {
+
+namespace half_impl {
+
+// sycl has these operator for half type
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator + (const half& a, const half& b) {
+  return half(a.x + b.x);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator * (const half& a, const half& b) {
+  return half(a.x * b.x);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator - (const half& a, const half& b) {
+  return half(a.x - b.x);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator / (const half& a, const half& b) {
+  return half(a.x / b.x);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator - (const half& a) {
+  half tmp;
+  tmp.x = - a.x;
+  return tmp;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half& operator += (half& a, const half& b) {
+  a.x += b.x;
+  return a;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half& operator *= (half& a, const half& b) {
+  a.x *= b.x;
+  return a;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half& operator -= (half& a, const half& b) {
+  a.x -= b.x;
+  return a;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half& operator /= (half& a, const half& b) {
+  a.x /= b.x;
+  return a;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator == (const half& a, const half& b) {
+  return a.x == b.x;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator != (const half& a, const half& b) {
+  return a.x != b.x;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator < (const half& a, const half& b) {
+  return a.x < b.x;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator <= (const half& a, const half& b) {
+  return a.x <= b.x;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator > (const half& a, const half& b) {
+  return a.x > b.x;
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator >= (const half& a, const half& b) {
+  return a.x >= b.x;
+}
+
+// Division by an index. Do it in full float precision to avoid accuracy
+// issues in converting the denominator to half.
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator / (const half& a, Index b) {
+  return half(static_cast<float>(a) / static_cast<float>(b));
+}
+
+// Conversion routines, including fallbacks for the host or older CUDA.
+// Note that newer Intel CPUs (Haswell or newer) have vectorized versions of
+// these in hardware. If we need more performance on older/other CPUs, they are
+// also possible to vectorize directly.
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __half_raw raw_uint16_to_half(unsigned short us) {
+  __half_raw h;
+  u16_to_sycl_half i;
+  i.u = us;
+  h.x = i.h;
+  return h;
+}
+
+union float32_bits {
+  unsigned int u;
+  float f;
+};
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __half_raw float_to_half_rtne(float ff) {
+  float32_bits f;
+  f.f = ff;
+
+  const float32_bits f32infty = { 255 << 23 };
+  const float32_bits f16max = { (127 + 16) << 23 };
+  const float32_bits denorm_magic = { ((127 - 15) + (23 - 10) + 1) << 23 };
+  unsigned int sign_mask = 0x80000000u;
+  __half_raw o;
+  u16_to_sycl_half u2h;
+  u2h.u = static_cast<unsigned short>(0x0u);
+
+  unsigned int sign = f.u & sign_mask;
+  f.u ^= sign;
+
+  // NOTE all the integer compares in this function can be safely
+  // compiled into signed compares since all operands are below
+  // 0x80000000. Important if you want fast straight SSE2 code
+  // (since there's no unsigned PCMPGTD).
+
+  if (f.u >= f16max.u) {  // result is Inf or NaN (all exponent bits set)
+    u2h.u = (f.u > f32infty.u) ? 0x7e00 : 0x7c00; // NaN->qNaN and Inf->Inf
+  } else {  // (De)normalized number or zero
+    if (f.u < (113 << 23)) {  // resulting FP16 is subnormal or zero
+      // use a magic value to align our 10 mantissa bits at the bottom of
+      // the float. as long as FP addition is round-to-nearest-even this
+      // just works.
+      f.f += denorm_magic.f;
+
+      // and one integer subtract of the bias later, we have our final float!
+      u2h.u = static_cast<unsigned short>(f.u - denorm_magic.u);
+    } else {
+      unsigned int mant_odd = (f.u >> 13) & 1; // resulting mantissa is odd
+
+      // update exponent, rounding bias part 1
+      f.u += ((unsigned int)(15 - 127) << 23) + 0xfff;
+      // rounding bias part 2
+      f.u += mant_odd;
+      // take the bits!
+      u2h.u = static_cast<unsigned short>(f.u >> 13);
+    }
+  }
+
+  u2h.u |= static_cast<unsigned short>(sign >> 16);
+  o.x = u2h.h;
+  return o;
+}
+
+// keep it for potential comptibility
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC float half_to_float(__half_raw h) {
+  return static_cast<float>(h.x);
+}
+
+// --- standard functions ---
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool (isinf)(const half& a) {
+  return sycl::isinf(a.x);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool (isnan)(const half& a) {
+  return sycl::isnan(a.x);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool (isfinite)(const half& a) {
+  return sycl::isfinite(a.x);
+}
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half acos(const half& a) {
+  return Eigen::half(sycl::acos(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half acosh(const half& a) {
+  return Eigen::half(sycl::acosh(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half acospi(const half& a) {
+  return Eigen::half(sycl::acospi(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half asin(const half& a) {
+  return Eigen::half(sycl::asin(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half asinh(const half& a) {
+  return Eigen::half(sycl::asinh(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half asinpi(const half& a) {
+  return Eigen::half(sycl::asinpi(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half atan(const half& a) {
+  return Eigen::half(sycl::atan(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half atan2(const half& a, const half& b) {
+  return Eigen::half(sycl::atan2(a.x, b.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half atanh(const half& a) {
+  return Eigen::half(sycl::atanh(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half atanpi(const half& a) {
+  return Eigen::half(sycl::atanpi(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half atan2pi(const half& a, const half& b) {
+  return Eigen::half(sycl::atan2pi(a.x, b.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half cbrt(const half& a) {
+  return Eigen::half(sycl::cbrt(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half ceil(const half& a) {
+  return Eigen::half(sycl::ceil(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half copysign(const half& a,const half& b) {
+  return Eigen::half(sycl::copysign(a.x, b.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half cos(const half& a) {
+  return Eigen::half(sycl::cos(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half cosh(const half& a) {
+  return Eigen::half(sycl::cosh(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half cospi(const half& a) {
+  return Eigen::half(sycl::cospi(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half erfc(const half& a) {
+  return Eigen::half(sycl::erfc(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half erf(const half& a) {
+  return Eigen::half(sycl::erf(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half exp(const half& a) {
+  return Eigen::half(sycl::exp(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half exp2(const half& a) {
+  return Eigen::half(sycl::exp2(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half exp10(const half& a) {
+  return Eigen::half(sycl::exp10(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half expm1(const half& a) {
+  return Eigen::half(sycl::expm1(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fabs(const half& a) {
+  return Eigen::half(sycl::fabs(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fdim(const half& a, const half& b) {
+  return Eigen::half(sycl::fdim(a.x, b.x));
+}
+
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half abs(const half& a) {
+  return Eigen::half(sycl::fabs(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half floor(const half& a) {
+  return Eigen::half(sycl::floor(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fma(const half& a, const half& b, const half& c) {
+  return Eigen::half(sycl::fma(a.x, b.x, c.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fmax(const half& a, const half& b) {
+  return Eigen::half(sycl::fmax(a.x, b.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fmin(const half& a, const half& b) {
+  return Eigen::half(sycl::fmin(a.x, b.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fmod(const half& a, const half& b) {
+  return Eigen::half(sycl::fmod(a.x, b.x));
+}
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fract(const half& a, const half* b) {
+//   return Eigen::half(sycl::fract(a.x, &(b->x)));
+// }
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fract(const half& a, sycl::cl_int* b) {
+//   return Eigen::half(sycl::fract(a.x, b));
+// }
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half hypot(const half& a, const half& b) {
+  return Eigen::half(sycl::hypot(a.x, b.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half ilogb(const half& a) {
+  return Eigen::half(sycl::ilogb(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half ldexp(const half& a, sycl::cl_int& b) {
+  return Eigen::half(sycl::ldexp(a.x, b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half lgamma(const half& a) {
+  return Eigen::half(sycl::lgamma(a.x));
+}
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half lgamma_r(const half& a, sycl::cl_int* b) {
+//   return Eigen::half(sycl::lgamma_r(a.x, b));
+// }
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half log(const half& a) {
+  return Eigen::half(sycl::log(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half log2(const half& a) {
+  return Eigen::half(sycl::log2(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half log10(const half& a) {
+  return Eigen::half(sycl::log10(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half log1p(const half& a) {
+  return Eigen::half(sycl::log1p(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half logb(const half& a) {
+  return Eigen::half(sycl::logb(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half mad(const half& a, const half& b, const half& c) {
+  return Eigen::half(sycl::mad(a.x, b.x, c.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half maxmag(const half& a, const half& b) {
+  return Eigen::half(sycl::maxmag(a.x, b.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half minmag(const half& a, const half& b) {
+  return Eigen::half(sycl::minmag(a.x, b.x));
+}
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half modf(const half& a, const half* b) {
+//   return Eigen::half(sycl::modf(a.x, &(b->x)));
+// }
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half nan(unsigned int u) {
+  return Eigen::half(sycl::nan(u));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half nextafter(const half& a, const half& b) {
+  return Eigen::half(sycl::nextafter(a.x, b.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half pow(const half& a, const half& b) {
+  return Eigen::half(sycl::pow(a.x, b.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half pow(const half& a, const sycl::cl_int& b) {
+  return Eigen::half(sycl::pown(a.x, b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half powr(const half& a, const half& b) {
+  return Eigen::half(sycl::powr(a.x, b.x));
+}
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half remquo(const half& a, const half& b, const sycl::cl_int* c) {
+//   return Eigen::half(sycl::remquo(a.x, b.x, c));
+// }
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half rint(const half& a) {
+  return Eigen::half(sycl::rint(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half rootn(const half& a, const sycl::cl_int& b) {
+  return Eigen::half(sycl::rootn(a.x, b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half round(const half& a) {
+  return Eigen::half(sycl::round(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half rsqrt(const half& a) {
+  return Eigen::half(sycl::rsqrt(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half sin(const half& a) {
+  return Eigen::half(sycl::sin(a.x));
+}
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half sincos(const half& a, const half* b) {
+//   return Eigen::half(sycl::sincos(a.x, b));
+// }
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half sinh(const half& a) {
+  return Eigen::half(sycl::sinh(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half sinpi(const half& a) {
+  return Eigen::half(sycl::sinpi(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half sqrt(const half& a) {
+  return Eigen::half(sycl::sqrt(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half tan(const half& a) {
+  return Eigen::half(sycl::tan(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half tanh(const half& a) {
+  return Eigen::half(sycl::tanh(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half tanpi(const half& a) {
+  return Eigen::half(sycl::tanpi(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half tgamma(const half& a) {
+  return Eigen::half(sycl::tgamma(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half trunc(const half& a) {
+  return Eigen::half(sycl::trunc(a.x));
+}
+// only enable for type T which is_genfloatf<T>::value is true
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half divide(const half& a, const half& b) {
+//   return Eigen::half(sycl::half_precision::divide(a.x, b.x));
+// }
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half recip(const half& a) {
+//   return Eigen::half(sycl::half_precision::recip(a.x));
+// }
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half clamp(const half& a, const half& b, const half& c) {
+  return Eigen::half(sycl::clamp(a.x, b.x, c.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half degrees(const half& a) {
+  return Eigen::half(sycl::degrees(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half radians(const half& a) {
+  return Eigen::half(sycl::radians(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half step(const half& a, const half& b) {
+  return Eigen::half(sycl::step(a.x, b.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half smoothstep(const half& a, const half& b, const half& c) {
+  return Eigen::half(sycl::smoothstep(a.x, b.x, c.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half sign(const half& a) {
+  return Eigen::half(sycl::sign(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half (min)(const half& a, const half& b) {
+  return Eigen::half(sycl::min(a.x, b.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half (max)(const half& a, const half& b) {
+  return Eigen::half(sycl::max(a.x, b.x));
+}
+
+#ifndef EIGEN_NO_IO
+EIGEN_ALWAYS_INLINE std::ostream& operator << (std::ostream& os, const half& v) {
+  os << static_cast<float>(v);
+  return os;
+}
+#endif
+
+} // end namespace half_impl
+
+// import Eigen::half_impl::half into Eigen namespace
+// using half_impl::half;
+
+namespace internal {
+
+template<>
+struct random_default_impl<half, false, false>
+{
+  static inline half run(const half& x, const half& y)
+  {
+    return x + (y-x) * half(float(std::rand()) / float(RAND_MAX));
+  }
+  static inline half run()
+  {
+    return run(half(-1.f), half(1.f));
+  }
+};
+
+template<> struct is_arithmetic<half> { enum { value = true }; };
+
+} // end namespace internal
+
+template<> struct NumTraits<Eigen::half>
+    : GenericNumTraits<Eigen::half>
+{
+  enum {
+    IsSigned = true,
+    IsInteger = false,
+    IsComplex = false,
+    RequireInitialization = false
+  };
+
+  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Eigen::half epsilon() {
+    return half_impl::raw_uint16_to_half(0x0800);
+  }
+  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Eigen::half dummy_precision() { return Eigen::half(1e-2f); }
+  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Eigen::half highest() {
+    return half_impl::raw_uint16_to_half(0x7bff);
+  }
+  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Eigen::half lowest() {
+    return half_impl::raw_uint16_to_half(0xfbff);
+  }
+  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Eigen::half infinity() {
+    return half_impl::raw_uint16_to_half(0x7c00);
+  }
+  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Eigen::half quiet_NaN() {
+    return half_impl::raw_uint16_to_half(0x7c01);
+  }
+};
+
+} // end namespace Eigen
+
+// C-like standard mathematical functions and trancendentals.
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Eigen::half fabsh(const Eigen::half& a) {
+  return Eigen::half(sycl::fabs(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Eigen::half exph(const Eigen::half& a) {
+  return Eigen::half(sycl::exp(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Eigen::half logh(const Eigen::half& a) {
+  return Eigen::half(sycl::log(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Eigen::half sqrth(const Eigen::half& a) {
+  return Eigen::half(sycl::sqrt(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Eigen::half powh(const Eigen::half& a, const Eigen::half& b) {
+  return Eigen::half(sycl::pow(a.x, b.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Eigen::half floorh(const Eigen::half& a) {
+  return Eigen::half(sycl::floor(a.x));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Eigen::half ceilh(const Eigen::half& a) {
+  return Eigen::half(sycl::ceil(a.x));
+}
+
+namespace std {
+
+#if __cplusplus > 199711L
+template <>
+struct hash<Eigen::half> {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE std::size_t operator()(const Eigen::half& a) const {
+    return static_cast<std::size_t>(a.x);
+  }
+};
+#endif
+
+} // end namespace std
+
+namespace Eigen {
+namespace numext {
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
+bool (isnan)(const Eigen::half& h) {
+  return sycl::isnan(h.x);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
+bool (isinf)(const Eigen::half& h) {
+  return sycl::isinf(h.x);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
+bool (isfinite)(const Eigen::half& h) {
+  return sycl::isfinite(h.x);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
+Eigen::half (rint)(const Eigen::half& h) {
+  return (half_impl::rint)(h);
+}
+} // namespace Eigen
+}  // namespace numext
+
+#endif //DPCPP_DEVICE_ONLY
+
+#endif // EIGEN_HALF_DPCPP_H
diff --git a/Eigen/src/Core/arch/DPCPP/InteropHeaders.h b/Eigen/src/Core/arch/DPCPP/InteropHeaders.h
new file mode 100644
index 000000000..0632da9e1
--- /dev/null
+++ b/Eigen/src/Core/arch/DPCPP/InteropHeaders.h
@@ -0,0 +1,518 @@
+// This file is part of Eigen, a lightweight C++ template library
+// for linear algebra.
+//
+// Mehdi Goli    Codeplay Software Ltd.
+// Ralph Potter  Codeplay Software Ltd.
+// Luke Iwanski  Codeplay Software Ltd.
+// Contact: <eigen@codeplay.com>
+//
+// This Source Code Form is subject to the terms of the Mozilla
+// Public License v. 2.0. If a copy of the MPL was not distributed
+// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+/*****************************************************************
+ * InteropHeaders.h
+ *
+ * \brief:
+ *  InteropHeaders
+ *
+ *****************************************************************/
+
+#ifndef EIGEN_INTEROP_HEADERS_DPCPP_H
+#define EIGEN_INTEROP_HEADERS_DPCPP_H
+
+namespace Eigen {
+
+#ifndef EIGEN_DONT_VECTORIZE_SYCL
+
+namespace internal {
+typedef sycl::ulonglong2 Packet4h2;
+typedef sycl::vec<bool,16> cl_bool16;
+
+#define PACKET_TYPE_ALIAS(length)                                 \
+  typedef sycl::vec<unsigned short, length> Packet##length##bf16; \
+  typedef sycl::vec<short , length> Packet##length##i16;          \
+  typedef sycl::vec<float, length> Packet##length##f32;           \
+  typedef sycl::vec<unsigned int, length> Packet##length##u32;    \
+  typedef sycl::vec<int, length> Packet##length##i32;
+
+PACKET_TYPE_ALIAS(2);
+PACKET_TYPE_ALIAS(4);
+PACKET_TYPE_ALIAS(8);
+#undef PACKET_TYPE_ALIAS
+
+template <bool is_vectorized, int lengths>
+struct dpcpp_packet_traits : default_packet_traits {
+  enum {
+    Vectorizable = 1,
+    AlignedOnScalar = 1,
+    size = lengths,
+
+    HasAdd       = 1,
+    HasSub       = 1,
+    HasShift     = 0,
+    HasMul       = 1,
+    HasNegate    = 1,
+    HasAbs       = 1,
+    HasArg       = 0,
+    HasAbs2      = 0,
+    HasAbsDiff   = 0,
+    HasMin       = 1,
+    HasMax       = 1,
+    HasConj      = 1,
+    HasBlend     = is_vectorized,
+    // This flag is used to indicate whether packet comparison is supported.
+    // pcmp_eq, pcmp_lt and pcmp_le should be defined for it to be true.
+    HasCmp       = is_vectorized,
+
+    HasDiv    = 1,
+    HasSqrt   = 1,
+    HasRsqrt  = 1,
+    HasExp    = 1,
+    HasExpm1  = 1,
+    HasLog    = 1,
+    HasLog1p  = 1,
+    HasLog10  = 1,
+    HasPow    = 1,
+
+    HasSin    = 1,
+    HasCos    = 1,
+    HasTan    = 1,
+    HasASin   = 1,
+    HasACos   = 1,
+    HasATan   = 1,
+    HasSinh   = 1,
+    HasCosh   = 1,
+    HasTanh   = 1,
+    HasLGamma = is_vectorized,
+    HasDiGamma = 0,
+    HasZeta = 0,
+    HasPolygamma = 0,
+    HasErf = is_vectorized,
+    HasErfc = is_vectorized,
+    HasNdtri = 0,
+    HasBessel = 0,
+    HasIGamma = 0,
+    HasIGammaDerA = 0,
+    HasGammaSampleDerAlpha = 0,
+    HasIGammac = 0,
+    HasBetaInc = 0,
+
+    HasRound  = 1,
+    HasRint   = 1,
+    HasFloor  = 1,
+    HasCeil   = 1,
+    HasSign   = is_vectorized,
+  };
+};
+
+#ifdef DPCPP_DEVICE_ONLY
+template<> struct unpacket_traits<Packet4h2> { typedef Eigen::half type; enum {size=8, alignment=Aligned16, vectorizable=true, masked_load_available=false, masked_store_available=false}; typedef Packet4h2 half; };
+template<> struct is_arithmetic<Packet4h2> { enum { value = true }; };
+
+template<> struct unpacket_traits<sycl::cl_half2> { typedef Eigen::half type; enum {size=2, alignment=Aligned16, vectorizable=true, masked_load_available=false, masked_store_available=false}; typedef sycl::cl_half2 half; };
+template<> struct is_arithmetic<sycl::cl_half2> { enum { value = true }; };
+
+template<> struct packet_traits<Eigen::half> : dpcpp_packet_traits<true, 8>
+{
+  typedef Packet4h2 type;
+  typedef Packet4h2 half;
+};
+template<> struct packet_traits<const Eigen::half> : dpcpp_packet_traits<true, 8> {
+  typedef Packet4h2 type;
+  typedef Packet4h2 half;
+};
+
+template<> struct unpacket_traits<Packet8bf16> { typedef Eigen::bfloat16 type; enum {size=8, alignment=Aligned16, vectorizable=true, masked_load_available=false, masked_store_available=false}; typedef  Packet8bf16 half; };
+template<> struct unpacket_traits<Packet4bf16> { typedef Eigen::bfloat16 type; enum {size=4, alignment=Aligned16, vectorizable=true, masked_load_available=false, masked_store_available=false}; typedef  Packet4bf16 half; };
+template<> struct unpacket_traits<Packet2bf16> { typedef Eigen::bfloat16 type; enum {size=2, alignment=Aligned16, vectorizable=true, masked_load_available=false, masked_store_available=false}; typedef  Packet2bf16 half; };
+
+template<> struct is_arithmetic<Packet8bf16> { enum { value = true }; };
+template<> struct is_arithmetic<Packet4bf16> { enum { value = true }; };
+template<> struct is_arithmetic<Packet2bf16> { enum { value = true }; };
+
+template<> struct packet_traits<Eigen::bfloat16> : dpcpp_packet_traits<true, 8> {
+  typedef Packet8bf16 type;
+  typedef Packet8bf16 half;
+};
+template<> struct packet_traits<const Eigen::bfloat16> : dpcpp_packet_traits<true, 8>
+{
+  typedef Packet8bf16 type;
+  typedef Packet8bf16 half;
+};
+
+
+#define DPCPP_PACKET_TRAITS(packet_type, is_vectorized, unpacket_type, lengths) \
+  template <>                                                                   \
+  struct packet_traits<unpacket_type>                                           \
+      : dpcpp_packet_traits<is_vectorized, lengths> {                           \
+    typedef packet_type type;                                                   \
+    typedef packet_type half;                                                   \
+  };
+
+DPCPP_PACKET_TRAITS(sycl::cl_float4, true, float, 4)
+DPCPP_PACKET_TRAITS(sycl::cl_float4, true, const float, 4)
+DPCPP_PACKET_TRAITS(cl_bool16, true, const bool, 16)
+DPCPP_PACKET_TRAITS(cl_bool16, true, bool, 16)
+#undef DPCPP_PACKET_TRAITS
+
+// Make sure this is only available when targeting a GPU: we don't want to
+// introduce conflicts between these packet_traits definitions and the ones
+// we'll use on the host side (SSE, AVX, ...)
+#define SYCL_ARITHMETIC(packet_type)  \
+  template <>                         \
+  struct is_arithmetic<packet_type> { \
+    enum { value = true };            \
+  };
+SYCL_ARITHMETIC(sycl::cl_float4)
+#undef SYCL_ARITHMETIC
+
+#define SYCL_UNPACKET_TRAITS(packet_type, unpacket_type, lengths)        \
+  template <>                                                            \
+  struct unpacket_traits<packet_type> {                                  \
+    typedef unpacket_type type;                                          \
+    enum { size = lengths, vectorizable = true, alignment = Aligned16 }; \
+    typedef packet_type half;                                            \
+  };
+SYCL_UNPACKET_TRAITS(sycl::cl_float2, float, 2)
+SYCL_UNPACKET_TRAITS(sycl::cl_float4, float, 4)
+SYCL_UNPACKET_TRAITS(sycl::cl_float8, float, 8)
+SYCL_UNPACKET_TRAITS(cl_bool16, bool, 16)
+
+#undef SYCL_UNPACKET_TRAITS
+#else
+typedef sycl::ulonglong2 Packet4h2;
+ template<> struct unpacket_traits<Packet4h2> { typedef Eigen::half type; enum {size=8, alignment=Aligned16, vectorizable=false, masked_load_available=false, masked_store_available=false}; typedef Packet4h2 half; };
+ template<> struct is_arithmetic<Packet4h2> { enum { value = true }; };
+
+ template<> struct unpacket_traits<sycl::cl_half2> { typedef Eigen::half type; enum {size=2, alignment=Aligned16, vectorizable=false, masked_load_available=false, masked_store_available=false}; typedef sycl::cl_half2 half; };
+ template<> struct is_arithmetic<sycl::cl_half2> { enum { value = true }; };
+
+ template<> struct packet_traits<Eigen::half> : default_packet_traits
+ {
+   typedef Packet4h2 type;
+   typedef Packet4h2 half;
+   enum {
+     Vectorizable = 0,
+     AlignedOnScalar = 1,
+     size=8,
+     HasHalfPacket = 0,
+     HasAdd    = 1,
+     HasSub    = 1,
+     HasMul    = 1,
+     HasDiv    = 1,
+     HasSqrt   = 1,
+     HasRsqrt  = 1,
+     HasExp    = 1,
+     HasExpm1  = 1,
+     HasLog    = 1,
+     HasLog1p  = 1
+   };
+ };
+ template<> struct packet_traits<const Eigen::half> : default_packet_traits
+ {
+   typedef Packet4h2 type;
+   typedef Packet4h2 half;
+   enum {
+     Vectorizable = 0,
+     AlignedOnScalar = 1,
+     size=8,
+     HasHalfPacket = 0,
+     HasAdd    = 1,
+     HasSub    = 1,
+     HasMul    = 1,
+     HasDiv    = 1,
+     HasSqrt   = 1,
+     HasRsqrt  = 1,
+     HasExp    = 1,
+     HasExpm1  = 1,
+     HasLog    = 1,
+     HasLog1p  = 1
+   };
+ };
+
+template<> struct unpacket_traits<Packet8bf16> { typedef Eigen::bfloat16 type; enum {size=8, alignment=Aligned16, vectorizable=false, masked_load_available=false, masked_store_available=false}; typedef  Packet8bf16 half; };
+template<> struct is_arithmetic<Packet8bf16> { enum { value = true }; };
+template<> struct packet_traits<Eigen::bfloat16> : default_packet_traits
+{
+  typedef Packet8bf16 type;
+  typedef Packet8bf16 half;
+  enum {
+    Vectorizable = 0,
+    AlignedOnScalar = 1,
+    size=8,
+    HasHalfPacket = 0,
+    HasAdd    = 1,
+    HasSub    = 1,
+    HasMul    = 1,
+    HasDiv    = 1,
+    HasSqrt   = 1,
+    HasRsqrt  = 1,
+    HasExp    = 1,
+    HasExpm1  = 1,
+    HasLog    = 1,
+    HasLog1p  = 1
+  };
+};
+template<> struct packet_traits<const Eigen::bfloat16> : default_packet_traits
+{
+  typedef Packet8bf16 type;
+  typedef Packet8bf16 half;
+  enum {
+    Vectorizable = 0,
+    AlignedOnScalar = 1,
+    size=8,
+    HasHalfPacket = 0,
+    HasAdd    = 1,
+    HasSub    = 1,
+    HasMul    = 1,
+    HasDiv    = 1,
+    HasSqrt   = 1,
+    HasRsqrt  = 1,
+    HasExp    = 1,
+    HasExpm1  = 1,
+    HasLog    = 1,
+    HasLog1p  = 1
+  };
+};
+
+#define PACKET_TRAITS_FOR_RINT(TYPE)                  \
+template <>                                           \
+struct packet_traits<TYPE> : default_packet_traits {  \
+  typedef TYPE type;                                  \
+  typedef TYPE half;                                  \
+  enum {                                              \
+    Vectorizable = 0,                                 \
+    size = 1,                                         \
+    AlignedOnScalar = 0,                              \
+    HasHalfPacket = 0,                                \
+  };                                                  \
+  enum {                                              \
+    HasAdd    = 0,                                    \
+    HasSub    = 0,                                    \
+    HasMul    = 0,                                    \
+    HasNegate = 0,                                    \
+    HasAbs    = 0,                                    \
+    HasAbs2   = 0,                                    \
+    HasMin    = 0,                                    \
+    HasMax    = 0,                                    \
+    HasConj   = 0,                                    \
+    HasSetLinear = 0,                                 \
+    HasRint = 1,                                      \
+  };                                                  \
+};
+PACKET_TRAITS_FOR_RINT(float)
+#undef PACKET_TRAITS_FOR_RINT
+#endif // DPCPP_DEVICE_ONLY
+
+}  // end namespace internal
+
+#endif  // EIGEN_DONT_VECTORIZE_SYCL
+
+namespace internal {
+
+template <typename PacketReturnType, int PacketSize>
+struct PacketWrapper;
+// This function should never get called on the device
+#ifndef DPCPP_DEVICE_ONLY
+template <typename PacketReturnType, int PacketSize>
+struct PacketWrapper {
+  typedef typename ::Eigen::internal::unpacket_traits<PacketReturnType>::type
+      Scalar;
+  template <typename Index>
+  EIGEN_DEVICE_FUNC static Scalar scalarize(Index, PacketReturnType &) {
+    eigen_assert(false && "THERE IS NO PACKETIZE VERSION FOR  THE CHOSEN TYPE");
+    abort();
+  }
+  EIGEN_DEVICE_FUNC static PacketReturnType convert_to_packet_type(Scalar in,
+                                                                   Scalar) {
+    return ::Eigen::internal::template plset<PacketReturnType>(in);
+  }
+  EIGEN_DEVICE_FUNC static void set_packet(PacketReturnType, Scalar *) {
+    eigen_assert(false && "THERE IS NO PACKETIZE VERSION FOR  THE CHOSEN TYPE");
+    abort();
+  }
+};
+
+#elif defined(DPCPP_DEVICE_ONLY)
+template <typename PacketReturnType>
+struct PacketWrapper<PacketReturnType, 4> {
+  typedef typename ::Eigen::internal::unpacket_traits<PacketReturnType>::type
+      Scalar;
+  template <typename Index>
+  EIGEN_DEVICE_FUNC static Scalar scalarize(Index index, PacketReturnType &in) {
+    switch (index) {
+      case 0:
+        return in.x();
+      case 1:
+        return in.y();
+      case 2:
+        return in.z();
+      case 3:
+        return in.w();
+      default:
+        eigen_assert(false && "INDEX MUST BE BETWEEN 0 and 3");
+        abort();
+    }
+    __builtin_unreachable();
+
+  }
+  EIGEN_DEVICE_FUNC static PacketReturnType convert_to_packet_type(
+      Scalar in, Scalar other) {
+    return PacketReturnType(in, other, other, other);
+  }
+  EIGEN_DEVICE_FUNC static void set_packet(PacketReturnType &lhs, Scalar *rhs) {
+    lhs = PacketReturnType(rhs[0], rhs[1], rhs[2], rhs[3]);
+  }
+};
+
+template <typename PacketReturnType>
+struct PacketWrapper<PacketReturnType, 1> {
+  typedef typename ::Eigen::internal::unpacket_traits<PacketReturnType>::type
+      Scalar;
+  template <typename Index>
+  EIGEN_DEVICE_FUNC static Scalar scalarize(Index, PacketReturnType &in) {
+    return in;
+  }
+  EIGEN_DEVICE_FUNC static PacketReturnType convert_to_packet_type(Scalar in,
+                                                                   Scalar) {
+    return PacketReturnType(in);
+  }
+  EIGEN_DEVICE_FUNC static void set_packet(PacketReturnType &lhs, Scalar *rhs) {
+    lhs = rhs[0];
+  }
+};
+
+template <typename PacketReturnType>
+struct PacketWrapper<PacketReturnType, 2> {
+  typedef typename ::Eigen::internal::unpacket_traits<PacketReturnType>::type
+      Scalar;
+  template <typename Index>
+  EIGEN_DEVICE_FUNC static Scalar scalarize(Index index, PacketReturnType &in) {
+    switch (index) {
+      case 0:
+        return in.x();
+      case 1:
+        return in.y();
+      default:
+        eigen_assert(false && "INDEX MUST BE BETWEEN 0 and 1");
+        abort();
+    }
+    __builtin_unreachable();
+  }
+  EIGEN_DEVICE_FUNC static PacketReturnType convert_to_packet_type(
+      Scalar in, Scalar other) {
+    return PacketReturnType(in.x, other.x);
+  }
+  EIGEN_DEVICE_FUNC static void set_packet(PacketReturnType &lhs, Scalar *rhs) {
+    lhs = PacketReturnType(rhs[0].x, rhs[1].x);
+  }
+};
+
+#ifndef EIGEN_DONT_VECTORIZE_SYCL
+template <>
+struct PacketWrapper<Packet4h2, 8> {
+  typedef typename ::Eigen::internal::unpacket_traits<Packet4h2>::type
+      Scalar;
+  template <typename Index>
+  EIGEN_DEVICE_FUNC static Scalar scalarize(Index index, Packet4h2 &in) {
+    const sycl::cl_half2* in_alias = reinterpret_cast<const sycl::cl_half2*>(&in);
+    switch (index) {
+      case 0:
+        return Eigen::half(in_alias[0].x());
+      case 1:
+        return Eigen::half(in_alias[0].y());
+      case 2:
+        return Eigen::half(in_alias[1].x());
+      case 3:
+        return Eigen::half(in_alias[1].y());
+      case 4:
+        return Eigen::half(in_alias[2].x());
+      case 5:
+        return Eigen::half(in_alias[2].y());
+      case 6:
+        return Eigen::half(in_alias[3].x());
+      case 7:
+        return Eigen::half(in_alias[3].y());
+      default:
+        eigen_assert(false && "INDEX MUST BE BETWEEN 0 and 7!");
+        abort();
+    }
+    __builtin_unreachable();
+  }
+  EIGEN_DEVICE_FUNC static Packet4h2 convert_to_packet_type(
+      Scalar in, Scalar other) {
+    Packet4h2 res;
+    sycl::cl_half2* in_alias = reinterpret_cast<sycl::cl_half2*>(&res);
+    in_alias[0] = sycl::cl_half2(in.x, other.x);
+    in_alias[1] = sycl::cl_half2(in.x, other.x);
+    in_alias[2] = sycl::cl_half2(in.x, other.x);
+    in_alias[3] = sycl::cl_half2(in.x, other.x);
+
+    return res;
+  }
+  EIGEN_DEVICE_FUNC static void set_packet(Packet4h2 lhs, Scalar *rhs) {
+    sycl::cl_half2* lhs_alias = reinterpret_cast<sycl::cl_half2*>(&lhs);
+    lhs_alias[0] = sycl::cl_half2(rhs[0].x, rhs[1].x);
+    lhs_alias[1] = sycl::cl_half2(rhs[2].x, rhs[3].x);
+    lhs_alias[2] = sycl::cl_half2(rhs[4].x, rhs[5].x);
+    lhs_alias[3] = sycl::cl_half2(rhs[6].x, rhs[7].x);
+  }
+};
+
+template <>
+struct PacketWrapper<Packet8bf16, 8> {
+  typedef typename ::Eigen::internal::unpacket_traits<Packet8bf16>::type Scalar;
+  template <typename Index>
+  EIGEN_DEVICE_FUNC static Scalar scalarize(Index index, Packet8bf16 &in) {
+    switch (index) {
+      case 0:
+        return Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(in[0]));
+      case 1:
+        return Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(in[1]));
+      case 2:
+        return Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(in[2]));
+      case 3:
+        return Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(in[3]));
+      case 4:
+        return Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(in[4]));
+      case 5:
+        return Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(in[5]));
+      case 6:
+        return Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(in[6]));
+      case 7:
+        return Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(in[7]));
+      default:
+        eigen_assert(false && "INDEX MUST BE BETWEEN 0 and 7!");
+        abort();
+    }
+    __builtin_unreachable();
+  }
+  EIGEN_DEVICE_FUNC static Packet8bf16 convert_to_packet_type(
+      Scalar in, Scalar other) {
+    Packet8bf16 res;
+    res[0] = in.value; res[1] = other.value;
+    res[2] = in.value; res[3] = other.value;
+    res[4] = in.value; res[5] = other.value;
+    res[6] = in.value; res[7] = other.value;
+    return res;
+  }
+  EIGEN_DEVICE_FUNC static void set_packet(Packet8bf16 lhs, Scalar *rhs) {
+    lhs[0] = rhs[0].value;
+    lhs[1] = rhs[1].value;
+    lhs[2] = rhs[2].value;
+    lhs[3] = rhs[3].value;
+    lhs[4] = rhs[4].value;
+    lhs[5] = rhs[5].value;
+    lhs[6] = rhs[6].value;
+    lhs[7] = rhs[7].value;
+  }
+};
+
+#endif // EIGEN_DONT_VECTORIZE_SYCL
+#endif // DPCPP_DEVICE_ONLY
+
+}  // end namespace internal
+}  // end namespace Eigen
+
+#endif  // EIGEN_INTEROP_HEADERS_DPCPP_H
diff --git a/Eigen/src/Core/arch/DPCPP/MathFunctions.h b/Eigen/src/Core/arch/DPCPP/MathFunctions.h
new file mode 100644
index 000000000..b030b6ad3
--- /dev/null
+++ b/Eigen/src/Core/arch/DPCPP/MathFunctions.h
@@ -0,0 +1,539 @@
+// This file is part of Eigen, a lightweight C++ template library
+// for linear algebra.
+//
+// Mehdi Goli    Codeplay Software Ltd.
+// Ralph Potter  Codeplay Software Ltd.
+// Luke Iwanski  Codeplay Software Ltd.
+// Contact: <eigen@codeplay.com>
+//
+// This Source Code Form is subject to the terms of the Mozilla
+// Public License v. 2.0. If a copy of the MPL was not distributed
+// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+/*****************************************************************
+ * MathFunctions.h
+ *
+ * \brief:
+ *  MathFunctions
+ *
+ *****************************************************************/
+
+#ifndef EIGEN_MATH_FUNCTIONS_DPCPP_H
+#define EIGEN_MATH_FUNCTIONS_DPCPP_H
+
+namespace Eigen {
+
+namespace internal {
+
+// Make sure this is only available when targeting a GPU: we don't want to
+// introduce conflicts between these packet_traits definitions and the ones
+// we'll use on the host side (SSE, AVX, ...)
+#ifdef DPCPP_DEVICE_ONLY
+
+#define SYCL_PLOG(packet_type)                                         \
+  template <>                                                          \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type plog<packet_type>( \
+      const packet_type& a) {                                          \
+    return sycl::log(a);                                               \
+  }
+
+SYCL_PLOG(sycl::cl_float4)
+SYCL_PLOG(sycl::cl_half2)
+#undef SYCL_PLOG
+
+#define SYCL_PLOG1P(packet_type)                                         \
+  template <>                                                            \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type plog1p<packet_type>( \
+      const packet_type& a) {                                            \
+    return sycl::log1p(a);                                               \
+  }
+
+SYCL_PLOG1P(sycl::cl_float4)
+SYCL_PLOG1P(sycl::cl_half2)
+#undef SYCL_PLOG1P
+
+#define SYCL_PLOG10(packet_type)                                         \
+  template <>                                                            \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type plog10<packet_type>( \
+      const packet_type& a) {                                            \
+    return sycl::log10(a);                                               \
+  }
+
+SYCL_PLOG10(sycl::cl_float4)
+SYCL_PLOG10(sycl::cl_half2)
+#undef SYCL_PLOG10
+
+#define SYCL_PEXP(packet_type)                                         \
+  template <>                                                          \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pexp<packet_type>( \
+      const packet_type& a) {                                          \
+    return sycl::exp(a);                                               \
+  }
+
+SYCL_PEXP(sycl::cl_float4)
+SYCL_PEXP(sycl::cl_half2)
+#undef SYCL_PEXP
+
+#define SYCL_PEXPM1(packet_type)                                         \
+  template <>                                                            \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pexpm1<packet_type>( \
+      const packet_type& a) {                                            \
+    return sycl::expm1(a);                                               \
+  }
+
+SYCL_PEXPM1(sycl::cl_float4)
+SYCL_PEXPM1(sycl::cl_half2)
+#undef SYCL_PEXPM1
+
+#define SYCL_PSQRT(packet_type)                                         \
+  template <>                                                           \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type psqrt<packet_type>( \
+      const packet_type& a) {                                           \
+    return sycl::sqrt(a);                                               \
+  }
+
+SYCL_PSQRT(sycl::cl_float4)
+SYCL_PSQRT(sycl::cl_half2)
+#undef SYCL_PSQRT
+
+#define SYCL_PRSQRT(packet_type)                                         \
+  template <>                                                            \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type prsqrt<packet_type>( \
+      const packet_type& a) {                                            \
+    return sycl::rsqrt(a);                                               \
+  }
+
+SYCL_PRSQRT(sycl::cl_float4)
+SYCL_PRSQRT(sycl::cl_half2)
+#undef SYCL_PRSQRT
+
+/** \internal \returns the hyperbolic sine of \a a (coeff-wise) */
+#define SYCL_PSIN(packet_type)                                         \
+  template <>                                                          \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type psin<packet_type>( \
+      const packet_type& a) {                                          \
+    return sycl::sin(a);                                               \
+  }
+
+SYCL_PSIN(sycl::cl_float4)
+SYCL_PSIN(sycl::cl_half2)
+#undef SYCL_PSIN
+
+/** \internal \returns the hyperbolic cosine of \a a (coeff-wise) */
+#define SYCL_PCOS(packet_type)                                         \
+  template <>                                                          \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pcos<packet_type>( \
+      const packet_type& a) {                                          \
+    return sycl::cos(a);                                               \
+  }
+
+SYCL_PCOS(sycl::cl_float4)
+SYCL_PCOS(sycl::cl_half2)
+#undef SYCL_PCOS
+
+/** \internal \returns the hyperbolic tan of \a a (coeff-wise) */
+#define SYCL_PTAN(packet_type)                                         \
+  template <>                                                          \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type ptan<packet_type>( \
+      const packet_type& a) {                                          \
+    return sycl::tan(a);                                               \
+  }
+
+SYCL_PTAN(sycl::cl_float4)
+SYCL_PTAN(sycl::cl_half2)
+#undef SYCL_PTAN
+
+/** \internal \returns the hyperbolic sine of \a a (coeff-wise) */
+#define SYCL_PASIN(packet_type)                                         \
+  template <>                                                           \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pasin<packet_type>( \
+      const packet_type& a) {                                           \
+    return sycl::asin(a);                                               \
+  }
+
+SYCL_PASIN(sycl::cl_float4)
+SYCL_PASIN(sycl::cl_half2)
+#undef SYCL_PASIN
+
+/** \internal \returns the hyperbolic cosine of \a a (coeff-wise) */
+#define SYCL_PACOS(packet_type)                                         \
+  template <>                                                           \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pacos<packet_type>( \
+      const packet_type& a) {                                           \
+    return sycl::acos(a);                                               \
+  }
+
+SYCL_PACOS(sycl::cl_float4)
+SYCL_PACOS(sycl::cl_half2)
+#undef SYCL_PACOS
+
+/** \internal \returns the hyperbolic tan of \a a (coeff-wise) */
+#define SYCL_PATAN(packet_type)                                         \
+  template <>                                                           \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type patan<packet_type>( \
+      const packet_type& a) {                                           \
+    return sycl::atan(a);                                               \
+  }
+
+SYCL_PATAN(sycl::cl_float4)
+SYCL_PATAN(sycl::cl_half2)
+#undef SYCL_PATAN
+
+/** \internal \returns the hyperbolic sine of \a a (coeff-wise) */
+#define SYCL_PSINH(packet_type)                                         \
+  template <>                                                           \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type psinh<packet_type>( \
+      const packet_type& a) {                                           \
+    return sycl::sinh(a);                                               \
+  }
+
+SYCL_PSINH(sycl::cl_float4)
+SYCL_PSINH(sycl::cl_half2)
+#undef SYCL_PSINH
+
+/** \internal \returns the hyperbolic cosine of \a a (coeff-wise) */
+#define SYCL_PCOSH(packet_type)                                         \
+  template <>                                                           \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pcosh<packet_type>( \
+      const packet_type& a) {                                           \
+    return sycl::cosh(a);                                               \
+  }
+
+SYCL_PCOSH(sycl::cl_float4)
+SYCL_PCOSH(sycl::cl_half2)
+#undef SYCL_PCOSH
+
+/** \internal \returns the hyperbolic tan of \a a (coeff-wise) */
+#define SYCL_PTANH(packet_type)                                         \
+  template <>                                                           \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type ptanh<packet_type>( \
+      const packet_type& a) {                                           \
+    return sycl::tanh(a);                                               \
+  }
+
+SYCL_PTANH(sycl::cl_float4)
+SYCL_PTANH(sycl::cl_half2)
+#undef SYCL_PTANH
+
+#define SYCL_PCEIL(packet_type)                                         \
+  template <>                                                           \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pceil<packet_type>( \
+      const packet_type& a) {                                           \
+    return sycl::ceil(a);                                               \
+  }
+
+SYCL_PCEIL(sycl::cl_float4)
+SYCL_PCEIL(sycl::cl_half2)
+#undef SYCL_PCEIL
+
+#define SYCL_PROUND(packet_type)                                         \
+  template <>                                                            \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pround<packet_type>( \
+      const packet_type& a) {                                            \
+    return sycl::round(a);                                               \
+  }
+
+SYCL_PROUND(sycl::cl_float4)
+SYCL_PROUND(sycl::cl_half2)
+#undef SYCL_PROUND
+
+#define SYCL_PRINT(packet_type)                                         \
+  template<>                                                            \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type print<packet_type>( \
+      const packet_type& a) {                                           \
+    return sycl::rint(a);                                               \
+  }
+
+SYCL_PRINT(sycl::cl_float4)
+SYCL_PRINT(sycl::cl_half2)
+#undef SYCL_PRINT
+
+#define SYCL_FLOOR(packet_type)                                          \
+  template <>                                                            \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pfloor<packet_type>( \
+      const packet_type& a) {                                            \
+    return sycl::floor(a);                                               \
+  }
+
+SYCL_FLOOR(sycl::cl_float4)
+SYCL_FLOOR(sycl::cl_half2)
+#undef SYCL_FLOOR
+
+#define SYCL_ERF(packet_type)                                          \
+  template <>                                                          \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type perf<packet_type>( \
+      const packet_type& a) {                                          \
+    return sycl::erf(a);                                               \
+  }
+
+SYCL_ERF(sycl::cl_float4)
+SYCL_ERF(sycl::cl_half2)
+#undef SYCL_ERF
+
+#define SYCL_ERFC(packet_type)                                          \
+  template <>                                                           \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type perfc<packet_type>( \
+      const packet_type& a) {                                           \
+    return sycl::erfc(a);                                               \
+  }
+
+SYCL_ERFC(sycl::cl_float4)
+SYCL_ERFC(sycl::cl_half2)
+#undef SYCL_ERFC
+
+#define SYCL_LGAMMA(packet_type)                                          \
+  template <>                                                             \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type plgamma<packet_type>( \
+      const packet_type& a) {                                             \
+    return sycl::lgamma(a);                                               \
+  }
+
+SYCL_LGAMMA(sycl::cl_float4)
+SYCL_LGAMMA(sycl::cl_half2)
+#undef SYCL_LGAMMA
+
+#define SYCL_PACKET_MATH_UNARY_HALF8(expr)                                       \
+  template <>                                                                    \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet4h2 p##expr<Packet4h2>             \
+      (const Packet4h2& a) {                                                     \
+    Packet4h2 r;                                                                 \
+    sycl::cl_half2* r_alias = reinterpret_cast<sycl::cl_half2*>(&r);             \
+    const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a); \
+    r_alias[0] = p##expr(a_alias[0]);                                            \
+    r_alias[1] = p##expr(a_alias[1]);                                            \
+    r_alias[2] = p##expr(a_alias[2]);                                            \
+    r_alias[3] = p##expr(a_alias[3]);                                            \
+    return r;                                                                    \
+  }
+
+SYCL_PACKET_MATH_UNARY_HALF8(log)
+SYCL_PACKET_MATH_UNARY_HALF8(log1p)
+SYCL_PACKET_MATH_UNARY_HALF8(log10)
+SYCL_PACKET_MATH_UNARY_HALF8(exp)
+SYCL_PACKET_MATH_UNARY_HALF8(expm1)
+SYCL_PACKET_MATH_UNARY_HALF8(sqrt)
+SYCL_PACKET_MATH_UNARY_HALF8(rsqrt)
+SYCL_PACKET_MATH_UNARY_HALF8(sin)
+SYCL_PACKET_MATH_UNARY_HALF8(cos)
+SYCL_PACKET_MATH_UNARY_HALF8(tan)
+SYCL_PACKET_MATH_UNARY_HALF8(asin)
+SYCL_PACKET_MATH_UNARY_HALF8(acos)
+SYCL_PACKET_MATH_UNARY_HALF8(atan)
+SYCL_PACKET_MATH_UNARY_HALF8(sinh)
+SYCL_PACKET_MATH_UNARY_HALF8(cosh)
+SYCL_PACKET_MATH_UNARY_HALF8(tanh)
+SYCL_PACKET_MATH_UNARY_HALF8(ceil)
+SYCL_PACKET_MATH_UNARY_HALF8(round)
+SYCL_PACKET_MATH_UNARY_HALF8(rint)
+SYCL_PACKET_MATH_UNARY_HALF8(floor)
+SYCL_PACKET_MATH_UNARY_HALF8(erf)
+SYCL_PACKET_MATH_UNARY_HALF8(erfc)
+SYCL_PACKET_MATH_UNARY_HALF8(lgamma)
+#undef SYCL_PACKET_MATH_UNARY_HALF8
+
+#define SYCL_PACKET_MATH_UNARY_BFLOAT16(expr)                             \
+  template <>                                                             \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet2bf16 p##expr<Packet2bf16>( \
+      const Packet2bf16& a) {                                             \
+    Packet2f32 r = sycl::expr(Eigen::bfloat16_impl::Bf16ToF32<2>(a));     \
+    return Eigen::bfloat16_impl::F32ToBf16<2>(r);                         \
+  }                                                                       \
+  template <>                                                             \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet4bf16 p##expr<Packet4bf16>( \
+      const Packet4bf16& a) {                                             \
+    Packet4f32 r = sycl::expr(Eigen::bfloat16_impl::Bf16ToF32<4>(a));     \
+    return Eigen::bfloat16_impl::F32ToBf16<4>(r);                         \
+  }                                                                       \
+  template <>                                                             \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet8bf16 p##expr<Packet8bf16>( \
+      const Packet8bf16& a) {                                             \
+    Packet8f32 r = sycl::expr(Eigen::bfloat16_impl::Bf16ToF32<8>(a));     \
+    return Eigen::bfloat16_impl::F32ToBf16<8>(r);                         \
+  }
+
+SYCL_PACKET_MATH_UNARY_BFLOAT16(log)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(log1p)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(log10)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(exp)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(expm1)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(sqrt)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(rsqrt)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(sin)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(cos)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(tan)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(asin)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(acos)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(atan)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(sinh)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(cosh)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(tanh)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(ceil)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(round)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(rint)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(floor)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(erf)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(erfc)
+SYCL_PACKET_MATH_UNARY_BFLOAT16(lgamma)
+#undef SYCL_PACKET_MATH_UNARY_BFLOAT16
+
+#define SYCL_PMIN(packet_type, expr)                                   \
+  template <>                                                          \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pmin<packet_type>( \
+      const packet_type& a, const packet_type& b) {                    \
+    return expr;                                                       \
+  }
+
+SYCL_PMIN(sycl::cl_float4, sycl::fmin(a, b))
+SYCL_PMIN(sycl::cl_half2, sycl::fmin(a, b))
+#undef SYCL_PMIN
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet4h2 pmin<Packet4h2>(
+    const Packet4h2& a, const Packet4h2& b) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias  = reinterpret_cast<sycl::cl_half2*>(&r);
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  const sycl::cl_half2* b_alias = reinterpret_cast<const sycl::cl_half2*>(&b);
+  r_alias[0] = pmin(a_alias[0], b_alias[0]);
+  r_alias[1] = pmin(a_alias[1], b_alias[1]);
+  r_alias[2] = pmin(a_alias[2], b_alias[2]);
+  r_alias[3] = pmin(a_alias[3], b_alias[3]);
+  return r;
+}
+
+#define SYCL_PMAX(packet_type, expr)                                   \
+  template <>                                                          \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pmax<packet_type>( \
+      const packet_type& a, const packet_type& b) {                    \
+    return expr;                                                       \
+  }
+
+SYCL_PMAX(sycl::cl_float4, sycl::fmax(a, b))
+SYCL_PMAX(sycl::cl_half2, sycl::fmax(a, b))
+#undef SYCL_PMAX
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet4h2 pmax<Packet4h2>(
+    const Packet4h2& a, const Packet4h2& b) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias  = reinterpret_cast<sycl::cl_half2*>(&r);
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  const sycl::cl_half2* b_alias = reinterpret_cast<const sycl::cl_half2*>(&b);
+  r_alias[0] = pmax(a_alias[0], b_alias[0]);
+  r_alias[1] = pmax(a_alias[1], b_alias[1]);
+  r_alias[2] = pmax(a_alias[2], b_alias[2]);
+  r_alias[3] = pmax(a_alias[3], b_alias[3]);
+  return r;
+}
+
+#define SYCL_PACKET_MATH_BINARY_BFLOAT16(expr, in_type, out_type)               \
+  template <>                                                                   \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE in_type p##expr<in_type>(               \
+      const in_type& a, const in_type& b) {                                     \
+    out_type r = sycl::f##expr(                                                 \
+        Eigen::bfloat16_impl::Bf16ToF32<unpacket_traits<in_type>::size>(a),     \
+        Eigen::bfloat16_impl::Bf16ToF32<unpacket_traits<in_type>::size>(b));    \
+    return Eigen::bfloat16_impl::F32ToBf16<unpacket_traits<out_type>::size>(r); \
+  }
+
+SYCL_PACKET_MATH_BINARY_BFLOAT16(max, Packet2bf16, Packet2f32)
+SYCL_PACKET_MATH_BINARY_BFLOAT16(min, Packet2bf16, Packet2f32)
+SYCL_PACKET_MATH_BINARY_BFLOAT16(max, Packet4bf16, Packet4f32)
+SYCL_PACKET_MATH_BINARY_BFLOAT16(min, Packet4bf16, Packet4f32)
+SYCL_PACKET_MATH_BINARY_BFLOAT16(max, Packet8bf16, Packet8f32)
+SYCL_PACKET_MATH_BINARY_BFLOAT16(min, Packet8bf16, Packet8f32)
+#undef SYCL_PACKET_MATH_BINARY_BFLOAT16
+
+#define SYCL_PISNAN(INTYPE, EXPR)                            \
+  template<>                                                 \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE                      \
+  sycl::vec<bool, packet_traits<INTYPE>::size>               \
+  p##EXPR<sycl::vec<bool, packet_traits<INTYPE>::size>,      \
+         typename packet_traits<INTYPE>::type>(              \
+         const typename packet_traits<INTYPE>::type& a) {    \
+    sycl::vec<bool, packet_traits<INTYPE>::size> packet_res; \
+    auto res = sycl::EXPR(a);                                \
+    __pragma(unroll)                                         \
+    for(int i = 0; i < packet_traits<INTYPE>::size; i++)     \
+      packet_res[i] = res[i] ? 1 : 0;                        \
+    return packet_res;                                       \
+  }
+
+SYCL_PISNAN(float, isnan)
+SYCL_PISNAN(float, isfinite)
+SYCL_PISNAN(float, isinf)
+#undef SYCL_PISNAN
+
+template<>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
+sycl::vec<bool, packet_traits<Eigen::half>::size>
+pisnan<sycl::vec<bool, packet_traits<Eigen::half>::size>,
+       typename packet_traits<Eigen::half>::type>(
+       const typename packet_traits<Eigen::half>::type& a) {
+  sycl::vec<bool, packet_traits<Eigen::half>::size> packet_res;
+#pragma unroll
+  for(int i=0; i < packet_traits<Eigen::half>::size / 2; i++) {
+    const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+    packet_res[2*i] = sycl::isnan(a_alias[i].x());
+    packet_res[2*i+1] = sycl::isnan(a_alias[i].y());
+  }
+  return packet_res;
+}
+
+template<>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
+sycl::vec<bool, packet_traits<Eigen::half>::size>
+pisfinite<sycl::vec<bool, packet_traits<Eigen::half>::size>,
+       typename packet_traits<Eigen::half>::type>(
+       const typename packet_traits<Eigen::half>::type& a) {
+  sycl::vec<bool, packet_traits<Eigen::half>::size> packet_res;
+  for(int i=0; i < packet_traits<Eigen::half>::size / 2; i++) {
+    const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+    packet_res[2*i] = sycl::isfinite(a_alias[i].x());
+    packet_res[2*i+1] = sycl::isfinite(a_alias[i].y());
+  }
+  return packet_res;
+}
+
+template<>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
+sycl::vec<bool, packet_traits<Eigen::half>::size>
+pisinf<sycl::vec<bool, packet_traits<Eigen::half>::size>,
+       typename packet_traits<Eigen::half>::type>(
+       const typename packet_traits<Eigen::half>::type& a) {
+  sycl::vec<bool, packet_traits<Eigen::half>::size> packet_res;
+  for(int i=0; i < packet_traits<Eigen::half>::size / 2; i++) {
+    const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+    packet_res[2*i] = sycl::isinf(a_alias[i].x());
+    packet_res[2*i+1] = sycl::isinf(a_alias[i].y());
+  }
+  return packet_res;
+}
+
+#define SYCL_PACKET_MATH_ANOMALY_BFLOAT16(EXPR)                                    \
+  template <>                                                                      \
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE                                            \
+  sycl::vec<bool, packet_traits<Eigen::bfloat16>::size>                            \
+  p##EXPR<sycl::vec<bool, packet_traits<Eigen::bfloat16>::size>,                   \
+          typename packet_traits<Eigen::bfloat16>::type>(                          \
+              const typename packet_traits<Eigen::bfloat16>::type& a) {            \
+    sycl::vec<bool, packet_traits<Eigen::bfloat16>::size> packet_res;              \
+    auto res =  sycl::EXPR(                                                        \
+        Eigen::bfloat16_impl::Bf16ToF32<packet_traits<Eigen::bfloat16>::size>(a)); \
+    __pragma(unroll)                                                               \
+    for(int i = 0; i < packet_traits<Eigen::bfloat16>::size; ++i)                  \
+      packet_res[i] = res[i] ? 1 : 0;                                              \
+    return packet_res;                                                             \
+  }
+SYCL_PACKET_MATH_ANOMALY_BFLOAT16(isnan)
+SYCL_PACKET_MATH_ANOMALY_BFLOAT16(isfinite)
+SYCL_PACKET_MATH_ANOMALY_BFLOAT16(isinf)
+#undef SYCL_PACKET_MATH_ANOMALY_BFLOAT16
+
+#endif  // DPCPP_DEVICE_ONLY
+
+}  // end namespace internal
+
+}  // end namespace Eigen
+
+#endif  // EIGEN_MATH_FUNCTIONS_DPCPP_H
diff --git a/Eigen/src/Core/arch/DPCPP/PacketMath.h b/Eigen/src/Core/arch/DPCPP/PacketMath.h
new file mode 100644
index 000000000..f59a83d70
--- /dev/null
+++ b/Eigen/src/Core/arch/DPCPP/PacketMath.h
@@ -0,0 +1,1499 @@
+// This file is part of Eigen, a lightweight C++ template library
+// for linear algebra.
+//
+// Mehdi Goli    Codeplay Software Ltd.
+// Ralph Potter  Codeplay Software Ltd.
+// Luke Iwanski  Codeplay Software Ltd.
+// Contact: <eigen@codeplay.com>
+//
+// This Source Code Form is subject to the terms of the Mozilla
+// Public License v. 2.0. If a copy of the MPL was not distributed
+// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+/*****************************************************************
+ * PacketMath.h
+ *
+ * \brief:
+ *  PacketMath
+ *
+ *****************************************************************/
+
+#ifndef EIGEN_PACKET_MATH_DPCPP_H
+#define EIGEN_PACKET_MATH_DPCPP_H
+#include <type_traits>
+namespace Eigen {
+
+namespace internal {
+
+#if defined(DPCPP_DEVICE_ONLY)
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float pfirst<sycl::cl_float4>(
+    const sycl::cl_float4& a) {
+  return a.x();
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Eigen::half pfirst<sycl::cl_half2>(
+    const sycl::cl_half2& a) {
+  return Eigen::half(a.x());
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float predux<sycl::cl_float4>(
+    const sycl::cl_float4& a) {
+  return a.x() + a.y() + a.z() + a.w();
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Eigen::half predux<sycl::cl_half2>(
+    const sycl::cl_half2& a) {
+  return Eigen::half(a.x() + a.y());
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float predux_max<sycl::cl_float4>(
+    const sycl::cl_float4& a) {
+  return sycl::fmax(sycl::fmax(a.x(), a.y()),
+                        sycl::fmax(a.z(), a.w()));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Eigen::half predux_max<sycl::cl_half2>(
+    const sycl::cl_half2& a) {
+  return Eigen::half(sycl::fmax(a.x(), a.y()));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Eigen::half predux_max<Packet4h2>(
+    const Packet4h2& a) {
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  sycl::cl_half2 candidate0 = sycl::cl_half2(predux_max(a_alias[0]).x,
+                                             predux_max(a_alias[1]).x);
+  sycl::cl_half2 candidate1 = sycl::cl_half2(predux_max(a_alias[2]).x,
+                                             predux_max(a_alias[3]).x);
+  return Eigen::half(sycl::fmax(predux_max(candidate0).x, predux_max(candidate1).x));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Eigen::bfloat16 predux_max<Packet2bf16>(
+    const Packet2bf16& a) {
+  return Eigen::bfloat16_impl::max(Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a[0])),
+                                   Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a[1])));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Eigen::bfloat16 predux_max<Packet4bf16>(
+    const Packet4bf16& a) {
+  float r = predux_max(Eigen::bfloat16_impl::Bf16ToF32<4>(a));
+  return Eigen::bfloat16_impl::float_to_bfloat16_rtne<false>(r);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Eigen::bfloat16 predux_max<Packet8bf16>(
+    const Packet8bf16& a) {
+  const Packet4bf16* a_alias = reinterpret_cast<const Packet4bf16*>(&a);
+  float candidate1, candidate2;
+  candidate1 = predux_max(Eigen::bfloat16_impl::Bf16ToF32<4>(a_alias[0]));
+  candidate2 = predux_max(Eigen::bfloat16_impl::Bf16ToF32<4>(a_alias[1]));
+  return Eigen::bfloat16_impl::float_to_bfloat16_rtne<false>(
+      sycl::fmax(candidate1, candidate2));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float predux_min<sycl::cl_float4>(
+    const sycl::cl_float4& a) {
+  return sycl::fmin(sycl::fmin(a.x(), a.y()),
+                        sycl::fmin(a.z(), a.w()));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Eigen::half predux_min<sycl::cl_half2>(
+    const sycl::cl_half2& a) {
+  return Eigen::half(sycl::fmin(a.x(), a.y()));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Eigen::half predux_min<Packet4h2>(
+    const Packet4h2& a) {
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  sycl::cl_half2 candidate0 = sycl::cl_half2(predux_min(a_alias[0]).x,
+                                             predux_min(a_alias[1]).x);
+  sycl::cl_half2 candidate1 = sycl::cl_half2(predux_min(a_alias[2]).x,
+                                             predux_min(a_alias[3]).x);
+  return Eigen::half(sycl::fmin(predux_min(candidate0).x, predux_min(candidate1).x));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Eigen::bfloat16 predux_min<Packet2bf16>(
+    const Packet2bf16& a) {
+  return Eigen::bfloat16_impl::min(Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a[0])),
+                                   Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a[1])));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Eigen::bfloat16 predux_min<Packet4bf16>(
+    const Packet4bf16& a) {
+  float r = predux_min(Eigen::bfloat16_impl::Bf16ToF32<4>(a));
+  return Eigen::bfloat16_impl::float_to_bfloat16_rtne<false>(r);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Eigen::bfloat16 predux_min<Packet8bf16>(
+    const Packet8bf16& a) {
+  const Packet4bf16* a_alias = reinterpret_cast<const Packet4bf16*>(&a);
+  float candidate1, candidate2;
+  candidate1 = predux_min(Eigen::bfloat16_impl::Bf16ToF32<4>(a_alias[0]));
+  candidate2 = predux_min(Eigen::bfloat16_impl::Bf16ToF32<4>(a_alias[1]));
+  return Eigen::bfloat16_impl::float_to_bfloat16_rtne<false>(
+      sycl::fmin(candidate1, candidate2));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float predux_mul<sycl::cl_float4>(
+    const sycl::cl_float4& a) {
+  return a.x() * a.y() * a.z() * a.w();
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Eigen::half predux_mul<sycl::cl_half2>(
+    const sycl::cl_half2& a) {
+  return Eigen::half(a.x() * a.y());
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_float4
+pabs<sycl::cl_float4>(const sycl::cl_float4& a) {
+  return sycl::cl_float4(sycl::fabs(a.x()), sycl::fabs(a.y()),
+                             sycl::fabs(a.z()), sycl::fabs(a.w()));
+}
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_half2
+pabs<sycl::half2>(const sycl::cl_half2& a) {
+  return sycl::cl_half2(sycl::fabs(a.x()), sycl::fabs(a.y()));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet2bf16
+pabs<Packet2bf16>(const Packet2bf16& a) {
+  Eigen::bfloat16 x_abs = Eigen::bfloat16_impl::abs(Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a.x())));
+  Eigen::bfloat16 y_abs = Eigen::bfloat16_impl::abs(Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a.y())));
+  Packet2bf16 res(x_abs.value, y_abs.value);
+
+  return res;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet4bf16
+pabs<Packet4bf16>(const Packet4bf16& a) {
+  Eigen::bfloat16 x_abs = Eigen::bfloat16_impl::abs(Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a.x())));
+  Eigen::bfloat16 y_abs = Eigen::bfloat16_impl::abs(Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a.y())));
+  Eigen::bfloat16 z_abs = Eigen::bfloat16_impl::abs(Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a.z())));
+  Eigen::bfloat16 w_abs = Eigen::bfloat16_impl::abs(Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a.w())));
+  Packet4bf16 res(x_abs.value, y_abs.value, z_abs.value, w_abs.value);
+
+  return res;
+}
+
+template <typename Packet>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet sycl_pcmp_le(const Packet &a,
+                                                          const Packet &b) {
+  return ((a <= b)
+              .template convert<typename unpacket_traits<Packet>::type,
+                                sycl::rounding_mode::automatic>());
+}
+
+template <typename Packet>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet sycl_pcmp_lt(const Packet &a,
+                                                          const Packet &b) {
+  return ((a < b)
+              .template convert<typename unpacket_traits<Packet>::type,
+                                sycl::rounding_mode::automatic>());
+}
+
+template <typename Packet>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet sycl_pcmp_eq(const Packet &a,
+                                                          const Packet &b) {
+  return ((a == b)
+              .template convert<typename unpacket_traits<Packet>::type,
+                                sycl::rounding_mode::automatic>());
+}
+
+#define SYCL_PACKET_COMPARE_HALF2(NAME, OP)                             \
+template <>                                                             \
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_half2 sycl_pcmp_##NAME(  \
+    const sycl::cl_half2 &a, const sycl::cl_half2 &b) {                 \
+  return ((a OP b).template convert<sycl::half,                         \
+                                    sycl::rounding_mode::automatic>()); \
+}
+
+SYCL_PACKET_COMPARE_HALF2(le, <=);
+SYCL_PACKET_COMPARE_HALF2(lt, <);
+SYCL_PACKET_COMPARE_HALF2(eq, ==);
+#undef SYCL_PACKET_COMPARE_HALF2
+
+#define SYCL_PACKET_COMPARE_HALF8(NAME)                                        \
+template <>                                                                    \
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet4h2 sycl_pcmp_##NAME(              \
+    const Packet4h2 &a, const Packet4h2 &b) {                                  \
+  Packet4h2 r;                                                                 \
+  sycl::cl_half2* r_alias = reinterpret_cast<sycl::cl_half2*>(&r);             \
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a); \
+  const sycl::cl_half2* b_alias = reinterpret_cast<const sycl::cl_half2*>(&b); \
+  r_alias[0] = sycl_pcmp_##NAME<sycl::cl_half2>(a_alias[0], b_alias[0]);       \
+  r_alias[1] = sycl_pcmp_##NAME<sycl::cl_half2>(a_alias[1], b_alias[1]);       \
+  r_alias[2] = sycl_pcmp_##NAME<sycl::cl_half2>(a_alias[2], b_alias[2]);       \
+  r_alias[3] = sycl_pcmp_##NAME<sycl::cl_half2>(a_alias[3], b_alias[3]);       \
+  return r;                                                                    \
+}
+SYCL_PACKET_COMPARE_HALF8(le);
+SYCL_PACKET_COMPARE_HALF8(lt);
+SYCL_PACKET_COMPARE_HALF8(eq);
+#undef SYCL_PACKET_COMPARE_HALF8
+
+#define SYCL_PACKET_COMPARE_BFLOAT16(NAME, OP, PACKET_TYPE)             \
+template <>                                                             \
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE PACKET_TYPE sycl_pcmp_##NAME(     \
+    const PACKET_TYPE &a, const PACKET_TYPE &b) {                       \
+  const int SIZE = unpacket_traits<PACKET_TYPE>::size;                  \
+  sycl::vec<float, SIZE> a2float, b2float, r_float;                     \
+  PACKET_TYPE r;                                                        \
+  a2float = Eigen::bfloat16_impl::Bf16ToF32<SIZE>(a);                   \
+  b2float = Eigen::bfloat16_impl::Bf16ToF32<SIZE>(b);                   \
+  r_float = ((a OP b).template convert<float,                           \
+                                    sycl::rounding_mode::automatic>()); \
+  r = Eigen::bfloat16_impl::F32ToBf16<SIZE>(r_float);                   \
+  return r;                                                             \
+}
+
+SYCL_PACKET_COMPARE_BFLOAT16(le, <=, Packet2bf16);
+SYCL_PACKET_COMPARE_BFLOAT16(lt, <, Packet2bf16);
+SYCL_PACKET_COMPARE_BFLOAT16(eq, ==, Packet2bf16);
+SYCL_PACKET_COMPARE_BFLOAT16(le, <=, Packet4bf16);
+SYCL_PACKET_COMPARE_BFLOAT16(lt, <, Packet4bf16);
+SYCL_PACKET_COMPARE_BFLOAT16(eq, ==, Packet4bf16);
+SYCL_PACKET_COMPARE_BFLOAT16(le, <=, Packet8bf16);
+SYCL_PACKET_COMPARE_BFLOAT16(lt, <, Packet8bf16);
+SYCL_PACKET_COMPARE_BFLOAT16(eq, ==, Packet8bf16);
+#undef SYCL_PACKET_COMPARE_BFLOAT16
+
+#define SYCL_PCMP(OP, TYPE)                                                 \
+template <>                                                                 \
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE TYPE pcmp_##OP<TYPE>(const TYPE &a,   \
+                                                           const TYPE &b) { \
+  return sycl_pcmp_##OP<TYPE>(a, b);                                        \
+}
+
+SYCL_PCMP(le, sycl::cl_float4)
+SYCL_PCMP(lt, sycl::cl_float4)
+SYCL_PCMP(eq, sycl::cl_float4)
+SYCL_PCMP(le, sycl::cl_half2)
+SYCL_PCMP(lt, sycl::cl_half2)
+SYCL_PCMP(eq, sycl::cl_half2)
+SYCL_PCMP(le, Packet4h2)
+SYCL_PCMP(lt, Packet4h2)
+SYCL_PCMP(eq, Packet4h2)
+SYCL_PCMP(le, Packet2bf16)
+SYCL_PCMP(lt, Packet2bf16)
+SYCL_PCMP(eq, Packet2bf16)
+SYCL_PCMP(le, Packet4bf16)
+SYCL_PCMP(lt, Packet4bf16)
+SYCL_PCMP(eq, Packet4bf16)
+SYCL_PCMP(le, Packet8bf16)
+SYCL_PCMP(lt, Packet8bf16)
+SYCL_PCMP(eq, Packet8bf16)
+#undef SYCL_PCMP
+
+#define SYCL_PCMP_BOOL(OP, CMP, INTYPE)                                                    \
+template <>                                                                                \
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::vec<bool, packet_traits<                       \
+    INTYPE>::size> boolean_pcmp_##OP<sycl::vec<bool, packet_traits<INTYPE>::size>,         \
+    typename packet_traits<INTYPE>::type>(const typename packet_traits<INTYPE>::type &a,   \
+                                          const typename packet_traits<INTYPE>::type &b) { \
+  sycl::vec<bool, packet_traits<INTYPE>::size> packet_res;                                 \
+  for(int i = 0; i < packet_traits<INTYPE>::size; i++)                                     \
+    packet_res[i] = (a[i] CMP b[i]);                                                       \
+  return packet_res;                                                                       \
+} 
+SYCL_PCMP_BOOL(le, <=, float)
+SYCL_PCMP_BOOL(lt, <, float)
+SYCL_PCMP_BOOL(eq, ==, float)
+SYCL_PCMP_BOOL(le, <=, bool)
+SYCL_PCMP_BOOL(lt, <, bool)
+SYCL_PCMP_BOOL(eq, ==, bool)
+#undef SYCL_PCMP_BOOL
+
+#define SYCL_PCMP_BOOL_FOR_HALF8(OP, CMP, INTYPE)                                          \
+template <>                                                                                \
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::vec<bool, packet_traits<                       \
+    INTYPE>::size> boolean_pcmp_##OP<sycl::vec<bool, packet_traits<INTYPE>::size>,         \
+    typename packet_traits<INTYPE>::type>(const typename packet_traits<INTYPE>::type &a,   \
+                                          const typename packet_traits<INTYPE>::type &b) { \
+  sycl::vec<bool, packet_traits<INTYPE>::size> packet_res;                                 \
+  typename packet_traits<INTYPE>::type cmp_res = sycl_pcmp_##OP(a, b);                     \
+  const short* cmp_res_alias = reinterpret_cast<const short*>(&cmp_res);                   \
+  for(int i = 0; i < packet_traits<INTYPE>::size; i++)                                     \
+    packet_res[i] = cmp_res_alias[i] ? true : false;                                       \
+  return packet_res;                                                                       \
+} 
+SYCL_PCMP_BOOL_FOR_HALF8(le, <=, Eigen::half)
+SYCL_PCMP_BOOL_FOR_HALF8(lt, <, Eigen::half)
+SYCL_PCMP_BOOL_FOR_HALF8(eq, ==, Eigen::half)
+#undef SYCL_PCMP_BOOL_FOR_HALF8
+
+#define SYCL_PCMP_BOOL_FOR_BFLOAT16(OP, CMP, INTYPE)                                       \
+template <>                                                                                \
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::vec<bool, packet_traits<                       \
+    INTYPE>::size> boolean_pcmp_##OP<sycl::vec<bool, packet_traits<INTYPE>::size>,         \
+    typename packet_traits<INTYPE>::type>(const typename packet_traits<INTYPE>::type &a,   \
+                                          const typename packet_traits<INTYPE>::type &b) { \
+  sycl::vec<bool, packet_traits<INTYPE>::size> packet_res;                                 \
+  for(int i = 0; i < packet_traits<INTYPE>::size; i++) {                                   \
+    float a2float = Eigen::bfloat16_impl::bfloat16_to_float(                               \
+        Eigen::bfloat16_impl::raw_uint16_to_bfloat16(a[i]));                               \
+    float b2float = Eigen::bfloat16_impl::bfloat16_to_float(                               \
+        Eigen::bfloat16_impl::raw_uint16_to_bfloat16(b[i]));                               \
+    packet_res[i] = (a2float CMP b2float);                                                 \
+  }                                                                                        \
+  return packet_res;                                                                       \
+} 
+SYCL_PCMP_BOOL_FOR_BFLOAT16(le, <=, Eigen::bfloat16)
+SYCL_PCMP_BOOL_FOR_BFLOAT16(lt, <, Eigen::bfloat16)
+SYCL_PCMP_BOOL_FOR_BFLOAT16(eq, ==, Eigen::bfloat16)
+#undef SYCL_PCMP_BOOL_FOR_BFLOAT16
+
+template <typename T> struct convert_to_integer;
+
+template <>
+struct convert_to_integer<float> {
+  using type = std::int32_t;
+  using packet_type = sycl::cl_int4;
+};
+
+template <>
+struct convert_to_integer<Eigen::half> {
+  using type = std::int16_t;
+  using packet_type = sycl::cl_short2;
+};
+
+template <>
+struct convert_to_integer<Eigen::bfloat16> {
+  using type = std::int32_t;
+  using packet_type = sycl::cl_int8; 
+};
+
+template <typename PacketIn>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename convert_to_integer<
+    typename unpacket_traits<PacketIn>::type>::packet_type
+vector_as_int(const PacketIn &p) {
+  return (
+      p.template convert<typename convert_to_integer<
+                             typename unpacket_traits<PacketIn>::type>::type,
+                         sycl::rounding_mode::automatic>());
+}
+
+template <typename packetOut, typename PacketIn>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packetOut
+convert_vector(const PacketIn &p) {
+  return (p.template convert<typename unpacket_traits<packetOut>::type,
+                             sycl::rounding_mode::automatic>());
+}
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_half2
+convert_vector(const sycl::cl_short2& p) {
+  return (p.template convert<sycl::half,
+                             sycl::rounding_mode::automatic>());
+}
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet8bf16
+convert_vector(const sycl::cl_int8& p) {
+  Packet8bf16 r;
+  Packet8f32 p2float = p.template convert<sycl::cl_float, sycl::rounding_mode::automatic>();
+  r = Eigen::bfloat16_impl::F32ToBf16<8>(p2float); 
+  return r;
+}
+
+#define SYCL_PAND(TYPE)                                                \
+template <>                                                            \
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TYPE pand<TYPE>(const TYPE &a,   \
+                                                      const TYPE &b) { \
+  return convert_vector<TYPE>(vector_as_int(a) & vector_as_int(b));    \
+}
+SYCL_PAND(sycl::cl_float4)
+SYCL_PAND(sycl::cl_half2)
+SYCL_PAND(Packet8bf16)
+#undef SYCL_PAND
+
+#define SYCL_PLOGICAL_NOT(SIZE)                                           \
+template <>                                                               \
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::vec<bool, SIZE>               \
+    plogical_not<sycl::vec<bool, SIZE>>(const sycl::vec<bool, SIZE>& a) { \
+  sycl::vec<bool, SIZE> b;                                                \
+  for(int i = 0; i < SIZE; ++i) {                                         \
+    b[i] = (!a[i]);                                                       \
+  }                                                                       \
+  return b;                                                               \
+}
+SYCL_PLOGICAL_NOT(16)
+SYCL_PLOGICAL_NOT(8)
+SYCL_PLOGICAL_NOT(4)
+SYCL_PLOGICAL_NOT(2)
+#undef SYCL_PLOGICAL_NOT
+
+#define SYCL_POR(TYPE)                                                \
+template <>                                                           \
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TYPE por<TYPE>(const TYPE &a,   \
+                                                     const TYPE &b) { \
+  return convert_vector<TYPE>(vector_as_int(a) | vector_as_int(b));   \
+}
+
+SYCL_POR(sycl::cl_float4)
+SYCL_POR(sycl::cl_half2)
+SYCL_POR(Packet8bf16)
+#undef SYCL_POR
+
+#define SYCL_PXOR(TYPE)                                                \
+template <>                                                            \
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TYPE pxor<TYPE>(const TYPE &a,   \
+                                                      const TYPE &b) { \
+  return convert_vector<TYPE>(vector_as_int(a) ^ vector_as_int(b));    \
+}
+
+SYCL_PXOR(sycl::cl_float4)
+SYCL_PXOR(sycl::cl_half2)
+SYCL_PXOR(Packet8bf16)
+#undef SYCL_PXOR
+
+#define SYCL_PANDNOT(TYPE)                                                \
+template <>                                                               \
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TYPE pandnot<TYPE>(const TYPE &a,   \
+                                                         const TYPE &b) { \
+  return convert_vector<TYPE>(vector_as_int(a) & (~vector_as_int(b)));    \
+}
+SYCL_PANDNOT(sycl::cl_float4)
+SYCL_PANDNOT(sycl::cl_half2)
+SYCL_PANDNOT(Packet8bf16)
+#undef SYCL_PANDNOT
+
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE void ptranspose(
+    PacketBlock<sycl::cl_float4, 4>& kernel) {
+  float tmp = kernel.packet[0].y();
+  kernel.packet[0].y() = kernel.packet[1].x();
+  kernel.packet[1].x() = tmp;
+
+  tmp = kernel.packet[0].z();
+  kernel.packet[0].z() = kernel.packet[2].x();
+  kernel.packet[2].x() = tmp;
+
+  tmp = kernel.packet[0].w();
+  kernel.packet[0].w() = kernel.packet[3].x();
+  kernel.packet[3].x() = tmp;
+
+  tmp = kernel.packet[1].z();
+  kernel.packet[1].z() = kernel.packet[2].y();
+  kernel.packet[2].y() = tmp;
+
+  tmp = kernel.packet[1].w();
+  kernel.packet[1].w() = kernel.packet[3].y();
+  kernel.packet[3].y() = tmp;
+
+  tmp = kernel.packet[2].w();
+  kernel.packet[2].w() = kernel.packet[3].z();
+  kernel.packet[3].z() = tmp;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_float4 pblend(
+    const Selector<unpacket_traits<sycl::cl_float4>::size>& ifPacket,
+    const sycl::cl_float4& thenPacket,
+    const sycl::cl_float4& elsePacket) {
+  sycl::cl_int4 condition(
+      ifPacket.select[0] ? 0 : -1, ifPacket.select[1] ? 0 : -1,
+      ifPacket.select[2] ? 0 : -1, ifPacket.select[3] ? 0 : -1);
+  return sycl::select(thenPacket, elsePacket, condition);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_float4 ploadt_ro<sycl::cl_float4, Unaligned>(const float* from) {
+  return sycl::cl_float4(from[0], from[1], from[2], from[3]);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_float4 ploadt_ro<sycl::cl_float4, Aligned>(const float* from) {
+  return *reinterpret_cast<const sycl::cl_float4*>(from);
+}
+
+template <> 
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 pload<sycl::cl_float4>(const float* from) {
+  return *reinterpret_cast<const sycl::cl_float4*>(from);
+}
+
+template <> 
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 ploadu<sycl::cl_float4>(const float* from) {
+  return sycl::cl_float4(from[0], from[1], from[2], from[3]);
+}
+template <> 
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 ploaddup<sycl::cl_float4>(const float* from) {
+  return sycl::cl_float4(from[0], from[1], from[2], from[3]);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_half2 ploadt_ro<sycl::cl_half2, Unaligned>(const Eigen::half* from) {
+  return sycl::cl_half2(from[0].x, from[1].x);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_half2 ploadt_ro<sycl::cl_half2, Aligned>(const Eigen::half* from) {
+  return *reinterpret_cast<const sycl::cl_half2*>(from);
+}
+
+template <> 
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_half2 pload<sycl::cl_half2>(const Eigen::half* from) {
+  return *reinterpret_cast<const sycl::cl_half2*>(from);
+}
+
+template <> 
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_half2 ploadu<sycl::cl_half2>(const Eigen::half* from) {
+  return sycl::cl_half2(from[0].x, from[1].x);
+}
+
+template <> 
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_half2 ploaddup<sycl::cl_half2>(const Eigen::half* from) {
+  return sycl::cl_half2(from[0].x, from[1].x);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet2bf16 ploadt_ro<Packet2bf16, Unaligned>(const Eigen::bfloat16* from) {
+  return Packet2bf16(from[0].value, from[1].value);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet4bf16 ploadt_ro<Packet4bf16, Unaligned>(const Eigen::bfloat16* from) {
+  return Packet4bf16(from[0].value, from[1].value, from[2].value, from[3].value);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet8bf16 ploadt_ro<Packet8bf16, Unaligned>(const Eigen::bfloat16* from) {
+  Packet8bf16 r;
+  Packet4bf16* r_alias = reinterpret_cast<Packet4bf16*>(&r);
+  r_alias[0] = ploadt_ro<Packet4bf16, Unaligned>(from);
+  r_alias[1] = ploadt_ro<Packet4bf16, Unaligned>(from + 4);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet2bf16 ploadt_ro<Packet2bf16, Aligned>(const Eigen::bfloat16* from) {
+  return *reinterpret_cast<const Packet2bf16*>(from);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet4bf16 ploadt_ro<Packet4bf16, Aligned>(const Eigen::bfloat16* from) {
+  return *reinterpret_cast<const Packet4bf16*>(from);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet8bf16 ploadt_ro<Packet8bf16, Aligned>(const Eigen::bfloat16* from) {
+  return *reinterpret_cast<const Packet8bf16*>(from);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet2bf16 pload<Packet2bf16>(const Eigen::bfloat16* from) {
+  return *reinterpret_cast<const Packet2bf16*>(from);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4bf16 pload<Packet4bf16>(const Eigen::bfloat16* from) {
+  return *reinterpret_cast<const Packet4bf16*>(from);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet8bf16 pload<Packet8bf16>(const Eigen::bfloat16* from) {
+  return *reinterpret_cast<const Packet8bf16*>(from);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet2bf16 ploadu<Packet2bf16>(const Eigen::bfloat16* from) {
+  return Packet2bf16(from[0].value, from[1].value);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet4bf16 ploadu<Packet4bf16>(const Eigen::bfloat16* from) {
+  return Packet4bf16(from[0].value, from[1].value, from[2].value, from[3].value);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet8bf16 ploadu<Packet8bf16>(const Eigen::bfloat16* from) {
+  Packet8bf16 r;
+  Packet4bf16* r_alias = reinterpret_cast<Packet4bf16*>(&r);
+  r_alias[0] = ploadu<Packet4bf16>(from);
+  r_alias[1] = ploadu<Packet4bf16>(from + 4);
+  return r;
+}
+
+template <> 
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet2bf16 ploaddup<Packet2bf16>(const Eigen::bfloat16* from) {
+  return Packet2bf16(from[0].value, from[1].value);
+}
+
+template <> 
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4bf16 ploaddup<Packet4bf16>(const Eigen::bfloat16* from) {
+  return Packet4bf16(from[0].value, from[1].value, from[2].value, from[3].value);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet8bf16 ploaddup<Packet8bf16>(const Eigen::bfloat16* from) {
+  Packet8bf16 r;
+  Packet4bf16* r_alias = reinterpret_cast<Packet4bf16*>(&r);
+  r_alias[0] = ploaddup<Packet4bf16>(from);
+  r_alias[1] = ploaddup<Packet4bf16>(from + 4);
+  return r;
+}
+
+template <> 
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::vec<bool,16> pload<sycl::vec<bool,16>>(const bool* from) {
+  return *reinterpret_cast<const sycl::vec<bool,16>*>(from);
+}
+
+template <> 
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::vec<bool,16> ploadu<sycl::vec<bool,16>>(const bool* from) {
+  return sycl::vec<bool,16>(from[0], from[1], from[2], from[3],    \
+                            from[4], from[5], from[6], from[7],    \
+                            from[8], from[9], from[10], from[11],  \
+                            from[12], from[13], from[14], from[15]);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstore<float>(float* to, const sycl::cl_float4& from) {
+  *reinterpret_cast<sycl::cl_float4*>(to) = from;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstoreu<float>(float* to, const sycl::cl_float4& from) {
+  to[0] = from.x();
+  to[1] = from.y();
+  to[2] = from.z();
+  to[3] = from.w();
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstore<Eigen::half>(Eigen::half* to, const sycl::cl_half2& from) {
+  *reinterpret_cast<sycl::cl_half2*>(to) = from;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstore<Eigen::bfloat16>(Eigen::bfloat16* to, const Packet2bf16& from) {
+  *reinterpret_cast<Packet2bf16*>(to) = from;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstore<Eigen::bfloat16>(Eigen::bfloat16* to, const Packet4bf16& from) {
+  *reinterpret_cast<Packet4bf16*>(to) = from;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstore<Eigen::bfloat16>(Eigen::bfloat16* to, const Packet8bf16& from) {
+  *reinterpret_cast<Packet8bf16*>(to) = from;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstoreu<Eigen::half>(Eigen::half* to, const sycl::cl_half2& from) {
+  to[0] = from.x();
+  to[1] = from.y();
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstoreu<Eigen::bfloat16>(Eigen::bfloat16* to, const Packet2bf16& from) {
+  to[0] = Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(from.x()));
+  to[1] = Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(from.y()));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstoreu<Eigen::bfloat16>(Eigen::bfloat16* to, const Packet4bf16& from) {
+  to[0] = Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(from.x()));
+  to[1] = Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(from.y()));
+  to[2] = Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(from.z()));
+  to[3] = Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(from.w()));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstoreu<Eigen::bfloat16>(Eigen::bfloat16* to, const Packet8bf16& from) {
+  #pragma unroll
+  for(int i = 0; i < unpacket_traits<Packet8bf16>::size; ++i)
+    to[i] = Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(from[i]));
+ }
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstore<bool,sycl::vec<bool,16>>(bool* to, const sycl::vec<bool,16>& from) {
+  *reinterpret_cast<sycl::vec<bool,16>*>(to) = from;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstore<bool,sycl::vec<bool,4>>(bool* to, const sycl::vec<bool,4>& from) {
+  *reinterpret_cast<sycl::vec<bool,4>*>(to) = from;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstore<bool,sycl::vec<bool,2>>(bool* to, const sycl::vec<bool,2>& from) {
+  *reinterpret_cast<sycl::vec<bool,2>*>(to) = from;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstore<bool,sycl::vec<bool,8>>(bool* to, const sycl::vec<bool,8>& from) {
+  *reinterpret_cast<sycl::vec<bool,8>*>(to) = from;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstoreu<bool>(bool* to, const sycl::vec<bool,2>& from) {
+  to[0] = from.x();
+  to[1] = from.y();
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstoreu<bool>(bool* to, const sycl::vec<bool,4>& from) {
+  to[0] = from.x();
+  to[1] = from.y();
+  to[2] = from.z();
+  to[3] = from.w();
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstoreu<bool>(bool* to, const sycl::vec<bool,8>& from) {
+  to[0] = from.s0();
+  to[1] = from.s1();
+  to[2] = from.s2();
+  to[3] = from.s3();
+  to[4] = from.s4();
+  to[5] = from.s5();
+  to[6] = from.s6();
+  to[7] = from.s7();
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstoreu<bool>(bool* to, const sycl::vec<bool,16>& from) {
+  to[0] = from.s0();
+  to[1] = from.s1();
+  to[2] = from.s2();
+  to[3] = from.s3();
+  to[4] = from.s4();
+  to[5] = from.s5();
+  to[6] = from.s6();
+  to[7] = from.s7();
+  to[8] = from.s8();
+  to[9] = from.s9();
+  to[10] = from.sA();
+  to[11] = from.sB();
+  to[12] = from.sC();
+  to[13] = from.sD();
+  to[14] = from.sE();
+  to[15] = from.sF();
+}
+
+/** \internal \returns a packet with constant coefficients \a a, e.g.: (a,a,a,a) */
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 pset1<sycl::cl_float4>(const float& from) {
+  return sycl::cl_float4(from, from, from, from); 
+}
+
+/** \internal \returns a packet with constant coefficients \a a[0], e.g.: (a[0],a[0],a[0],a[0]) */
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 pload1<sycl::cl_float4>(const float* a) {
+  return pset1<sycl::cl_float4>(*a);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 plset<sycl::cl_float4>(const float& a) {
+  return sycl::cl_float4(a, a + 1, a + 2, a + 3);
+}
+
+template <>
+EIGEN_DEVICE_FUNC inline sycl::cl_float4 pzero<sycl::cl_float4>(const sycl::cl_float4& /*a*/) {
+  return sycl::cl_float4(0);
+}
+
+/** \internal \returns a packet with constant coefficients \a a, e.g.: (a,a) */
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_half2 pset1<sycl::cl_half2>(const Eigen::half& from) {
+  return sycl::cl_half2(from.x, from.x); 
+}
+
+/** \internal \returns a packet with constant coefficients \a a[0], e.g.: (a[0],a[0])) */
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_half2 pload1<sycl::cl_half2>(const Eigen::half* a) {
+  return pset1<sycl::cl_half2>(*a);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_half2 plset<sycl::cl_half2>(const Eigen::half& a) {
+  Eigen::half b = a + Eigen::half(1.0f);
+  return sycl::cl_half2(a.x, b.x);
+}
+
+template <>
+EIGEN_DEVICE_FUNC inline sycl::cl_half2 pzero<sycl::cl_half2>(const sycl::cl_half2& /*a*/) {
+  return sycl::cl_half2(0);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet2bf16 pset1<Packet2bf16>(const Eigen::bfloat16& from) {
+  return Packet2bf16(from.value, from.value);
+}
+
+/** \internal \returns a packet with constant coefficients \a a[0], e.g.: (a[0],a[0])) */
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet2bf16 pload1<Packet2bf16>(const Eigen::bfloat16* a) {
+  return pset1<Packet2bf16>(*a);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet2bf16 plset<Packet2bf16>(const Eigen::bfloat16& a) {
+  Eigen::bfloat16 b = a + Eigen::bfloat16(1.0f);
+  return Packet2bf16(a.value, b.value);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4bf16 pset1<Packet4bf16>(const Eigen::bfloat16& from) {
+  return Packet4bf16(from.value, from.value, from.value, from.value);
+}
+
+/** \internal \returns a packet with constant coefficients \a a[0], e.g.: (a[0],a[0])) */
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4bf16 pload1<Packet4bf16>(const Eigen::bfloat16* a) {
+  return pset1<Packet4bf16>(*a);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4bf16 plset<Packet4bf16>(const Eigen::bfloat16& a) {
+  Eigen::bfloat16 b = a + Eigen::bfloat16(1.0f);
+  Eigen::bfloat16 c = a + Eigen::bfloat16(1.0f);
+  Eigen::bfloat16 d = a + Eigen::bfloat16(1.0f);
+  return Packet4bf16(a.value, b.value, c.value, d.value);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet8bf16 pset1<Packet8bf16>(const Eigen::bfloat16& from) {
+  return Packet8bf16(from.value);
+}
+
+/** \internal \returns a packet with constant coefficients \a a[0], e.g.: (a[0],a[0])) */
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet8bf16 pload1<Packet8bf16>(const Eigen::bfloat16* a) {
+  return pset1<Packet8bf16>(*a);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet8bf16 plset<Packet8bf16>(const Eigen::bfloat16& a) {
+  Packet8bf16 r;
+  Packet4bf16* r_alias = reinterpret_cast<Packet4bf16*>(&r);
+  r_alias[0] = plset<Packet4bf16>(a);
+  r_alias[1] = plset<Packet4bf16>(a + Eigen::bfloat16(4.0f));
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet8bf16 pzero<Packet8bf16>(const Packet8bf16& /*a*/) {
+  return Packet8bf16(0);
+}
+
+template <>
+EIGEN_DEVICE_FUNC inline sycl::vec<bool, 16> pzero<sycl::vec<bool, 16>>(const sycl::vec<bool, 16>& /*a*/) {
+  return sycl::vec<bool, 16>(0);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::vec<bool,16> pset1<sycl::vec<bool,16>>(const bool& from) {
+  return sycl::vec<bool,16>(from); 
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 padd<sycl::cl_float4>(const sycl::cl_float4& a, const sycl::cl_float4& b) {
+  return sycl::cl_float4(a.x() + b.x(), a.y() + b.y(), a.z() + b.z(), a.w() + b.w());
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_half2 padd<sycl::cl_half2>(const sycl::cl_half2& a, const sycl::cl_half2& b) {
+  return sycl::cl_half2(a.x() + b.x(), a.y() + b.y());
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 psub<sycl::cl_float4>(const sycl::cl_float4& a, const sycl::cl_float4& b) {
+  return sycl::cl_float4(a.x() - b.x(), a.y() - b.y(), a.z() - b.z(), a.w() - b.w());
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_half2 psub<sycl::cl_half2>(const sycl::cl_half2& a, const sycl::cl_half2& b) {
+  return sycl::cl_half2(a.x() - b.x(), a.y() - b.y());
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 pnegate<sycl::cl_float4>(const sycl::cl_float4& a) {
+  return sycl::cl_float4(-a.x(), -a.y(), -a.z(), -a.w());
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_half2 pnegate<sycl::cl_half2>(const sycl::cl_half2& a) {
+  return sycl::cl_half2(-a.x(), -a.y());
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 pconj<sycl::cl_float4>(const sycl::cl_float4& a) {
+  return a;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 pmul<sycl::cl_float4>(const sycl::cl_float4& a, const sycl::cl_float4& b) {
+  return sycl::cl_float4(a.x() * b.x(), a.y() * b.y(), a.z() * b.z(), a.w() * b.w());
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_half2 pmul<sycl::cl_half2>(const sycl::cl_half2& a, const sycl::cl_half2& b) {
+  return sycl::cl_half2(a.x() * b.x(), a.y() * b.y());
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 pdiv<sycl::cl_float4>(const sycl::cl_float4& a, const sycl::cl_float4& b) {
+  return sycl::cl_float4(a.x() / b.x(), a.y() / b.y(), a.z() / b.z(), a.w() / b.w());
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_half2 pdiv<sycl::cl_half2>(const sycl::cl_half2& a, const sycl::cl_half2& b) {
+  return sycl::cl_half2(a.x() / b.x(), a.y() / b.y());
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_half2 pblend(
+    const Selector<unpacket_traits<sycl::cl_half2>::size>& ifPacket,
+    const sycl::cl_half2& thenPacket,
+    const sycl::cl_half2& elsePacket) {
+  sycl::cl_short2 condition(
+      ifPacket.select[0] ? 0 : -1, ifPacket.select[1] ? 0 : -1);
+  return sycl::select(thenPacket, elsePacket, condition);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet4h2 pblend(
+    const Selector<unpacket_traits<Packet4h2>::size>& ifPacket,
+    const Packet4h2& thenPacket, const Packet4h2& elsePacket) {
+  Packet4h2 r;
+  const sycl::cl_half2* then_alias = reinterpret_cast<const sycl::cl_half2*>(&thenPacket);
+  const sycl::cl_half2* else_alias = reinterpret_cast<const sycl::cl_half2*>(&elsePacket);
+  sycl::cl_half2* r_alias = reinterpret_cast<sycl::cl_half2*>(&r);
+#pragma unroll
+  for(int i = 0; i < 4; ++i) {
+    sycl::cl_short2 condition(
+      ifPacket.select[2*i] ? 0 : -1, ifPacket.select[2*i+1] ? 0 : -1);
+    r_alias[i] = sycl::select(then_alias[i], else_alias[i], condition);
+  }
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet8bf16 pblend(
+    const Selector<unpacket_traits<Packet8bf16>::size>& ifPacket,
+    const Packet8bf16& thenPacket, const Packet8bf16& elsePacket) {
+  Packet8bf16 r;
+#pragma unroll
+  for(int i = 0; i < unpacket_traits<Packet8bf16>::size; ++i) {
+    sycl::cl_short condition(ifPacket.select[i] ? 0 : -1);
+    r[i] = sycl::select(thenPacket[i], elsePacket[i], condition);
+  }
+  return r;
+}
+
+template<>
+EIGEN_DEVICE_FUNC inline sycl::cl_half2 psign(const sycl::cl_half2& a) {
+  return sycl::sign(a);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 psign(const Packet4h2& a) {
+  Packet4h2 r;
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  sycl::cl_half2* r_alias = reinterpret_cast<sycl::cl_half2*>(&r);
+  r_alias[0] = psign<sycl::cl_half2>(a_alias[0]);
+  r_alias[1] = psign<sycl::cl_half2>(a_alias[1]);
+  r_alias[2] = psign<sycl::cl_half2>(a_alias[2]);
+  r_alias[3] = psign<sycl::cl_half2>(a_alias[3]);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet8bf16 psign(const Packet8bf16& a) {
+  const int SIZE = unpacket_traits<Packet8bf16>::size;
+  sycl::vec<float, SIZE> a2float, r_float;
+  a2float = Eigen::bfloat16_impl::Bf16ToF32<8>(a);
+  r_float = sycl::sign(a2float); 
+  return Eigen::bfloat16_impl::F32ToBf16<8>(r_float);
+}
+
+template <>
+EIGEN_DEVICE_FUNC inline sycl::cl_float4 psign(const sycl::cl_float4& a) {
+  return sycl::sign(a);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2
+pset1<Packet4h2>(const Eigen::half& from) {
+  Packet4h2 r;
+  sycl::cl_half2* p_alias = reinterpret_cast<sycl::cl_half2*>(&r);
+  p_alias[0] = pset1<sycl::cl_half2>(from);
+  p_alias[1] = pset1<sycl::cl_half2>(from);
+  p_alias[2] = pset1<sycl::cl_half2>(from);
+  p_alias[3] = pset1<sycl::cl_half2>(from);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2
+pload<Packet4h2>(const Eigen::half* from) {
+  return *reinterpret_cast<const Packet4h2*>(from);
+}
+
+// unaligned load;
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2
+ploadu<Packet4h2>(const Eigen::half* from) {
+  Packet4h2 r;
+  sycl::cl_half2* p_alias = reinterpret_cast<sycl::cl_half2*>(&r);
+  p_alias[0] = ploadu<sycl::cl_half2>(from + 0);
+  p_alias[1] = ploadu<sycl::cl_half2>(from + 2);
+  p_alias[2] = ploadu<sycl::cl_half2>(from + 4);
+  p_alias[3] = ploadu<sycl::cl_half2>(from + 6);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2
+ploaddup<Packet4h2>(const Eigen::half* from) {
+  Packet4h2 r;
+  sycl::cl_half2* p_alias = reinterpret_cast<sycl::cl_half2*>(&r);
+  p_alias[0] = ploaddup<sycl::cl_half2>(from + 0);
+  p_alias[1] = ploaddup<sycl::cl_half2>(from + 1);
+  p_alias[2] = ploaddup<sycl::cl_half2>(from + 2);
+  p_alias[3] = ploaddup<sycl::cl_half2>(from + 3);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstore<Eigen::half>(
+    Eigen::half* to, const Packet4h2& from) {
+  *reinterpret_cast<Packet4h2*>(to) = from;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pstoreu<Eigen::half>(
+    Eigen::half* to, const Packet4h2& from) {
+
+  const sycl::cl_half2* from_alias = reinterpret_cast<const sycl::cl_half2*>(&from);
+  pstoreu(to + 0,from_alias[0]);
+  pstoreu(to + 2,from_alias[1]);
+  pstoreu(to + 4,from_alias[2]);
+  pstoreu(to + 6,from_alias[3]);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet4h2
+ploadt_ro<Packet4h2, Aligned>(const Eigen::half* from) {
+  Packet4h2 r;
+  r = *reinterpret_cast<const Packet4h2*>(from);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet4h2
+ploadt_ro<Packet4h2, Unaligned>(const Eigen::half* from) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias = reinterpret_cast<sycl::cl_half2*>(&r);
+  r_alias[0] = ploadt_ro<sycl::cl_half2, Unaligned>(from + 0);
+  r_alias[1] = ploadt_ro<sycl::cl_half2, Unaligned>(from + 2);
+  r_alias[2] = ploadt_ro<sycl::cl_half2, Unaligned>(from + 4);
+  r_alias[3] = ploadt_ro<sycl::cl_half2, Unaligned>(from + 6);
+  return r;
+}
+
+template <> EIGEN_DEVICE_FUNC inline sycl::cl_float4 pgather<float, sycl::cl_float4>(const float* from, Index stride) {
+  return sycl::cl_float4(from[0*stride], from[1*stride], from[2*stride], from[3*stride]);
+}
+
+template <> EIGEN_DEVICE_FUNC inline sycl::cl_half2 pgather<Eigen::half, sycl::cl_half2>(const Eigen::half* from, Index stride) {
+  return sycl::cl_half2(from[0*stride].x, from[1*stride].x);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2
+pgather<Eigen::half, Packet4h2>(const Eigen::half* from, Index stride) {
+  Packet4h2 r;
+  sycl::cl_half2* p_alias = reinterpret_cast<sycl::cl_half2*>(&r);
+  p_alias[0] = sycl::cl_half2(from[0 * stride].x, from[1 * stride].x);
+  p_alias[1] = sycl::cl_half2(from[2 * stride].x, from[3 * stride].x);
+  p_alias[2] = sycl::cl_half2(from[4 * stride].x, from[5 * stride].x);
+  p_alias[3] = sycl::cl_half2(from[6 * stride].x, from[7 * stride].x);
+  return r;
+}
+
+template <> EIGEN_DEVICE_FUNC inline void pscatter<float, sycl::cl_float4>(float* to, const sycl::cl_float4& from, Index stride) {
+  to[stride*0] = from.x();
+  to[stride*1] = from.y();
+  to[stride*2] = from.z();
+  to[stride*3] = from.w();
+}
+
+template <> EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pscatter(
+    Eigen::half* to, const sycl::cl_half2& from, Index stride) {
+  to[stride*0] = from.x();
+  to[stride*1] = from.y();
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void pscatter<Eigen::half, Packet4h2>(
+    Eigen::half* to, const Packet4h2& from, Index stride) {
+  const sycl::cl_half2* from_alias = reinterpret_cast<const sycl::cl_half2*>(&from);
+  pscatter(to + stride * 0, from_alias[0], stride);
+  pscatter(to + stride * 2, from_alias[1], stride);
+  pscatter(to + stride * 4, from_alias[2], stride);
+  pscatter(to + stride * 6, from_alias[3], stride);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Eigen::half pfirst<Packet4h2>(
+    const Packet4h2& a) {
+  return pfirst(*(reinterpret_cast<const sycl::cl_half2*>(&a)));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Eigen::bfloat16 pfirst<Packet2bf16>(
+    const Packet2bf16& a) {
+  return Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a[0]));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Eigen::bfloat16 pfirst<Packet4bf16>(
+    const Packet4bf16& a) {
+  return Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a[0]));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Eigen::bfloat16 pfirst<Packet8bf16>(
+    const Packet8bf16& a) {
+  return Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a[0]));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 pabs<Packet4h2>(
+    const Packet4h2& a) {
+  Packet4h2 r;
+  sycl::cl_half2* p_alias = reinterpret_cast<sycl::cl_half2*>(&r);
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  p_alias[0] = pabs(a_alias[0]);
+  p_alias[1] = pabs(a_alias[1]);
+  p_alias[2] = pabs(a_alias[2]);
+  p_alias[3] = pabs(a_alias[3]);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet8bf16 pabs<Packet8bf16>(
+    const Packet8bf16& a) {
+  Packet8bf16 r;
+  Packet8f32 a2float = Eigen::bfloat16_impl::Bf16ToF32<8>(a);
+  r = Eigen::bfloat16_impl::F32ToBf16<8>(sycl::fabs(a2float));
+  return r;
+}
+
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 ptrue<Packet4h2>(
+    const Packet4h2& a) {
+  Eigen::half true_half = Eigen::half_impl::raw_uint16_to_half(0xffffu);
+  return pset1<Packet4h2>(true_half);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 pzero<Packet4h2>(const Packet4h2& a) {
+  Eigen::half false_half = Eigen::half_impl::raw_uint16_to_half(0x0000u);
+  return pset1<Packet4h2>(false_half);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2
+plset<Packet4h2>(const Eigen::half& a) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias = reinterpret_cast<sycl::cl_half2*>(&r);
+
+  r_alias[0] = sycl::half2(a.x, a.x + 1.0f);
+  r_alias[1] = sycl::half2(a.x + 2.0f, a.x + 3.0f);
+  r_alias[2] = sycl::half2(a.x + 4.0f, a.x + 5.0f);
+  r_alias[3] = sycl::half2(a.x + 6.0f, a.x + 7.0f);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 pand<Packet4h2>(
+    const Packet4h2& a, const Packet4h2& b) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias  = reinterpret_cast<sycl::cl_half2*>(&r);
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  const sycl::cl_half2* b_alias = reinterpret_cast<const sycl::cl_half2*>(&b);
+  r_alias[0] = pand(a_alias[0], b_alias[0]);
+  r_alias[1] = pand(a_alias[1], b_alias[1]);
+  r_alias[2] = pand(a_alias[2], b_alias[2]);
+  r_alias[3] = pand(a_alias[3], b_alias[3]);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 por<Packet4h2>(
+    const Packet4h2& a, const Packet4h2& b) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias  = reinterpret_cast<sycl::cl_half2*>(&r);
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  const sycl::cl_half2* b_alias = reinterpret_cast<const sycl::cl_half2*>(&b);
+  r_alias[0] = por(a_alias[0], b_alias[0]);
+  r_alias[1] = por(a_alias[1], b_alias[1]);
+  r_alias[2] = por(a_alias[2], b_alias[2]);
+  r_alias[3] = por(a_alias[3], b_alias[3]);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 pxor<Packet4h2>(
+    const Packet4h2& a, const Packet4h2& b) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias  = reinterpret_cast<sycl::cl_half2*>(&r);
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  const sycl::cl_half2* b_alias = reinterpret_cast<const sycl::cl_half2*>(&b);
+  r_alias[0] = pxor(a_alias[0], b_alias[0]);
+  r_alias[1] = pxor(a_alias[1], b_alias[1]);
+  r_alias[2] = pxor(a_alias[2], b_alias[2]);
+  r_alias[3] = pxor(a_alias[3], b_alias[3]);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 pandnot<Packet4h2>(
+    const Packet4h2& a, const Packet4h2& b) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias  = reinterpret_cast<sycl::cl_half2*>(&r);
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  const sycl::cl_half2* b_alias = reinterpret_cast<const sycl::cl_half2*>(&b);
+  r_alias[0] = pandnot(a_alias[0], b_alias[0]);
+  r_alias[1] = pandnot(a_alias[1], b_alias[1]);
+  r_alias[2] = pandnot(a_alias[2], b_alias[2]);
+  r_alias[3] = pandnot(a_alias[3], b_alias[3]);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 padd<Packet4h2>(
+    const Packet4h2& a, const Packet4h2& b) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias  = reinterpret_cast<sycl::cl_half2*>(&r);
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  const sycl::cl_half2* b_alias = reinterpret_cast<const sycl::cl_half2*>(&b);
+  r_alias[0] = padd(a_alias[0], b_alias[0]);
+  r_alias[1] = padd(a_alias[1], b_alias[1]);
+  r_alias[2] = padd(a_alias[2], b_alias[2]);
+  r_alias[3] = padd(a_alias[3], b_alias[3]);
+  return r;
+}
+
+#define DPCPP_PACKET_BASIC_FUNC_BFLOAT16(NAME, OP)                      \
+template <>                                                             \
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet8bf16 p##NAME<Packet8bf16>( \
+    const Packet8bf16& a, const Packet8bf16& b) {                       \
+  Packet8f32 r = (Eigen::bfloat16_impl::Bf16ToF32<8>(a)) OP             \
+                 (Eigen::bfloat16_impl::Bf16ToF32<8>(b));               \
+  return Eigen::bfloat16_impl::F32ToBf16<8>(r);                         \
+}                                                                       \
+template <>                                                             \
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4bf16 p##NAME<Packet4bf16>( \
+    const Packet4bf16& a, const Packet4bf16& b) {                       \
+  Packet4f32 r = Eigen::bfloat16_impl::Bf16ToF32<4>(a) OP               \
+                 Eigen::bfloat16_impl::Bf16ToF32<4>(b);                 \
+  return Eigen::bfloat16_impl::F32ToBf16<4>(r);                         \
+}                                                                       \
+template <>                                                             \
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet2bf16 p##NAME<Packet2bf16>( \
+    const Packet2bf16& a, const Packet2bf16& b) {                       \
+  Packet2f32 r = Eigen::bfloat16_impl::Bf16ToF32<2>(a) OP               \
+                 Eigen::bfloat16_impl::Bf16ToF32<2>(b);                 \
+  return Eigen::bfloat16_impl::F32ToBf16<2>(r);                         \
+}
+
+DPCPP_PACKET_BASIC_FUNC_BFLOAT16(add, +);
+DPCPP_PACKET_BASIC_FUNC_BFLOAT16(sub, -);
+DPCPP_PACKET_BASIC_FUNC_BFLOAT16(mul, *);
+DPCPP_PACKET_BASIC_FUNC_BFLOAT16(div, /);
+#undef DPCPP_PACKET_BASIC_FUNC_BFLOAT16
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 pmadd<Packet4h2>(
+    const Packet4h2& a, const Packet4h2& b, const Packet4h2& c) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias  = reinterpret_cast<sycl::cl_half2*>(&r);
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  const sycl::cl_half2* b_alias = reinterpret_cast<const sycl::cl_half2*>(&b);
+  const sycl::cl_half2* c_alias = reinterpret_cast<const sycl::cl_half2*>(&c);
+  r_alias[0] = pmadd(a_alias[0], b_alias[0], c_alias[0]);
+  r_alias[1] = pmadd(a_alias[1], b_alias[1], c_alias[1]);
+  r_alias[2] = pmadd(a_alias[2], b_alias[2], c_alias[2]);
+  r_alias[3] = pmadd(a_alias[3], b_alias[3], c_alias[3]);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 psub<Packet4h2>(
+    const Packet4h2& a, const Packet4h2& b) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias  = reinterpret_cast<sycl::cl_half2*>(&r);
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  const sycl::cl_half2* b_alias = reinterpret_cast<const sycl::cl_half2*>(&b);
+  r_alias[0] = psub(a_alias[0], b_alias[0]);
+  r_alias[1] = psub(a_alias[1], b_alias[1]);
+  r_alias[2] = psub(a_alias[2], b_alias[2]);
+  r_alias[3] = psub(a_alias[3], b_alias[3]);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 pnegate<Packet4h2>(const Packet4h2& a) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias  = reinterpret_cast<sycl::cl_half2*>(&r);
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  r_alias[0] = pnegate(a_alias[0]);
+  r_alias[1] = pnegate(a_alias[1]);
+  r_alias[2] = pnegate(a_alias[2]);
+  r_alias[3] = pnegate(a_alias[3]);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet2bf16 pnegate<Packet2bf16>(
+    const Packet2bf16& a) {
+  Packet2f32 r = - Eigen::bfloat16_impl::Bf16ToF32<2>(a);
+  return Eigen::bfloat16_impl::F32ToBf16<2>(r);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4bf16 pnegate<Packet4bf16>(
+    const Packet4bf16& a) {
+  Packet4f32 r = - Eigen::bfloat16_impl::Bf16ToF32<4>(a);
+  return Eigen::bfloat16_impl::F32ToBf16<4>(r);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet8bf16 pnegate<Packet8bf16>(
+    const Packet8bf16& a) {
+  Packet8f32 r = - Eigen::bfloat16_impl::Bf16ToF32<8>(a);
+  return Eigen::bfloat16_impl::F32ToBf16<8>(r);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 pconj(const Packet4h2& a) {
+  return a;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet8bf16 pconj(const Packet8bf16& a) {
+  return a;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 pmul<Packet4h2>(
+    const Packet4h2& a, const Packet4h2& b) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias  = reinterpret_cast<sycl::cl_half2*>(&r);
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  const sycl::cl_half2* b_alias = reinterpret_cast<const sycl::cl_half2*>(&b);
+  r_alias[0] = pmul(a_alias[0], b_alias[0]);
+  r_alias[1] = pmul(a_alias[1], b_alias[1]);
+  r_alias[2] = pmul(a_alias[2], b_alias[2]);
+  r_alias[3] = pmul(a_alias[3], b_alias[3]);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet4h2 pdiv<Packet4h2>(
+    const Packet4h2& a, const Packet4h2& b) {
+  Packet4h2 r;
+  sycl::cl_half2* r_alias  = reinterpret_cast<sycl::cl_half2*>(&r);
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  const sycl::cl_half2* b_alias = reinterpret_cast<const sycl::cl_half2*>(&b);
+  r_alias[0] = pdiv(a_alias[0], b_alias[0]);
+  r_alias[1] = pdiv(a_alias[1], b_alias[1]);
+  r_alias[2] = pdiv(a_alias[2], b_alias[2]);
+  r_alias[3] = pdiv(a_alias[3], b_alias[3]);
+  return r;
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Eigen::half predux<Packet4h2>(
+    const Packet4h2& a) {
+  const sycl::cl_half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  return predux(a_alias[0]) + predux(a_alias[1]) +
+         predux(a_alias[2]) + predux(a_alias[3]);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Eigen::bfloat16 predux<Packet2bf16>(
+    const Packet2bf16& a) {
+    return Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a.x())) + Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a.y()));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Eigen::bfloat16 predux<Packet4bf16>(
+    const Packet4bf16& a) {
+  float r = predux(Eigen::bfloat16_impl::Bf16ToF32<4>(a)); 
+  return Eigen::bfloat16_impl::float_to_bfloat16_rtne<false>(r);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Eigen::bfloat16 predux<Packet8bf16>(
+    const Packet8bf16& a) {
+  const Packet4bf16* a_alias = reinterpret_cast<const Packet4bf16*>(&a);
+  float r = 0;
+#pragma unroll
+  for (int i = 0; i < 2; ++i) {
+    r += predux(Eigen::bfloat16_impl::Bf16ToF32<4>(a_alias[i]));
+  }
+  return Eigen::bfloat16_impl::float_to_bfloat16_rtne<false>(r);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Eigen::half predux_mul<Packet4h2>(
+    const Packet4h2& a) {
+  const sycl::half2* a_alias = reinterpret_cast<const sycl::cl_half2*>(&a);
+  return predux_mul(pmul(pmul(a_alias[0], a_alias[1]),
+                         pmul(a_alias[2], a_alias[3])));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Eigen::bfloat16 predux_mul<Packet2bf16>(
+    const Packet2bf16& a) {
+  return Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a.x())) * Eigen::bfloat16(bfloat16_impl::raw_uint16_to_bfloat16(a.y()));
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Eigen::bfloat16 predux_mul<Packet4bf16>(
+    const Packet4bf16& a) {
+  const Packet2bf16* a_alias = reinterpret_cast<const Packet2bf16*>(&a);
+  float r = predux_mul(Eigen::bfloat16_impl::Bf16ToF32<4>(a));
+  return Eigen::bfloat16_impl::float_to_bfloat16_rtne<false>(r);
+}
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Eigen::bfloat16 predux_mul<Packet8bf16>(
+    const Packet8bf16& a) {
+  const Packet4bf16* a_alias = reinterpret_cast<const Packet4bf16*>(&a);
+  float r = 0;
+#pragma unroll
+  for (int i = 0; i < 2; ++i) {
+    r += predux_mul(Eigen::bfloat16_impl::Bf16ToF32<4>(a_alias[i]));
+  }
+  return Eigen::bfloat16_impl::float_to_bfloat16_rtne<false>(r);
+}
+
+#endif  // DPCPP_DEVICE_ONLY
+
+}  // end namespace internal
+
+}  // end namespace Eigen
+
+#endif  // EIGEN_PACKET_MATH_DPCPP_H
diff --git a/Eigen/src/Core/arch/DPCPP/TypeCasting.h b/Eigen/src/Core/arch/DPCPP/TypeCasting.h
new file mode 100644
index 000000000..aa16baa85
--- /dev/null
+++ b/Eigen/src/Core/arch/DPCPP/TypeCasting.h
@@ -0,0 +1,79 @@
+// This file is part of Eigen, a lightweight C++ template library
+// for linear algebra.
+//
+// Mehdi Goli    Codeplay Software Ltd.
+// Ralph Potter  Codeplay Software Ltd.
+// Luke Iwanski  Codeplay Software Ltd.
+// Contact: <eigen@codeplay.com>
+//
+// This Source Code Form is subject to the terms of the Mozilla
+// Public License v. 2.0. If a copy of the MPL was not distributed
+// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+/*****************************************************************
+ * TypeCasting.h
+ *
+ * \brief:
+ *  TypeCasting
+ *
+ *****************************************************************/
+
+#ifndef EIGEN_TYPE_CASTING_DPCPP_H
+#define EIGEN_TYPE_CASTING_DPCPP_H
+
+namespace Eigen {
+
+namespace internal {
+#if defined(DPCPP_DEVICE_ONLY)
+
+template <>
+struct type_casting_traits<float, int> {
+  enum { VectorizedCast = 1, SrcCoeffRatio = 1, TgtCoeffRatio = 1 };
+};
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_int4
+pcast<sycl::cl_float4, sycl::cl_int4>(const sycl::cl_float4& a) {
+  return a
+      .template convert<sycl::cl_int, sycl::rounding_mode::automatic>();
+}
+
+template <>
+struct type_casting_traits<int, float> {
+  enum { VectorizedCast = 1, SrcCoeffRatio = 1, TgtCoeffRatio = 1 };
+};
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_float4
+pcast<sycl::cl_int4, sycl::cl_float4>(const sycl::cl_int4& a) {
+  return a.template convert<sycl::cl_float,
+                            sycl::rounding_mode::automatic>();
+}
+
+template <>
+struct type_casting_traits<double, float> {
+  enum { VectorizedCast = 1, SrcCoeffRatio = 2, TgtCoeffRatio = 1 };
+};
+
+template <>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_float4
+pcast<sycl::cl_double2, sycl::cl_float4>(
+    const sycl::cl_double2& a, const sycl::cl_double2& b) {
+  auto a1 = a.template convert<sycl::cl_float,
+                               sycl::rounding_mode::automatic>();
+  auto b1 = b.template convert<sycl::cl_float,
+                               sycl::rounding_mode::automatic>();
+  return sycl::float4(a1.x(), a1.y(), b1.x(), b1.y());
+}
+
+template <>
+struct type_casting_traits<float, double> {
+  enum { VectorizedCast = 1, SrcCoeffRatio = 1, TgtCoeffRatio = 2 };
+};
+
+#endif
+}  // end namespace internal
+
+}  // end namespace Eigen
+
+#endif  // EIGEN_TYPE_CASTING_DPCPP_H
diff --git a/Eigen/src/Core/arch/Default/BFloat16.h b/Eigen/src/Core/arch/Default/BFloat16.h
index 30f3cd456..e4d138869 100644
--- a/Eigen/src/Core/arch/Default/BFloat16.h
+++ b/Eigen/src/Core/arch/Default/BFloat16.h
@@ -263,12 +263,7 @@ EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw truncate_to_bfloat16(const
     output.value = std::signbit(v) ? 0x8000 : 0;
     return output;
   }
-  const uint16_t* p = reinterpret_cast<const uint16_t*>(&v);
-#if defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
-  output.value = p[0];
-#else
-  output.value = p[1];
-#endif
+  output.value = static_cast<numext::uint16_t>(numext::bit_cast<numext::uint32_t>(v) >> 16);
   return output;
 }
 
@@ -475,14 +470,7 @@ EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw float_to_bfloat16_rtne<true
 }
 
 EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC float bfloat16_to_float(__bfloat16_raw h) {
-    float result = 0;
-    unsigned short* q = reinterpret_cast<unsigned short*>(&result);
-#if defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
-    q[0] = h.value;
-#else
-    q[1] = h.value;
-#endif
-    return result;
+    return numext::bit_cast<float>(static_cast<numext::uint32_t>(h.value) << 16);
 }
 // --- standard functions ---
 
@@ -566,6 +554,9 @@ EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 floor(const bfloat16& a) {
 EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 ceil(const bfloat16& a) {
   return bfloat16(::ceilf(float(a)));
 }
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 rint(const bfloat16& a) {
+  return bfloat16(::rint(float(a)));
+}
 EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 fmod(const bfloat16& a, const bfloat16& b) {
   return bfloat16(::fmodf(float(a), float(b)));
 }
@@ -674,6 +665,12 @@ bool (isfinite)(const Eigen::bfloat16& h) {
   return (bfloat16_impl::isfinite)(h);
 }
 
+template<>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
+Eigen::bfloat16 (rint)(const Eigen::bfloat16& h) {
+  return (bfloat16_impl::rint)(h);
+}
+
 } // namespace numext
 }  // namespace Eigen
 
diff --git a/Eigen/src/Core/arch/Default/Half.h b/Eigen/src/Core/arch/Default/Half.h
index bbc15d463..535901194 100644
--- a/Eigen/src/Core/arch/Default/Half.h
+++ b/Eigen/src/Core/arch/Default/Half.h
@@ -81,7 +81,7 @@ struct __half_raw {
  #endif // defined(EIGEN_HAS_CUDA_FP16)
 
 #elif defined(SYCL_DEVICE_ONLY)
-typedef cl::sycl::half __half_raw;
+typedef sycl::half __half_raw;
 
 #endif
 
@@ -623,6 +623,219 @@ EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half (max)(const half& a, const half& b) {
 #endif
 }
 
+#if EIGEN_USE_DPCPP
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half acos(const half& a) {
+  return Eigen::half(::acos(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half acosh(const half& a) {
+  return Eigen::half(::acosh(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half acospi(const half& a) {
+  return Eigen::half(::acos(float(a)) / EIGEN_PI);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half asin(const half& a) {
+  return Eigen::half(::asin(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half asinh(const half& a) {
+  return Eigen::half(::asinh(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half asinpi(const half& a) {
+  return Eigen::half(::asin(float(a)) / EIGEN_PI);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half atan(const half& a) {
+  return Eigen::half(::atan(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half atan2(const half& a, const half& b) {
+  return Eigen::half(::atan2(float(a), float(b)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half atanh(const half& a) {
+  return Eigen::half(::atanh(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half atanpi(const half& a) {
+  return Eigen::half(::atan(float(a)) / EIGEN_PI);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half atan2pi(const half& a, const half& b) {
+  return Eigen::half(::atan2(float(a), float(b)) / EIGEN_PI);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half cbrt(const half& a) {
+  return Eigen::half(::cbrt(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half copysign(const half& a,const half& b) {
+  return Eigen::half(::copysign(float(a), float(b)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half cosh(const half& a) {
+  return Eigen::half(::cosh(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half cospi(const half& a) {
+  return Eigen::half(::cos(float(a)) / EIGEN_PI);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half erfc(const half& a) {
+  return Eigen::half(::erfc(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half erf(const half& a) {
+  return Eigen::half(::erf(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half exp2(const half& a) {
+  return Eigen::half(::exp2(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half exp10(const half& a) {
+  return Eigen::half(::exp10(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fabs(const half& a) {
+  return Eigen::half(::fabs(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fdim(const half& a, const half& b) {
+  return Eigen::half(::fdim(float(a), float(b)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fma(const half& a, const half& b, const half& c) {
+  return Eigen::half(::fma(float(a), float(b), float(c)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fmax(const half& a, const half& b) {
+  return Eigen::half(::fmax(float(a), float(b)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fmin(const half& a, const half& b) {
+  return Eigen::half(::fmin(float(a), float(b)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fmod(const half& a, const half& b) {
+  return Eigen::half(::fmod(float(a), float(b)));
+}
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fract(const half& a, const half* b) {
+//   return Eigen::half(::fract(float(a), &(b->x)));
+// }
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fract(const half& a, ::cl_int* b) {
+//   return Eigen::half(::fract(float(a), b));
+// }
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half hypot(const half& a, const half& b) {
+  return Eigen::half(::hypot(float(a), float(b)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half ilogb(const half& a) {
+  return Eigen::half(::ilogb(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half ldexp(const half& a, ::cl_int& b) {
+  return Eigen::half(::ldexp(float(a), b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half lgamma(const half& a) {
+  return Eigen::half(::lgamma(float(a)));
+}
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half lgamma_r(const half& a, ::cl_int* b) {
+//   return Eigen::half(::lgamma_r(float(a), b));
+// }
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half log2(const half& a) {
+  return Eigen::half(::log2(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half logb(const half& a) {
+  return Eigen::half(::logb(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half mad(const half& a, const half& b, const half& c) {
+  return Eigen::half(float(a) * float(b) + float(c));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half maxmag(const half& a, const half& b) {
+  float absa = ::fabs(float(a));
+  float absb = ::fabs(float(b));
+  if (absa > absb)
+    return a;
+  else if (absa < absb)
+    return b;
+  else
+    return max(a, b);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half minmag(const half& a, const half& b) {
+  float absa = ::fabs(float(a));
+  float absb = ::fabs(float(b));
+  if (absa > absb)
+    return b;
+  else if (absa < absb)
+    return a;
+  else
+    return min(a, b);
+}
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half modf(const half& a, const half* b) {
+//   return Eigen::half(::modf(float(a), &(b->x)));
+// }
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half nan(unsigned int u) {
+  return half_impl::raw_uint16_to_half(0x7c01);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half nextafter(const half& a, const half& b) {
+  return Eigen::half(::nextafter(float(a), float(b)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half pow(const half& a, const ::cl_int& b) {
+  return Eigen::half(::pow(float(a), b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half powr(const half& a, const half& b) {
+  return Eigen::half(::pow(float(a), float(b)));
+}
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half remquo(const half& a, const half& b, const ::cl_int* c) {
+//   return Eigen::half(::remquo(float(a), float(b), c));
+// }
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half rint(const half& a) {
+  return Eigen::half(::rint(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half rootn(const half& a, const ::cl_int& b) {
+  return Eigen::half(::pow(float(a), 1/b));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half round(const half& a) {
+  return Eigen::half(::round(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half rsqrt(const half& a) {
+  return Eigen::half(1 / ::sqrt(float(a)));
+}
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half sincos(const half& a, const half* b) {
+//   return Eigen::half(::sincos(float(a), b));
+// }
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half sinh(const half& a) {
+  return Eigen::half(::sinh(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half sinpi(const half& a) {
+  return Eigen::half(::sin(float(a)) / EIGEN_PI);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half tanpi(const half& a) {
+  return Eigen::half(::tan(float(a)) / EIGEN_PI);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half tgamma(const half& a) {
+  return Eigen::half(::tgamma(float(a)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half trunc(const half& a) {
+  return Eigen::half(::trunc(float(a)));
+}
+// only enable for type T which is_genfloatf<T>::value is true
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half divide(const half& a, const half& b) {
+//   return Eigen::half(::half_precision::divide(float(a), float(b)));
+// }
+// EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half recip(const half& a) {
+//   return Eigen::half(::half_precision::recip(float(a)));
+// }
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half clamp(const half& a, const half& b, const half& c) {
+  return min(max(a, b), c);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half degrees(const half& a) {
+  return Eigen::half((180 / EIGEN_PI) * float(a));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half radians(const half& a) {
+  return Eigen::half((EIGEN_PI / 180) * float(a));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half step(const half& a, const half& b) {
+  if (float(b) < float(a))
+    return Eigen::half(0.0);
+  else
+    return Eigen::half(1.0);
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half smoothstep(const half& a, const half& b, const half& c) {
+  auto t = clamp(Eigen::half(float((c <= a) / (b >= a))), Eigen::half(0.0), Eigen::half(1.0));
+  return t * t * Eigen::half(float(3.0 - 2.0 * float(t)));
+}
+EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half sign(const half& a) {
+  float fa = float(a);
+  if (fa > 0.0)
+    return Eigen::half(1.0);
+  else if (fa < 0.0)
+    return Eigen::half(-1.0);
+  else if(isnan(a))
+    return Eigen::half(0.0);
+
+  return a;
+}
+#endif // EIGEN_USE_DPCPP
+
 #ifndef EIGEN_NO_IO
 EIGEN_ALWAYS_INLINE std::ostream& operator << (std::ostream& os, const half& v) {
   os << static_cast<float>(v);
@@ -773,6 +986,12 @@ bool (isfinite)(const Eigen::half& h) {
   return (half_impl::isfinite)(h);
 }
 
+template<>
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
+Eigen::half& (rint)(const Eigen::half& h) {
+  return (half_impl::rint)(h);
+}
+
 } // namespace Eigen
 }  // namespace numext
 #endif
diff --git a/Eigen/src/Core/arch/SYCL/InteropHeaders.h b/Eigen/src/Core/arch/SYCL/InteropHeaders.h
index 710059d50..90973499e 100644
--- a/Eigen/src/Core/arch/SYCL/InteropHeaders.h
+++ b/Eigen/src/Core/arch/SYCL/InteropHeaders.h
@@ -81,10 +81,10 @@ struct sycl_packet_traits : default_packet_traits {
     typedef packet_type half;                                              \
   };
 
-SYCL_PACKET_TRAITS(cl::sycl::cl_float4, 1, float, 4)
-SYCL_PACKET_TRAITS(cl::sycl::cl_float4, 1, const float, 4)
-SYCL_PACKET_TRAITS(cl::sycl::cl_double2, 0, double, 2)
-SYCL_PACKET_TRAITS(cl::sycl::cl_double2, 0, const double, 2)
+SYCL_PACKET_TRAITS(sycl::cl_float4, 1, float, 4)
+SYCL_PACKET_TRAITS(sycl::cl_float4, 1, const float, 4)
+SYCL_PACKET_TRAITS(sycl::cl_double2, 0, double, 2)
+SYCL_PACKET_TRAITS(sycl::cl_double2, 0, const double, 2)
 #undef SYCL_PACKET_TRAITS
 
 // Make sure this is only available when targeting a GPU: we don't want to
@@ -95,8 +95,8 @@ SYCL_PACKET_TRAITS(cl::sycl::cl_double2, 0, const double, 2)
   struct is_arithmetic<packet_type> { \
     enum { value = true };            \
   };
-SYCL_ARITHMETIC(cl::sycl::cl_float4)
-SYCL_ARITHMETIC(cl::sycl::cl_double2)
+SYCL_ARITHMETIC(sycl::cl_float4)
+SYCL_ARITHMETIC(sycl::cl_double2)
 #undef SYCL_ARITHMETIC
 
 #define SYCL_UNPACKET_TRAITS(packet_type, unpacket_type, lengths)        \
@@ -106,8 +106,8 @@ SYCL_ARITHMETIC(cl::sycl::cl_double2)
     enum { size = lengths, vectorizable = true, alignment = Aligned16 }; \
     typedef packet_type half;                                            \
   };
-SYCL_UNPACKET_TRAITS(cl::sycl::cl_float4, float, 4)
-SYCL_UNPACKET_TRAITS(cl::sycl::cl_double2, double, 2)
+SYCL_UNPACKET_TRAITS(sycl::cl_float4, float, 4)
+SYCL_UNPACKET_TRAITS(sycl::cl_double2, double, 2)
 
 #undef SYCL_UNPACKET_TRAITS
 #endif
diff --git a/Eigen/src/Core/arch/SYCL/MathFunctions.h b/Eigen/src/Core/arch/SYCL/MathFunctions.h
index a96625e2c..d9c067140 100644
--- a/Eigen/src/Core/arch/SYCL/MathFunctions.h
+++ b/Eigen/src/Core/arch/SYCL/MathFunctions.h
@@ -33,77 +33,77 @@ namespace internal {
   template <>                                                          \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type plog<packet_type>( \
       const packet_type& a) {                                          \
-    return cl::sycl::log(a);                                           \
+    return sycl::log(a);                                           \
   }
 
-SYCL_PLOG(cl::sycl::cl_float4)
-SYCL_PLOG(cl::sycl::cl_double2)
+SYCL_PLOG(sycl::cl_float4)
+SYCL_PLOG(sycl::cl_double2)
 #undef SYCL_PLOG
 
 #define SYCL_PLOG1P(packet_type)                                         \
   template <>                                                            \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type plog1p<packet_type>( \
       const packet_type& a) {                                            \
-    return cl::sycl::log1p(a);                                           \
+    return sycl::log1p(a);                                           \
   }
 
-SYCL_PLOG1P(cl::sycl::cl_float4)
-SYCL_PLOG1P(cl::sycl::cl_double2)
+SYCL_PLOG1P(sycl::cl_float4)
+SYCL_PLOG1P(sycl::cl_double2)
 #undef SYCL_PLOG1P
 
 #define SYCL_PLOG10(packet_type)                                         \
   template <>                                                            \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type plog10<packet_type>( \
       const packet_type& a) {                                            \
-    return cl::sycl::log10(a);                                           \
+    return sycl::log10(a);                                           \
   }
 
-SYCL_PLOG10(cl::sycl::cl_float4)
-SYCL_PLOG10(cl::sycl::cl_double2)
+SYCL_PLOG10(sycl::cl_float4)
+SYCL_PLOG10(sycl::cl_double2)
 #undef SYCL_PLOG10
 
 #define SYCL_PEXP(packet_type)                                         \
   template <>                                                          \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type pexp<packet_type>( \
       const packet_type& a) {                                          \
-    return cl::sycl::exp(a);                                           \
+    return sycl::exp(a);                                           \
   }
 
-SYCL_PEXP(cl::sycl::cl_float4)
-SYCL_PEXP(cl::sycl::cl_double2)
+SYCL_PEXP(sycl::cl_float4)
+SYCL_PEXP(sycl::cl_double2)
 #undef SYCL_PEXP
 
 #define SYCL_PEXPM1(packet_type)                                         \
   template <>                                                            \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type pexpm1<packet_type>( \
       const packet_type& a) {                                            \
-    return cl::sycl::expm1(a);                                           \
+    return sycl::expm1(a);                                           \
   }
 
-SYCL_PEXPM1(cl::sycl::cl_float4)
-SYCL_PEXPM1(cl::sycl::cl_double2)
+SYCL_PEXPM1(sycl::cl_float4)
+SYCL_PEXPM1(sycl::cl_double2)
 #undef SYCL_PEXPM1
 
 #define SYCL_PSQRT(packet_type)                                         \
   template <>                                                           \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type psqrt<packet_type>( \
       const packet_type& a) {                                           \
-    return cl::sycl::sqrt(a);                                           \
+    return sycl::sqrt(a);                                           \
   }
 
-SYCL_PSQRT(cl::sycl::cl_float4)
-SYCL_PSQRT(cl::sycl::cl_double2)
+SYCL_PSQRT(sycl::cl_float4)
+SYCL_PSQRT(sycl::cl_double2)
 #undef SYCL_PSQRT
 
 #define SYCL_PRSQRT(packet_type)                                         \
   template <>                                                            \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type prsqrt<packet_type>( \
       const packet_type& a) {                                            \
-    return cl::sycl::rsqrt(a);                                           \
+    return sycl::rsqrt(a);                                           \
   }
 
-SYCL_PRSQRT(cl::sycl::cl_float4)
-SYCL_PRSQRT(cl::sycl::cl_double2)
+SYCL_PRSQRT(sycl::cl_float4)
+SYCL_PRSQRT(sycl::cl_double2)
 #undef SYCL_PRSQRT
 
 /** \internal \returns the hyperbolic sine of \a a (coeff-wise) */
@@ -111,11 +111,11 @@ SYCL_PRSQRT(cl::sycl::cl_double2)
   template <>                                                          \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type psin<packet_type>( \
       const packet_type& a) {                                          \
-    return cl::sycl::sin(a);                                           \
+    return sycl::sin(a);                                           \
   }
 
-SYCL_PSIN(cl::sycl::cl_float4)
-SYCL_PSIN(cl::sycl::cl_double2)
+SYCL_PSIN(sycl::cl_float4)
+SYCL_PSIN(sycl::cl_double2)
 #undef SYCL_PSIN
 
 /** \internal \returns the hyperbolic cosine of \a a (coeff-wise) */
@@ -123,11 +123,11 @@ SYCL_PSIN(cl::sycl::cl_double2)
   template <>                                                          \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type pcos<packet_type>( \
       const packet_type& a) {                                          \
-    return cl::sycl::cos(a);                                           \
+    return sycl::cos(a);                                           \
   }
 
-SYCL_PCOS(cl::sycl::cl_float4)
-SYCL_PCOS(cl::sycl::cl_double2)
+SYCL_PCOS(sycl::cl_float4)
+SYCL_PCOS(sycl::cl_double2)
 #undef SYCL_PCOS
 
 /** \internal \returns the hyperbolic tan of \a a (coeff-wise) */
@@ -135,11 +135,11 @@ SYCL_PCOS(cl::sycl::cl_double2)
   template <>                                                          \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type ptan<packet_type>( \
       const packet_type& a) {                                          \
-    return cl::sycl::tan(a);                                           \
+    return sycl::tan(a);                                           \
   }
 
-SYCL_PTAN(cl::sycl::cl_float4)
-SYCL_PTAN(cl::sycl::cl_double2)
+SYCL_PTAN(sycl::cl_float4)
+SYCL_PTAN(sycl::cl_double2)
 #undef SYCL_PTAN
 
 /** \internal \returns the hyperbolic sine of \a a (coeff-wise) */
@@ -147,11 +147,11 @@ SYCL_PTAN(cl::sycl::cl_double2)
   template <>                                                           \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type pasin<packet_type>( \
       const packet_type& a) {                                           \
-    return cl::sycl::asin(a);                                           \
+    return sycl::asin(a);                                           \
   }
 
-SYCL_PASIN(cl::sycl::cl_float4)
-SYCL_PASIN(cl::sycl::cl_double2)
+SYCL_PASIN(sycl::cl_float4)
+SYCL_PASIN(sycl::cl_double2)
 #undef SYCL_PASIN
 
 /** \internal \returns the hyperbolic cosine of \a a (coeff-wise) */
@@ -159,11 +159,11 @@ SYCL_PASIN(cl::sycl::cl_double2)
   template <>                                                           \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type pacos<packet_type>( \
       const packet_type& a) {                                           \
-    return cl::sycl::acos(a);                                           \
+    return sycl::acos(a);                                           \
   }
 
-SYCL_PACOS(cl::sycl::cl_float4)
-SYCL_PACOS(cl::sycl::cl_double2)
+SYCL_PACOS(sycl::cl_float4)
+SYCL_PACOS(sycl::cl_double2)
 #undef SYCL_PACOS
 
 /** \internal \returns the hyperbolic tan of \a a (coeff-wise) */
@@ -171,11 +171,11 @@ SYCL_PACOS(cl::sycl::cl_double2)
   template <>                                                           \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type patan<packet_type>( \
       const packet_type& a) {                                           \
-    return cl::sycl::atan(a);                                           \
+    return sycl::atan(a);                                           \
   }
 
-SYCL_PATAN(cl::sycl::cl_float4)
-SYCL_PATAN(cl::sycl::cl_double2)
+SYCL_PATAN(sycl::cl_float4)
+SYCL_PATAN(sycl::cl_double2)
 #undef SYCL_PATAN
 
 /** \internal \returns the hyperbolic sine of \a a (coeff-wise) */
@@ -183,11 +183,11 @@ SYCL_PATAN(cl::sycl::cl_double2)
   template <>                                                           \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type psinh<packet_type>( \
       const packet_type& a) {                                           \
-    return cl::sycl::sinh(a);                                           \
+    return sycl::sinh(a);                                           \
   }
 
-SYCL_PSINH(cl::sycl::cl_float4)
-SYCL_PSINH(cl::sycl::cl_double2)
+SYCL_PSINH(sycl::cl_float4)
+SYCL_PSINH(sycl::cl_double2)
 #undef SYCL_PSINH
 
 /** \internal \returns the hyperbolic cosine of \a a (coeff-wise) */
@@ -195,11 +195,11 @@ SYCL_PSINH(cl::sycl::cl_double2)
   template <>                                                           \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type pcosh<packet_type>( \
       const packet_type& a) {                                           \
-    return cl::sycl::cosh(a);                                           \
+    return sycl::cosh(a);                                           \
   }
 
-SYCL_PCOSH(cl::sycl::cl_float4)
-SYCL_PCOSH(cl::sycl::cl_double2)
+SYCL_PCOSH(sycl::cl_float4)
+SYCL_PCOSH(sycl::cl_double2)
 #undef SYCL_PCOSH
 
 /** \internal \returns the hyperbolic tan of \a a (coeff-wise) */
@@ -207,55 +207,55 @@ SYCL_PCOSH(cl::sycl::cl_double2)
   template <>                                                           \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type ptanh<packet_type>( \
       const packet_type& a) {                                           \
-    return cl::sycl::tanh(a);                                           \
+    return sycl::tanh(a);                                           \
   }
 
-SYCL_PTANH(cl::sycl::cl_float4)
-SYCL_PTANH(cl::sycl::cl_double2)
+SYCL_PTANH(sycl::cl_float4)
+SYCL_PTANH(sycl::cl_double2)
 #undef SYCL_PTANH
 
 #define SYCL_PCEIL(packet_type)                                         \
   template <>                                                           \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type pceil<packet_type>( \
       const packet_type& a) {                                           \
-    return cl::sycl::ceil(a);                                           \
+    return sycl::ceil(a);                                           \
   }
 
-SYCL_PCEIL(cl::sycl::cl_float4)
-SYCL_PCEIL(cl::sycl::cl_double2)
+SYCL_PCEIL(sycl::cl_float4)
+SYCL_PCEIL(sycl::cl_double2)
 #undef SYCL_PCEIL
 
 #define SYCL_PROUND(packet_type)                                         \
   template <>                                                            \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type pround<packet_type>( \
       const packet_type& a) {                                            \
-    return cl::sycl::round(a);                                           \
+    return sycl::round(a);                                           \
   }
 
-SYCL_PROUND(cl::sycl::cl_float4)
-SYCL_PROUND(cl::sycl::cl_double2)
+SYCL_PROUND(sycl::cl_float4)
+SYCL_PROUND(sycl::cl_double2)
 #undef SYCL_PROUND
 
 #define SYCL_PRINT(packet_type)                                         \
   template<>                                                            \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type print<packet_type>( \
       const packet_type& a) {                                           \
-    return cl::sycl::rint(a);                                           \
+    return sycl::rint(a);                                           \
   }
 
-SYCL_PRINT(cl::sycl::cl_float4)
-SYCL_PRINT(cl::sycl::cl_double2)
+SYCL_PRINT(sycl::cl_float4)
+SYCL_PRINT(sycl::cl_double2)
 #undef SYCL_PRINT
 
 #define SYCL_FLOOR(packet_type)                                          \
   template <>                                                            \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type pfloor<packet_type>( \
       const packet_type& a) {                                            \
-    return cl::sycl::floor(a);                                           \
+    return sycl::floor(a);                                           \
   }
 
-SYCL_FLOOR(cl::sycl::cl_float4)
-SYCL_FLOOR(cl::sycl::cl_double2)
+SYCL_FLOOR(sycl::cl_float4)
+SYCL_FLOOR(sycl::cl_double2)
 #undef SYCL_FLOOR
 
 #define SYCL_PMIN(packet_type, expr)                                   \
@@ -265,8 +265,8 @@ SYCL_FLOOR(cl::sycl::cl_double2)
     return expr;                                                       \
   }
 
-SYCL_PMIN(cl::sycl::cl_float4, cl::sycl::fmin(a, b))
-SYCL_PMIN(cl::sycl::cl_double2, cl::sycl::fmin(a, b))
+SYCL_PMIN(sycl::cl_float4, sycl::fmin(a, b))
+SYCL_PMIN(sycl::cl_double2, sycl::fmin(a, b))
 #undef SYCL_PMIN
 
 #define SYCL_PMAX(packet_type, expr)                                   \
@@ -276,8 +276,8 @@ SYCL_PMIN(cl::sycl::cl_double2, cl::sycl::fmin(a, b))
     return expr;                                                       \
   }
 
-SYCL_PMAX(cl::sycl::cl_float4, cl::sycl::fmax(a, b))
-SYCL_PMAX(cl::sycl::cl_double2, cl::sycl::fmax(a, b))
+SYCL_PMAX(sycl::cl_float4, sycl::fmax(a, b))
+SYCL_PMAX(sycl::cl_double2, sycl::fmax(a, b))
 #undef SYCL_PMAX
 
 #endif
diff --git a/Eigen/src/Core/arch/SYCL/PacketMath.h b/Eigen/src/Core/arch/SYCL/PacketMath.h
index 87badc076..9488ef0c4 100644
--- a/Eigen/src/Core/arch/SYCL/PacketMath.h
+++ b/Eigen/src/Core/arch/SYCL/PacketMath.h
@@ -29,13 +29,13 @@ namespace internal {
 #define SYCL_PLOADT_RO(address_space_target)                                 \
   template <typename packet_type, int Alignment>                             \
   EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type ploadt_ro(               \
-      typename cl::sycl::multi_ptr<                                          \
+      typename sycl::multi_ptr<                                          \
           const typename unpacket_traits<packet_type>::type,                 \
-          cl::sycl::access::address_space::address_space_target>::pointer_t  \
+          sycl::access::address_space::address_space_target>::pointer_t  \
           from) {                                                            \
     typedef typename unpacket_traits<packet_type>::type scalar;              \
-    typedef cl::sycl::multi_ptr<                                             \
-        scalar, cl::sycl::access::address_space::address_space_target>       \
+    typedef sycl::multi_ptr<                                             \
+        scalar, sycl::access::address_space::address_space_target>       \
         multi_ptr;                                                           \
     auto res = packet_type(                                                  \
         static_cast<typename unpacket_traits<packet_type>::type>(0));        \
@@ -51,7 +51,7 @@ SYCL_PLOADT_RO(local_space)
 template <typename packet_type, int Alignment, typename T>
 EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type
 ploadt_ro(const Eigen::TensorSycl::internal::RangeAccess<
-          cl::sycl::access::mode::read_write, T>& from) {
+          sycl::access::mode::read_write, T>& from) {
   return ploadt_ro<packet_type, Alignment>(from.get_pointer());
 }
 
@@ -59,9 +59,9 @@ ploadt_ro(const Eigen::TensorSycl::internal::RangeAccess<
 #define SYCL_PLOAD(address_space_target, Alignment, AlignedType)            \
   template <typename packet_type>                                           \
   EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pload##AlignedType(     \
-      typename cl::sycl::multi_ptr<                                         \
+      typename sycl::multi_ptr<                                         \
           const typename unpacket_traits<packet_type>::type,                \
-          cl::sycl::access::address_space::address_space_target>::pointer_t \
+          sycl::access::address_space::address_space_target>::pointer_t \
           from) {                                                           \
     return ploadt_ro<packet_type, Alignment>(from);                         \
   }
@@ -80,7 +80,7 @@ SYCL_PLOAD(local_space, Aligned, )
   template <typename packet_type>                                       \
   EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pload##AlignedType( \
       const Eigen::TensorSycl::internal::RangeAccess<                   \
-          cl::sycl::access::mode::read_write,                           \
+          sycl::access::mode::read_write,                           \
           typename unpacket_traits<packet_type>::type>                  \
           from) {                                                       \
     return ploadt_ro<packet_type, Alignment>(from);                     \
@@ -95,9 +95,9 @@ SYCL_PLOAD(Aligned, )
 #define SYCL_PLOADT(address_space_target)                                   \
   template <typename packet_type, int Alignment>                            \
   EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type ploadt(                 \
-      typename cl::sycl::multi_ptr<                                         \
+      typename sycl::multi_ptr<                                         \
           const typename unpacket_traits<packet_type>::type,                \
-          cl::sycl::access::address_space::address_space_target>::pointer_t \
+          sycl::access::address_space::address_space_target>::pointer_t \
           from) {                                                           \
     if (Alignment >= unpacket_traits<packet_type>::alignment)               \
       return pload<packet_type>(from);                                      \
@@ -115,7 +115,7 @@ SYCL_PLOADT(local_space)
 template <typename packet_type, int Alignment>
 EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type
 ploadt(const Eigen::TensorSycl::internal::RangeAccess<
-       cl::sycl::access::mode::read_write,
+       sycl::access::mode::read_write,
        typename unpacket_traits<packet_type>::type>& from) {
   return ploadt<packet_type, Alignment>(from.get_pointer());
 }
@@ -129,15 +129,15 @@ ploadt(const Eigen::TensorSycl::internal::RangeAccess<
       const typename unpacket_traits<packet_type>::type* from) {       \
     typedef typename unpacket_traits<packet_type>::type scalar;        \
     auto res = packet_type(static_cast<scalar>(0));                    \
-    res.template load<cl::sycl::access::address_space::private_space>( \
+    res.template load<sycl::access::address_space::private_space>( \
         0, const_cast<scalar*>(from));                                 \
     return res;                                                        \
   }
 
-SYCL_PLOADT_RO_SPECIAL(cl::sycl::cl_float4, Aligned)
-SYCL_PLOADT_RO_SPECIAL(cl::sycl::cl_double2, Aligned)
-SYCL_PLOADT_RO_SPECIAL(cl::sycl::cl_float4, Unaligned)
-SYCL_PLOADT_RO_SPECIAL(cl::sycl::cl_double2, Unaligned)
+SYCL_PLOADT_RO_SPECIAL(sycl::cl_float4, Aligned)
+SYCL_PLOADT_RO_SPECIAL(sycl::cl_double2, Aligned)
+SYCL_PLOADT_RO_SPECIAL(sycl::cl_float4, Unaligned)
+SYCL_PLOADT_RO_SPECIAL(sycl::cl_double2, Unaligned)
 
 #define SYCL_PLOAD_SPECIAL(packet_type, alignment_type)                    \
   template <>                                                              \
@@ -145,53 +145,53 @@ SYCL_PLOADT_RO_SPECIAL(cl::sycl::cl_double2, Unaligned)
       const typename unpacket_traits<packet_type>::type* from) {           \
     typedef typename unpacket_traits<packet_type>::type scalar;            \
     auto res = packet_type(static_cast<scalar>(0));                        \
-    res.template load<cl::sycl::access::address_space::private_space>(     \
+    res.template load<sycl::access::address_space::private_space>(     \
         0, const_cast<scalar*>(from));                                     \
     return res;                                                            \
   }
-SYCL_PLOAD_SPECIAL(cl::sycl::cl_float4, )
-SYCL_PLOAD_SPECIAL(cl::sycl::cl_double2, )
-SYCL_PLOAD_SPECIAL(cl::sycl::cl_float4, u)
-SYCL_PLOAD_SPECIAL(cl::sycl::cl_double2, u)
+SYCL_PLOAD_SPECIAL(sycl::cl_float4, )
+SYCL_PLOAD_SPECIAL(sycl::cl_double2, )
+SYCL_PLOAD_SPECIAL(sycl::cl_float4, u)
+SYCL_PLOAD_SPECIAL(sycl::cl_double2, u)
 
 #undef SYCL_PLOAD_SPECIAL
 
 #define SYCL_PSTORE(scalar, packet_type, address_space_target, alignment)   \
   template <>                                                               \
   EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE void pstore##alignment(             \
-      typename cl::sycl::multi_ptr<                                         \
+      typename sycl::multi_ptr<                                         \
           scalar,                                                           \
-          cl::sycl::access::address_space::address_space_target>::pointer_t \
+          sycl::access::address_space::address_space_target>::pointer_t \
           to,                                                               \
       const packet_type& from) {                                            \
-    typedef cl::sycl::multi_ptr<                                            \
-        scalar, cl::sycl::access::address_space::address_space_target>      \
+    typedef sycl::multi_ptr<                                            \
+        scalar, sycl::access::address_space::address_space_target>      \
         multi_ptr;                                                          \
     from.store(0, multi_ptr(to));                                           \
   }
 
 // global space
-SYCL_PSTORE(float, cl::sycl::cl_float4, global_space, )
-SYCL_PSTORE(float, cl::sycl::cl_float4, global_space, u)
-SYCL_PSTORE(double, cl::sycl::cl_double2, global_space, )
-SYCL_PSTORE(double, cl::sycl::cl_double2, global_space, u)
-SYCL_PSTORE(float, cl::sycl::cl_float4, local_space, )
-SYCL_PSTORE(float, cl::sycl::cl_float4, local_space, u)
-SYCL_PSTORE(double, cl::sycl::cl_double2, local_space, )
-SYCL_PSTORE(double, cl::sycl::cl_double2, local_space, u)
-
-SYCL_PSTORE(float, cl::sycl::cl_float4, private_space, )
-SYCL_PSTORE(float, cl::sycl::cl_float4, private_space, u)
-SYCL_PSTORE(double, cl::sycl::cl_double2, private_space, )
-SYCL_PSTORE(double, cl::sycl::cl_double2, private_space, u)
+SYCL_PSTORE(float, sycl::cl_float4, global_space, )
+SYCL_PSTORE(float, sycl::cl_float4, global_space, u)
+SYCL_PSTORE(double, sycl::cl_double2, global_space, )
+SYCL_PSTORE(double, sycl::cl_double2, global_space, u)
+SYCL_PSTORE(float, sycl::cl_float4, local_space, )
+SYCL_PSTORE(float, sycl::cl_float4, local_space, u)
+SYCL_PSTORE(double, sycl::cl_double2, local_space, )
+SYCL_PSTORE(double, sycl::cl_double2, local_space, u)
+
+SYCL_PSTORE(float, sycl::cl_float4, private_space, )
+SYCL_PSTORE(float, sycl::cl_float4, private_space, u)
+SYCL_PSTORE(double, sycl::cl_double2, private_space, )
+SYCL_PSTORE(double, sycl::cl_double2, private_space, u)
 #undef SYCL_PSTORE
 
 #define SYCL_PSTORE_T(address_space_target)                                 \
   template <typename scalar, typename packet_type, int Alignment>           \
   EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE void pstoret(                       \
-      typename cl::sycl::multi_ptr<                                         \
+      typename sycl::multi_ptr<                                         \
           scalar,                                                           \
-          cl::sycl::access::address_space::address_space_target>::pointer_t \
+          sycl::access::address_space::address_space_target>::pointer_t \
           to,                                                               \
       const packet_type& from) {                                            \
     if (Alignment)                                                          \
@@ -214,8 +214,8 @@ SYCL_PSTORE_T(local_space)
   }
 
 // global space
-SYCL_PSET1(cl::sycl::cl_float4)
-SYCL_PSET1(cl::sycl::cl_double2)
+SYCL_PSET1(sycl::cl_float4)
+SYCL_PSET1(sycl::cl_double2)
 
 #undef SYCL_PSET1
 
@@ -231,60 +231,60 @@ struct get_base_packet {
 };
 
 template <>
-struct get_base_packet<cl::sycl::cl_float4> {
+struct get_base_packet<sycl::cl_float4> {
   template <typename sycl_multi_pointer>
-  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE cl::sycl::cl_float4 get_ploaddup(
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 get_ploaddup(
       sycl_multi_pointer from) {
-    return cl::sycl::cl_float4(from[0], from[0], from[1], from[1]);
+    return sycl::cl_float4(from[0], from[0], from[1], from[1]);
   }
   template <typename sycl_multi_pointer>
-  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE cl::sycl::cl_float4 get_pgather(
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 get_pgather(
       sycl_multi_pointer from, Index stride) {
-    return cl::sycl::cl_float4(from[0 * stride], from[1 * stride],
+    return sycl::cl_float4(from[0 * stride], from[1 * stride],
                                from[2 * stride], from[3 * stride]);
   }
 
   template <typename sycl_multi_pointer>
   static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void set_pscatter(
-      sycl_multi_pointer to, const cl::sycl::cl_float4& from, Index stride) {
+      sycl_multi_pointer to, const sycl::cl_float4& from, Index stride) {
     auto tmp = stride;
     to[0] = from.x();
     to[tmp] = from.y();
     to[tmp += stride] = from.z();
     to[tmp += stride] = from.w();
   }
-  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE cl::sycl::cl_float4 set_plset(
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_float4 set_plset(
       const float& a) {
-    return cl::sycl::cl_float4(static_cast<float>(a), static_cast<float>(a + 1),
+    return sycl::cl_float4(static_cast<float>(a), static_cast<float>(a + 1),
                                static_cast<float>(a + 2),
                                static_cast<float>(a + 3));
   }
 };
 
 template <>
-struct get_base_packet<cl::sycl::cl_double2> {
+struct get_base_packet<sycl::cl_double2> {
   template <typename sycl_multi_pointer>
-  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE cl::sycl::cl_double2
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_double2
   get_ploaddup(const sycl_multi_pointer from) {
-    return cl::sycl::cl_double2(from[0], from[0]);
+    return sycl::cl_double2(from[0], from[0]);
   }
 
   template <typename sycl_multi_pointer, typename Index>
-  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE cl::sycl::cl_double2 get_pgather(
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_double2 get_pgather(
       const sycl_multi_pointer from, Index stride) {
-    return cl::sycl::cl_double2(from[0 * stride], from[1 * stride]);
+    return sycl::cl_double2(from[0 * stride], from[1 * stride]);
   }
 
   template <typename sycl_multi_pointer>
   static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void set_pscatter(
-      sycl_multi_pointer to, const cl::sycl::cl_double2& from, Index stride) {
+      sycl_multi_pointer to, const sycl::cl_double2& from, Index stride) {
     to[0] = from.x();
     to[stride] = from.y();
   }
 
-  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE cl::sycl::cl_double2 set_plset(
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE sycl::cl_double2 set_plset(
       const double& a) {
-    return cl::sycl::cl_double2(static_cast<double>(a),
+    return sycl::cl_double2(static_cast<double>(a),
                                 static_cast<double>(a + 1));
   }
 };
@@ -292,9 +292,9 @@ struct get_base_packet<cl::sycl::cl_double2> {
 #define SYCL_PLOAD_DUP(address_space_target)                                \
   template <typename packet_type>                                           \
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packet_type ploaddup(               \
-      typename cl::sycl::multi_ptr<                                         \
+      typename sycl::multi_ptr<                                         \
           const typename unpacket_traits<packet_type>::type,                \
-          cl::sycl::access::address_space::address_space_target>::pointer_t \
+          sycl::access::address_space::address_space_target>::pointer_t \
           from) {                                                           \
     return get_base_packet<packet_type>::get_ploaddup(from);                \
   }
@@ -312,8 +312,8 @@ SYCL_PLOAD_DUP(local_space)
     return get_base_packet<packet_type>::get_ploaddup(from);               \
   }
 
-SYCL_PLOAD_DUP_SPECILIZE(cl::sycl::cl_float4)
-SYCL_PLOAD_DUP_SPECILIZE(cl::sycl::cl_double2)
+SYCL_PLOAD_DUP_SPECILIZE(sycl::cl_float4)
+SYCL_PLOAD_DUP_SPECILIZE(sycl::cl_double2)
 
 #undef SYCL_PLOAD_DUP_SPECILIZE
 
@@ -324,17 +324,17 @@ SYCL_PLOAD_DUP_SPECILIZE(cl::sycl::cl_double2)
     return get_base_packet<packet_type>::set_plset(a);                  \
   }
 
-SYCL_PLSET(cl::sycl::cl_float4)
-SYCL_PLSET(cl::sycl::cl_double2)
+SYCL_PLSET(sycl::cl_float4)
+SYCL_PLSET(sycl::cl_double2)
 
 #undef SYCL_PLSET
 
 #define SYCL_PGATHER(address_space_target)                                  \
   template <typename Scalar, typename packet_type>                          \
   EIGEN_DEVICE_FUNC inline packet_type pgather(                             \
-      typename cl::sycl::multi_ptr<                                         \
+      typename sycl::multi_ptr<                                         \
           const typename unpacket_traits<packet_type>::type,                \
-          cl::sycl::access::address_space::address_space_target>::pointer_t \
+          sycl::access::address_space::address_space_target>::pointer_t \
           from,                                                             \
       Index stride) {                                                       \
     return get_base_packet<packet_type>::get_pgather(from, stride);         \
@@ -355,17 +355,17 @@ SYCL_PGATHER(local_space)
     return get_base_packet<packet_type>::get_pgather(from, stride);            \
   }
 
-SYCL_PGATHER_SPECILIZE(float, cl::sycl::cl_float4)
-SYCL_PGATHER_SPECILIZE(double, cl::sycl::cl_double2)
+SYCL_PGATHER_SPECILIZE(float, sycl::cl_float4)
+SYCL_PGATHER_SPECILIZE(double, sycl::cl_double2)
 
 #undef SYCL_PGATHER_SPECILIZE
 
 #define SYCL_PSCATTER(address_space_target)                                 \
   template <typename Scalar, typename packet_type>                          \
   EIGEN_DEVICE_FUNC inline void pscatter(                                   \
-      typename cl::sycl::multi_ptr<                                         \
+      typename sycl::multi_ptr<                                         \
           typename unpacket_traits<packet_type>::type,                      \
-          cl::sycl::access::address_space::address_space_target>::pointer_t \
+          sycl::access::address_space::address_space_target>::pointer_t \
           to,                                                               \
       const packet_type& from, Index stride) {                              \
     get_base_packet<packet_type>::set_pscatter(to, from, stride);           \
@@ -386,8 +386,8 @@ SYCL_PSCATTER(local_space)
     get_base_packet<packet_type>::set_pscatter(to, from, stride);           \
   }
 
-SYCL_PSCATTER_SPECILIZE(float, cl::sycl::cl_float4)
-SYCL_PSCATTER_SPECILIZE(double, cl::sycl::cl_double2)
+SYCL_PSCATTER_SPECILIZE(float, sycl::cl_float4)
+SYCL_PSCATTER_SPECILIZE(double, sycl::cl_double2)
 
 #undef SYCL_PSCATTER_SPECILIZE
 
@@ -395,81 +395,81 @@ SYCL_PSCATTER_SPECILIZE(double, cl::sycl::cl_double2)
   template <>                                                             \
   EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE packet_type pmadd(                \
       const packet_type& a, const packet_type& b, const packet_type& c) { \
-    return cl::sycl::mad(a, b, c);                                        \
+    return sycl::mad(a, b, c);                                        \
   }
 
-SYCL_PMAD(cl::sycl::cl_float4)
-SYCL_PMAD(cl::sycl::cl_double2)
+SYCL_PMAD(sycl::cl_float4)
+SYCL_PMAD(sycl::cl_double2)
 #undef SYCL_PMAD
 
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float pfirst<cl::sycl::cl_float4>(
-    const cl::sycl::cl_float4& a) {
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float pfirst<sycl::cl_float4>(
+    const sycl::cl_float4& a) {
   return a.x();
 }
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double pfirst<cl::sycl::cl_double2>(
-    const cl::sycl::cl_double2& a) {
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double pfirst<sycl::cl_double2>(
+    const sycl::cl_double2& a) {
   return a.x();
 }
 
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float predux<cl::sycl::cl_float4>(
-    const cl::sycl::cl_float4& a) {
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float predux<sycl::cl_float4>(
+    const sycl::cl_float4& a) {
   return a.x() + a.y() + a.z() + a.w();
 }
 
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double predux<cl::sycl::cl_double2>(
-    const cl::sycl::cl_double2& a) {
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double predux<sycl::cl_double2>(
+    const sycl::cl_double2& a) {
   return a.x() + a.y();
 }
 
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float predux_max<cl::sycl::cl_float4>(
-    const cl::sycl::cl_float4& a) {
-  return cl::sycl::fmax(cl::sycl::fmax(a.x(), a.y()),
-                        cl::sycl::fmax(a.z(), a.w()));
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float predux_max<sycl::cl_float4>(
+    const sycl::cl_float4& a) {
+  return sycl::fmax(sycl::fmax(a.x(), a.y()),
+                        sycl::fmax(a.z(), a.w()));
 }
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double predux_max<cl::sycl::cl_double2>(
-    const cl::sycl::cl_double2& a) {
-  return cl::sycl::fmax(a.x(), a.y());
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double predux_max<sycl::cl_double2>(
+    const sycl::cl_double2& a) {
+  return sycl::fmax(a.x(), a.y());
 }
 
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float predux_min<cl::sycl::cl_float4>(
-    const cl::sycl::cl_float4& a) {
-  return cl::sycl::fmin(cl::sycl::fmin(a.x(), a.y()),
-                        cl::sycl::fmin(a.z(), a.w()));
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float predux_min<sycl::cl_float4>(
+    const sycl::cl_float4& a) {
+  return sycl::fmin(sycl::fmin(a.x(), a.y()),
+                        sycl::fmin(a.z(), a.w()));
 }
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double predux_min<cl::sycl::cl_double2>(
-    const cl::sycl::cl_double2& a) {
-  return cl::sycl::fmin(a.x(), a.y());
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double predux_min<sycl::cl_double2>(
+    const sycl::cl_double2& a) {
+  return sycl::fmin(a.x(), a.y());
 }
 
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float predux_mul<cl::sycl::cl_float4>(
-    const cl::sycl::cl_float4& a) {
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float predux_mul<sycl::cl_float4>(
+    const sycl::cl_float4& a) {
   return a.x() * a.y() * a.z() * a.w();
 }
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double predux_mul<cl::sycl::cl_double2>(
-    const cl::sycl::cl_double2& a) {
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double predux_mul<sycl::cl_double2>(
+    const sycl::cl_double2& a) {
   return a.x() * a.y();
 }
 
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE cl::sycl::cl_float4
-pabs<cl::sycl::cl_float4>(const cl::sycl::cl_float4& a) {
-  return cl::sycl::cl_float4(cl::sycl::fabs(a.x()), cl::sycl::fabs(a.y()),
-                             cl::sycl::fabs(a.z()), cl::sycl::fabs(a.w()));
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_float4
+pabs<sycl::cl_float4>(const sycl::cl_float4& a) {
+  return sycl::cl_float4(sycl::fabs(a.x()), sycl::fabs(a.y()),
+                             sycl::fabs(a.z()), sycl::fabs(a.w()));
 }
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE cl::sycl::cl_double2
-pabs<cl::sycl::cl_double2>(const cl::sycl::cl_double2& a) {
-  return cl::sycl::cl_double2(cl::sycl::fabs(a.x()), cl::sycl::fabs(a.y()));
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_double2
+pabs<sycl::cl_double2>(const sycl::cl_double2& a) {
+  return sycl::cl_double2(sycl::fabs(a.x()), sycl::fabs(a.y()));
 }
 
 template <typename Packet>
@@ -477,7 +477,7 @@ EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet sycl_pcmp_le(const Packet &a,
                                                           const Packet &b) {
   return ((a <= b)
               .template convert<typename unpacket_traits<Packet>::type,
-                                cl::sycl::rounding_mode::automatic>());
+                                sycl::rounding_mode::automatic>());
 }
 
 template <typename Packet>
@@ -485,7 +485,7 @@ EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet sycl_pcmp_lt(const Packet &a,
                                                           const Packet &b) {
   return ((a < b)
               .template convert<typename unpacket_traits<Packet>::type,
-                                cl::sycl::rounding_mode::automatic>());
+                                sycl::rounding_mode::automatic>());
 }
 
 template <typename Packet>
@@ -493,7 +493,7 @@ EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet sycl_pcmp_eq(const Packet &a,
                                                           const Packet &b) {
   return ((a == b)
               .template convert<typename unpacket_traits<Packet>::type,
-                                cl::sycl::rounding_mode::automatic>());
+                                sycl::rounding_mode::automatic>());
 }
 
 #define SYCL_PCMP(OP, TYPE)                                                    \
@@ -503,23 +503,23 @@ EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet sycl_pcmp_eq(const Packet &a,
     return sycl_pcmp_##OP<TYPE>(a, b);                                         \
   }
 
-SYCL_PCMP(le, cl::sycl::cl_float4)
-SYCL_PCMP(lt, cl::sycl::cl_float4)
-SYCL_PCMP(eq, cl::sycl::cl_float4)
-SYCL_PCMP(le, cl::sycl::cl_double2)
-SYCL_PCMP(lt, cl::sycl::cl_double2)
-SYCL_PCMP(eq, cl::sycl::cl_double2)
+SYCL_PCMP(le, sycl::cl_float4)
+SYCL_PCMP(lt, sycl::cl_float4)
+SYCL_PCMP(eq, sycl::cl_float4)
+SYCL_PCMP(le, sycl::cl_double2)
+SYCL_PCMP(lt, sycl::cl_double2)
+SYCL_PCMP(eq, sycl::cl_double2)
 #undef SYCL_PCMP
 
 template <typename T> struct convert_to_integer;
 
 template <> struct convert_to_integer<float> {
   using type = std::int32_t;
-  using packet_type = cl::sycl::cl_int4;
+  using packet_type = sycl::cl_int4;
 };
 template <> struct convert_to_integer<double> {
   using type = std::int64_t;
-  using packet_type = cl::sycl::cl_long2;
+  using packet_type = sycl::cl_long2;
 };
 
 template <typename PacketIn>
@@ -529,14 +529,14 @@ vector_as_int(const PacketIn &p) {
   return (
       p.template convert<typename convert_to_integer<
                              typename unpacket_traits<PacketIn>::type>::type,
-                         cl::sycl::rounding_mode::automatic>());
+                         sycl::rounding_mode::automatic>());
 }
 
 template <typename packetOut, typename PacketIn>
 EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE packetOut
 convert_vector(const PacketIn &p) {
   return (p.template convert<typename unpacket_traits<packetOut>::type,
-                             cl::sycl::rounding_mode::automatic>());
+                             sycl::rounding_mode::automatic>());
 }
 
 #define SYCL_PAND(TYPE)                                                        \
@@ -545,8 +545,8 @@ convert_vector(const PacketIn &p) {
                                                         const TYPE &b) {       \
     return convert_vector<TYPE>(vector_as_int(a) & vector_as_int(b));          \
   }
-SYCL_PAND(cl::sycl::cl_float4)
-SYCL_PAND(cl::sycl::cl_double2)
+SYCL_PAND(sycl::cl_float4)
+SYCL_PAND(sycl::cl_double2)
 #undef SYCL_PAND
 
 #define SYCL_POR(TYPE)                                                         \
@@ -556,8 +556,8 @@ SYCL_PAND(cl::sycl::cl_double2)
     return convert_vector<TYPE>(vector_as_int(a) | vector_as_int(b));          \
   }
 
-SYCL_POR(cl::sycl::cl_float4)
-SYCL_POR(cl::sycl::cl_double2)
+SYCL_POR(sycl::cl_float4)
+SYCL_POR(sycl::cl_double2)
 #undef SYCL_POR
 
 #define SYCL_PXOR(TYPE)                                                        \
@@ -567,8 +567,8 @@ SYCL_POR(cl::sycl::cl_double2)
     return convert_vector<TYPE>(vector_as_int(a) ^ vector_as_int(b));          \
   }
 
-SYCL_PXOR(cl::sycl::cl_float4)
-SYCL_PXOR(cl::sycl::cl_double2)
+SYCL_PXOR(sycl::cl_float4)
+SYCL_PXOR(sycl::cl_double2)
 #undef SYCL_PXOR
 
 #define SYCL_PANDNOT(TYPE)                                                     \
@@ -577,12 +577,12 @@ SYCL_PXOR(cl::sycl::cl_double2)
                                                            const TYPE &b) {    \
     return convert_vector<TYPE>(vector_as_int(a) & (~vector_as_int(b)));       \
   }
-SYCL_PANDNOT(cl::sycl::cl_float4)
-SYCL_PANDNOT(cl::sycl::cl_double2)
+SYCL_PANDNOT(sycl::cl_float4)
+SYCL_PANDNOT(sycl::cl_double2)
 #undef SYCL_PANDNOT
 
 EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE void ptranspose(
-    PacketBlock<cl::sycl::cl_float4, 4>& kernel) {
+    PacketBlock<sycl::cl_float4, 4>& kernel) {
   float tmp = kernel.packet[0].y();
   kernel.packet[0].y() = kernel.packet[1].x();
   kernel.packet[1].x() = tmp;
@@ -609,31 +609,31 @@ EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE void ptranspose(
 }
 
 EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE void ptranspose(
-    PacketBlock<cl::sycl::cl_double2, 2>& kernel) {
+    PacketBlock<sycl::cl_double2, 2>& kernel) {
   double tmp = kernel.packet[0].y();
   kernel.packet[0].y() = kernel.packet[1].x();
   kernel.packet[1].x() = tmp;
 }
 
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE cl::sycl::cl_float4 pblend(
-    const Selector<unpacket_traits<cl::sycl::cl_float4>::size>& ifPacket,
-    const cl::sycl::cl_float4& thenPacket,
-    const cl::sycl::cl_float4& elsePacket) {
-  cl::sycl::cl_int4 condition(
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_float4 pblend(
+    const Selector<unpacket_traits<sycl::cl_float4>::size>& ifPacket,
+    const sycl::cl_float4& thenPacket,
+    const sycl::cl_float4& elsePacket) {
+  sycl::cl_int4 condition(
       ifPacket.select[0] ? 0 : -1, ifPacket.select[1] ? 0 : -1,
       ifPacket.select[2] ? 0 : -1, ifPacket.select[3] ? 0 : -1);
-  return cl::sycl::select(thenPacket, elsePacket, condition);
+  return sycl::select(thenPacket, elsePacket, condition);
 }
 
 template <>
-inline cl::sycl::cl_double2 pblend(
-    const Selector<unpacket_traits<cl::sycl::cl_double2>::size>& ifPacket,
-    const cl::sycl::cl_double2& thenPacket,
-    const cl::sycl::cl_double2& elsePacket) {
-  cl::sycl::cl_long2 condition(ifPacket.select[0] ? 0 : -1,
+inline sycl::cl_double2 pblend(
+    const Selector<unpacket_traits<sycl::cl_double2>::size>& ifPacket,
+    const sycl::cl_double2& thenPacket,
+    const sycl::cl_double2& elsePacket) {
+  sycl::cl_long2 condition(ifPacket.select[0] ? 0 : -1,
                                ifPacket.select[1] ? 0 : -1);
-  return cl::sycl::select(thenPacket, elsePacket, condition);
+  return sycl::select(thenPacket, elsePacket, condition);
 }
 #endif  // SYCL_DEVICE_ONLY
 
@@ -641,7 +641,7 @@ inline cl::sycl::cl_double2 pblend(
   template <typename packet_type>                               \
   EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE void pstore##alignment( \
       const Eigen::TensorSycl::internal::RangeAccess<           \
-          cl::sycl::access::mode::read_write,                   \
+          sycl::access::mode::read_write,                   \
           typename unpacket_traits<packet_type>::type>& to,     \
       const packet_type& from) {                                \
     pstore##alignment(to.get_pointer(), from);                  \
@@ -656,7 +656,7 @@ SYCL_PSTORE(u)
 template <typename scalar, typename packet_type, int Alignment>
 EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE void pstoret(
     Eigen::TensorSycl::internal::RangeAccess<
-        cl::sycl::access::mode::read_write,
+        sycl::access::mode::read_write,
         typename unpacket_traits<packet_type>::type>
         to,
     const packet_type& from) {
diff --git a/Eigen/src/Core/arch/SYCL/SyclMemoryModel.h b/Eigen/src/Core/arch/SYCL/SyclMemoryModel.h
index f81e59db5..642f90e00 100644
--- a/Eigen/src/Core/arch/SYCL/SyclMemoryModel.h
+++ b/Eigen/src/Core/arch/SYCL/SyclMemoryModel.h
@@ -37,8 +37,8 @@ namespace Eigen {
 namespace TensorSycl {
 namespace internal {
 
-using sycl_acc_target = cl::sycl::access::target;
-using sycl_acc_mode = cl::sycl::access::mode;
+using sycl_acc_target = sycl::access::target;
+using sycl_acc_mode = sycl::access::mode;
 
 /**
  * Default values for template arguments
@@ -139,7 +139,7 @@ class PointerMapper {
 
   /* basic type for all buffers
    */
-  using buffer_t = cl::sycl::buffer_mem;
+  using buffer_t = sycl::buffer_mem;
 
   /**
    * Node that stores information about a device allocation.
@@ -233,9 +233,9 @@ class PointerMapper {
    * Returns a buffer from the map using the pointer address
    */
   template <typename buffer_data_type = buffer_data_type_t>
-  cl::sycl::buffer<buffer_data_type, 1> get_buffer(
+  sycl::buffer<buffer_data_type, 1> get_buffer(
       const virtual_pointer_t ptr) {
-    using sycl_buffer_t = cl::sycl::buffer<buffer_data_type, 1>;
+    using sycl_buffer_t = sycl::buffer<buffer_data_type, 1>;
 
     // get_node() returns a `buffer_mem`, so we need to cast it to a `buffer<>`.
     // We can do this without the `buffer_mem` being a pointer, as we
@@ -257,7 +257,7 @@ class PointerMapper {
   template <sycl_acc_mode access_mode = default_acc_mode,
             sycl_acc_target access_target = default_acc_target,
             typename buffer_data_type = buffer_data_type_t>
-  cl::sycl::accessor<buffer_data_type, 1, access_mode, access_target>
+  sycl::accessor<buffer_data_type, 1, access_mode, access_target>
   get_access(const virtual_pointer_t ptr) {
     auto buf = get_buffer<buffer_data_type>(ptr);
     return buf.template get_access<access_mode, access_target>();
@@ -274,8 +274,8 @@ class PointerMapper {
   template <sycl_acc_mode access_mode = default_acc_mode,
             sycl_acc_target access_target = default_acc_target,
             typename buffer_data_type = buffer_data_type_t>
-  cl::sycl::accessor<buffer_data_type, 1, access_mode, access_target>
-  get_access(const virtual_pointer_t ptr, cl::sycl::handler &cgh) {
+  sycl::accessor<buffer_data_type, 1, access_mode, access_target>
+  get_access(const virtual_pointer_t ptr, sycl::handler &cgh) {
     auto buf = get_buffer<buffer_data_type>(ptr);
     return buf.template get_access<access_mode, access_target>(cgh);
   }
@@ -509,15 +509,15 @@ inline void PointerMapper::remove_pointer<false>(const virtual_pointer_t ptr) {
  * Given a size, creates a byte-typed buffer and returns a
  * fake pointer to keep track of it.
  * \param size Size in bytes of the desired allocation
- * \throw cl::sycl::exception if error while creating the buffer
+ * \throw sycl::exception if error while creating the buffer
  */
 inline void *SYCLmalloc(size_t size, PointerMapper &pMap) {
   if (size == 0) {
     return nullptr;
   }
   // Create a generic buffer of the given size
-  using buffer_t = cl::sycl::buffer<buffer_data_type_t, 1>;
-  auto thePointer = pMap.add_pointer(buffer_t(cl::sycl::range<1>{size}));
+  using buffer_t = sycl::buffer<buffer_data_type_t, 1>;
+  auto thePointer = pMap.add_pointer(buffer_t(sycl::range<1>{size}));
   // Store the buffer on the global list
   return static_cast<void *>(thePointer);
 }
@@ -542,16 +542,16 @@ inline void SYCLfreeAll(PointerMapper &pMap) {
   pMap.clear();
 }
 
-template <cl::sycl::access::mode AcMd, typename T>
+template <sycl::access::mode AcMd, typename T>
 struct RangeAccess {
-  static const auto global_access = cl::sycl::access::target::global_buffer;
-  static const auto is_place_holder = cl::sycl::access::placeholder::true_t;
+  static const auto global_access = sycl::access::target::global_buffer;
+  static const auto is_place_holder = sycl::access::placeholder::true_t;
   typedef T scalar_t;
   typedef scalar_t &ref_t;
-  typedef typename cl::sycl::global_ptr<scalar_t>::pointer_t ptr_t;
+  typedef typename sycl::global_ptr<scalar_t>::pointer_t ptr_t;
 
   // the accessor type does not necessarily the same as T
-  typedef cl::sycl::accessor<scalar_t, 1, AcMd, global_access, is_place_holder>
+  typedef sycl::accessor<scalar_t, 1, AcMd, global_access, is_place_holder>
       accessor;
 
   typedef RangeAccess<AcMd, T> self_t;
@@ -560,8 +560,8 @@ struct RangeAccess {
                                                     std::intptr_t virtual_ptr)
       : access_(access), offset_(offset), virtual_ptr_(virtual_ptr) {}
 
-  RangeAccess(cl::sycl::buffer<scalar_t, 1> buff =
-                  cl::sycl::buffer<scalar_t, 1>(cl::sycl::range<1>(1)))
+  RangeAccess(sycl::buffer<scalar_t, 1> buff =
+                  sycl::buffer<scalar_t, 1>(sycl::range<1>(1)))
       : access_{accessor{buff}}, offset_(0), virtual_ptr_(-1) {}
 
   // This should be only used for null constructor on the host side
@@ -671,7 +671,7 @@ struct RangeAccess {
   }
   // binding placeholder accessors to a command group handler for SYCL
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(
-      cl::sycl::handler &cgh) const {
+      sycl::handler &cgh) const {
     cgh.require(access_);
   }
 
@@ -681,7 +681,7 @@ struct RangeAccess {
   std::intptr_t virtual_ptr_;  // the location of the buffer in the map
 };
 
-template <cl::sycl::access::mode AcMd, typename T>
+template <sycl::access::mode AcMd, typename T>
 struct RangeAccess<AcMd, const T> : RangeAccess<AcMd, T> {
   typedef RangeAccess<AcMd, T> Base;
   using Base::Base;
diff --git a/Eigen/src/Core/arch/SYCL/TypeCasting.h b/Eigen/src/Core/arch/SYCL/TypeCasting.h
index 9208ab21d..05065da30 100644
--- a/Eigen/src/Core/arch/SYCL/TypeCasting.h
+++ b/Eigen/src/Core/arch/SYCL/TypeCasting.h
@@ -31,10 +31,10 @@ struct type_casting_traits<float, int> {
 };
 
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE cl::sycl::cl_int4
-pcast<cl::sycl::cl_float4, cl::sycl::cl_int4>(const cl::sycl::cl_float4& a) {
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_int4
+pcast<sycl::cl_float4, sycl::cl_int4>(const sycl::cl_float4& a) {
   return a
-      .template convert<cl::sycl::cl_int, cl::sycl::rounding_mode::automatic>();
+      .template convert<sycl::cl_int, sycl::rounding_mode::automatic>();
 }
 
 template <>
@@ -43,10 +43,10 @@ struct type_casting_traits<int, float> {
 };
 
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE cl::sycl::cl_float4
-pcast<cl::sycl::cl_int4, cl::sycl::cl_float4>(const cl::sycl::cl_int4& a) {
-  return a.template convert<cl::sycl::cl_float,
-                            cl::sycl::rounding_mode::automatic>();
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_float4
+pcast<sycl::cl_int4, sycl::cl_float4>(const sycl::cl_int4& a) {
+  return a.template convert<sycl::cl_float,
+                            sycl::rounding_mode::automatic>();
 }
 
 template <>
@@ -55,14 +55,14 @@ struct type_casting_traits<double, float> {
 };
 
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE cl::sycl::cl_float4
-pcast<cl::sycl::cl_double2, cl::sycl::cl_float4>(
-    const cl::sycl::cl_double2& a, const cl::sycl::cl_double2& b) {
-  auto a1 = a.template convert<cl::sycl::cl_float,
-                               cl::sycl::rounding_mode::automatic>();
-  auto b1 = b.template convert<cl::sycl::cl_float,
-                               cl::sycl::rounding_mode::automatic>();
-  return cl::sycl::float4(a1.x(), a1.y(), b1.x(), b1.y());
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_float4
+pcast<sycl::cl_double2, sycl::cl_float4>(
+    const sycl::cl_double2& a, const sycl::cl_double2& b) {
+  auto a1 = a.template convert<sycl::cl_float,
+                               sycl::rounding_mode::automatic>();
+  auto b1 = b.template convert<sycl::cl_float,
+                               sycl::rounding_mode::automatic>();
+  return sycl::float4(a1.x(), a1.y(), b1.x(), b1.y());
 }
 
 template <>
@@ -71,10 +71,10 @@ struct type_casting_traits<float, double> {
 };
 
 template <>
-EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE cl::sycl::cl_double2
-pcast<cl::sycl::cl_float4, cl::sycl::cl_double2>(const cl::sycl::cl_float4& a) {
+EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE sycl::cl_double2
+pcast<sycl::cl_float4, sycl::cl_double2>(const sycl::cl_float4& a) {
   // Simply discard the second half of the input
-  return cl::sycl::cl_double2(a.x(), a.y());
+  return sycl::cl_double2(a.x(), a.y());
 }
 
 #endif
diff --git a/Eigen/src/Core/functors/BinaryFunctors.h b/Eigen/src/Core/functors/BinaryFunctors.h
index d8b7b1eba..f484d3a92 100644
--- a/Eigen/src/Core/functors/BinaryFunctors.h
+++ b/Eigen/src/Core/functors/BinaryFunctors.h
@@ -42,7 +42,7 @@ struct scalar_sum_op : binary_op_base<LhsScalar,RhsScalar>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator() (const LhsScalar& a, const RhsScalar& b) const { return a + b; }
   template<typename Packet>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const
-  { return internal::padd(a,b); }
+  { return internal::padd(a, b); }
   template<typename Packet>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type predux(const Packet& a) const
   { return internal::predux(a); }
diff --git a/Eigen/src/Core/functors/NullaryFunctors.h b/Eigen/src/Core/functors/NullaryFunctors.h
index 4aa33a19f..435e005c0 100644
--- a/Eigen/src/Core/functors/NullaryFunctors.h
+++ b/Eigen/src/Core/functors/NullaryFunctors.h
@@ -16,7 +16,6 @@ namespace internal {
 
 template<typename Scalar>
 struct scalar_constant_op {
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE scalar_constant_op(const scalar_constant_op& other) : m_other(other.m_other) { }
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE scalar_constant_op(const Scalar& other) : m_other(other) { }
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator() () const { return m_other; }
   template<typename PacketType>
diff --git a/Eigen/src/Core/functors/UnaryFunctors.h b/Eigen/src/Core/functors/UnaryFunctors.h
index a9cbfd967..e09121058 100644
--- a/Eigen/src/Core/functors/UnaryFunctors.h
+++ b/Eigen/src/Core/functors/UnaryFunctors.h
@@ -825,13 +825,17 @@ template<typename Scalar> struct scalar_isnan_op {
     return (numext::isnan)(a);
 #endif
   }
+  template <typename OutPacket, typename InPacket>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE OutPacket packetOp(const InPacket& a) const {
+    return internal::pisnan<OutPacket, InPacket>(a);
+  }
 };
 template<typename Scalar>
 struct functor_traits<scalar_isnan_op<Scalar> >
 {
   enum {
     Cost = NumTraits<Scalar>::MulCost,
-    PacketAccess = false
+    PacketAccess = true
   };
 };
 
@@ -849,13 +853,17 @@ template<typename Scalar> struct scalar_isinf_op {
     return (numext::isinf)(a);
 #endif
   }
+  template<typename OutPacket, typename InPacket>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE OutPacket packetOp(const InPacket& a) const {
+    return internal::pisinf<OutPacket,InPacket>(a);
+  }
 };
 template<typename Scalar>
 struct functor_traits<scalar_isinf_op<Scalar> >
 {
   enum {
     Cost = NumTraits<Scalar>::MulCost,
-    PacketAccess = false
+    PacketAccess = true
   };
 };
 
@@ -873,13 +881,17 @@ template<typename Scalar> struct scalar_isfinite_op {
     return (numext::isfinite)(a);
 #endif
   }
+  template<typename OutPacket, typename InPacket>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE OutPacket packetOp(const InPacket& a) const {
+    return internal::pisfinite<OutPacket,InPacket>(a);
+  }
 };
 template<typename Scalar>
 struct functor_traits<scalar_isfinite_op<Scalar> >
 {
   enum {
     Cost = NumTraits<Scalar>::MulCost,
-    PacketAccess = false
+    PacketAccess = true
   };
 };
 
@@ -891,12 +903,16 @@ struct functor_traits<scalar_isfinite_op<Scalar> >
 template<typename Scalar> struct scalar_boolean_not_op {
   EIGEN_EMPTY_STRUCT_CTOR(scalar_boolean_not_op)
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool operator() (const bool& a) const { return !a; }
+  template<typename Packet>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a) const { 
+    return internal::plogical_not(a); 
+  }
 };
 template<typename Scalar>
 struct functor_traits<scalar_boolean_not_op<Scalar> > {
   enum {
     Cost = NumTraits<bool>::AddCost,
-    PacketAccess = false
+    PacketAccess = true
   };
 };
 
@@ -912,9 +928,10 @@ struct scalar_sign_op<Scalar, false, true> {
   {
       return Scalar( (a>Scalar(0)) - (a<Scalar(0)) );
   }
-  //TODO
-  //template <typename Packet>
-  //EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const { return internal::psign(a); }
+  
+  // TODO
+  // template <typename Packet>
+  // EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const { return internal::psign(a); }
 };
 
 template<typename Scalar>
@@ -924,9 +941,9 @@ struct scalar_sign_op<Scalar, false, false> {
   {
     return (numext::isnan)(a) ? a : Scalar( (a>Scalar(0)) - (a<Scalar(0)) );
   }
-  //TODO
-  //template <typename Packet>
-  //EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const { return internal::psign(a); }
+
+  template <typename Packet>
+  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const { return internal::psign(a); }
 };
 
 template<typename Scalar, bool is_integer>
@@ -941,9 +958,10 @@ struct scalar_sign_op<Scalar,true, is_integer> {
     aa = real_type(1)/aa;
     return Scalar(a.real()*aa, a.imag()*aa );
   }
-  //TODO
-  //template <typename Packet>
-  //EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const { return internal::psign(a); }
+ 
+  // TODO
+  // template <typename Packet>
+  // EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const { return internal::psign(a); }
 };
 template<typename Scalar>
 struct functor_traits<scalar_sign_op<Scalar> >
@@ -952,7 +970,9 @@ struct functor_traits<scalar_sign_op<Scalar> >
         NumTraits<Scalar>::IsComplex
         ? ( 8*NumTraits<Scalar>::MulCost  ) // roughly
         : ( 3*NumTraits<Scalar>::AddCost),
-    PacketAccess = packet_traits<Scalar>::HasSign
+    PacketAccess = NumTraits<Scalar>::IsComplex == 0 &&
+                   NumTraits<Scalar>::IsInteger == 0 &&
+                   packet_traits<Scalar>::HasSign
   };
 };
 
@@ -1061,12 +1081,7 @@ struct functor_traits<scalar_logistic_op<T> > {
                 ? NumTraits<T>::AddCost * 15 + NumTraits<T>::MulCost * 11
                 : NumTraits<T>::AddCost * 2 +
                       functor_traits<scalar_exp_op<T> >::Cost),
-    PacketAccess =
-        packet_traits<T>::HasAdd && packet_traits<T>::HasDiv &&
-        (internal::is_same<T, float>::value
-             ? packet_traits<T>::HasMul && packet_traits<T>::HasMax &&
-                   packet_traits<T>::HasMin
-             : packet_traits<T>::HasNegate && packet_traits<T>::HasExp)
+    PacketAccess = false,
   };
 };
 
diff --git a/Eigen/src/Core/util/ConfigureVectorization.h b/Eigen/src/Core/util/ConfigureVectorization.h
index 739dab60d..23fc561a7 100644
--- a/Eigen/src/Core/util/ConfigureVectorization.h
+++ b/Eigen/src/Core/util/ConfigureVectorization.h
@@ -212,8 +212,7 @@
   #endif
 #endif
 
-#if !(defined(EIGEN_DONT_VECTORIZE) || defined(EIGEN_GPUCC))
-
+#if !(defined(EIGEN_DONT_VECTORIZE) || defined(EIGEN_GPUCC) || defined(EIGEN_USE_DPCPP))
   #if defined (EIGEN_SSE2_ON_NON_MSVC_BUT_NOT_OLD_GCC) || defined(EIGEN_SSE2_ON_MSVC_2008_OR_LATER)
 
     // Defines symbols for compile-time detection of which instructions are
@@ -240,7 +239,7 @@
       #define EIGEN_VECTORIZE_SSE4_2
     #endif
     #ifdef __AVX__
-      #ifndef EIGEN_USE_SYCL 
+      #if !defined(EIGEN_USE_SYCL) && !defined(EIGEN_USE_DPCPP) 
         #define EIGEN_VECTORIZE_AVX
       #endif
       #define EIGEN_VECTORIZE_SSE3
@@ -249,7 +248,7 @@
       #define EIGEN_VECTORIZE_SSE4_2
     #endif
     #ifdef __AVX2__
-      #ifndef EIGEN_USE_SYCL 
+      #if !defined(EIGEN_USE_SYCL) && !defined(EIGEN_USE_DPCPP)
         #define EIGEN_VECTORIZE_AVX2
         #define EIGEN_VECTORIZE_AVX
       #endif
@@ -271,7 +270,7 @@
       #error Please enable FMA in your compiler flags (e.g. -mfma): compiling with AVX512 alone without SSE/AVX FMA is not supported (bug 1638).
       #endif
       #endif
-      #ifndef EIGEN_USE_SYCL
+      #if !defined(EIGEN_USE_SYCL) && !defined(EIGEN_USE_DPCPP)
         #define EIGEN_VECTORIZE_AVX512
         #define EIGEN_VECTORIZE_AVX2
         #define EIGEN_VECTORIZE_AVX
@@ -281,7 +280,7 @@
       #define EIGEN_VECTORIZE_SSSE3
       #define EIGEN_VECTORIZE_SSE4_1
       #define EIGEN_VECTORIZE_SSE4_2
-      #ifndef EIGEN_USE_SYCL
+      #if !defined(EIGEN_USE_SYCL) && !defined(EIGEN_USE_DPCPP)
         #ifdef __AVX512DQ__
           #define EIGEN_VECTORIZE_AVX512DQ
         #endif
@@ -293,7 +292,6 @@
         #endif
       #endif
     #endif
-
     // Disable AVX support on broken xcode versions
     #if defined(__apple_build_version__) && (__apple_build_version__ == 11000033 ) && ( __MAC_OS_X_VERSION_MIN_REQUIRED == 101500 )
       // A nasty bug in the clang compiler shipped with xcode in a common compilation situation
diff --git a/Eigen/src/Core/util/Constants.h b/Eigen/src/Core/util/Constants.h
index 7ada82195..795642390 100644
--- a/Eigen/src/Core/util/Constants.h
+++ b/Eigen/src/Core/util/Constants.h
@@ -534,7 +534,12 @@ struct IteratorBased {};
 /** \internal
  * Constants for comparison functors
  */
-enum ComparisonName {
+
+enum ComparisonName
+#ifdef EIGEN_USE_DPCPP_BUILD
+    : unsigned int
+#endif
+{
   cmp_EQ = 0,
   cmp_LT = 1,
   cmp_LE = 2,
diff --git a/Eigen/src/Core/util/Macros.h b/Eigen/src/Core/util/Macros.h
index d0499a1c9..cb4ac5c80 100644
--- a/Eigen/src/Core/util/Macros.h
+++ b/Eigen/src/Core/util/Macros.h
@@ -503,6 +503,13 @@
 #define SYCL_DEVICE_ONLY
 #endif
 
+#if defined(EIGEN_USE_DPCPP) && defined(__SYCL_DEVICE_ONLY__)
+// EIGEN_USE_DPCPP is a user-defined macro while __SYCL_DEVICE_ONLY__ is a compiler-defined macro.
+// In most cases we want to check if both macros are defined which can be done using the define below.
+#define SYCL_DEVICE_ONLY
+#define DPCPP_DEVICE_ONLY
+#endif
+
 //------------------------------------------------------------------------------------------
 // Detect Compiler/Architecture/OS specific features
 //------------------------------------------------------------------------------------------
@@ -1217,7 +1224,7 @@ namespace Eigen {
   EIGEN_MAKE_SCALAR_BINARY_OP_ONTHERIGHT(METHOD,OPNAME)
 
 
-#if (defined(_CPPUNWIND) || defined(__EXCEPTIONS)) && !defined(EIGEN_CUDA_ARCH) && !defined(EIGEN_EXCEPTIONS) && !defined(EIGEN_USE_SYCL) && !defined(EIGEN_HIP_DEVICE_COMPILE)
+#if (defined(_CPPUNWIND) || defined(__EXCEPTIONS)) && !defined(EIGEN_CUDA_ARCH) && !defined(EIGEN_EXCEPTIONS) && !defined(EIGEN_USE_SYCL) && !defined(EIGEN_USE_DPCPP) && !defined(EIGEN_HIP_DEVICE_COMPILE)
   #define EIGEN_EXCEPTIONS
 #endif
 
diff --git a/Eigen/src/Core/util/Memory.h b/Eigen/src/Core/util/Memory.h
index 1b12544d2..8857ad116 100644
--- a/Eigen/src/Core/util/Memory.h
+++ b/Eigen/src/Core/util/Memory.h
@@ -82,7 +82,7 @@ inline void throw_std_bad_alloc()
     //
     new int[huge];
     #else
-    ::operator new(huge);
+    (void)::operator new(huge);
     #endif
   #endif
 }
@@ -522,14 +522,33 @@ template<typename T> struct smart_copy_helper<T,true> {
     IntPtr size = IntPtr(end)-IntPtr(start);
     if(size==0) return;
     eigen_internal_assert(start!=0 && end!=0 && target!=0);
+
+#if defined(DPCPP_DEVICE_ONLY)
+    int count = size / sizeof(T);
+    for (auto i = 0; i < count; ++i)
+        *(target + i) = *(start + i);
+#else
     EIGEN_USING_STD(memcpy)
     memcpy(target, start, size);
+#endif // DPCPP_DEVICE_ONLY
   }
 };
 
 template<typename T> struct smart_copy_helper<T,false> {
   EIGEN_DEVICE_FUNC static inline void run(const T* start, const T* end, T* target)
-  { std::copy(start, end, target); }
+  {
+#if defined(DPCPP_DEVICE_ONLY)
+    IntPtr size = IntPtr(end)-IntPtr(start);
+    if(size==0) return;
+    eigen_internal_assert(start!=0 && end!=0 && target!=0);
+
+    int count = size / sizeof(T);
+    for (auto i = 0; i < count; ++i)
+        *(target + i) = *(start + i);
+#else
+    std::copy(start, end, target);
+#endif // DPCPP_DEVICE_ONLY
+  }
 };
 
 // intelligent memmove. falls back to std::memmove for POD types, uses std::copy otherwise.
diff --git a/Eigen/src/Core/util/Meta.h b/Eigen/src/Core/util/Meta.h
index 68452ecfa..28da63509 100755
--- a/Eigen/src/Core/util/Meta.h
+++ b/Eigen/src/Core/util/Meta.h
@@ -688,8 +688,12 @@ bool not_equal_strict(const double& x,const double& y) { return std::not_equal_t
 EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC unsigned int as_uint(float x)
 {
   unsigned int ret;
+#if defined(DPCPP_DEVICE_ONLY)
+  ret = *(unsigned int*)&x;
+#else
   EIGEN_USING_STD(memcpy);
   memcpy(&ret, &x, sizeof(float));
+#endif // DPCPP_DEVICE_ONLY
   return ret;
 }
 
diff --git a/Eigen/src/Core/util/XprHelper.h b/Eigen/src/Core/util/XprHelper.h
index fd2db56a4..913171b88 100644
--- a/Eigen/src/Core/util/XprHelper.h
+++ b/Eigen/src/Core/util/XprHelper.h
@@ -15,9 +15,13 @@
 // FIXME: gcc 4.3 generates bad code when strict-aliasing is enabled
 // so currently we simply disable this optimization for gcc 4.3
 #if EIGEN_COMP_GNUC && !EIGEN_GNUC_AT(4,3)
+#if EIGEN_USE_DPCPP_BUILD
+  #define EIGEN_EMPTY_STRUCT_CTOR(X)
+#else
   #define EIGEN_EMPTY_STRUCT_CTOR(X) \
     EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE X() {} \
     EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE X(const X& ) {}
+#endif // EIGEN_USE_DPCPP_BUILD
 #else
   #define EIGEN_EMPTY_STRUCT_CTOR(X)
 #endif
diff --git a/Eigen/src/Geometry/arch/Geometry_SSE.h b/Eigen/src/Geometry/arch/Geometry_SSE.h
index 108cc9f8e..43677f478 100644
--- a/Eigen/src/Geometry/arch/Geometry_SSE.h
+++ b/Eigen/src/Geometry/arch/Geometry_SSE.h
@@ -33,13 +33,14 @@ struct quat_product<Architecture::SSE, Derived, OtherDerived, float>
     Packet4f b = be.template packet<BAlignment,Packet4f>(0);
     Packet4f s1 = pmul(vec4f_swizzle1(a,1,2,0,2),vec4f_swizzle1(b,2,0,1,2));
     Packet4f s2 = pmul(vec4f_swizzle1(a,3,3,3,1),vec4f_swizzle1(b,0,1,2,1));
-    pstoret<float,Packet4f,ResAlignment>(
-              &res.x(),
-              padd(psub(pmul(a,vec4f_swizzle1(b,3,3,3,3)),
-                                    pmul(vec4f_swizzle1(a,2,0,1,0),
-                                               vec4f_swizzle1(b,1,2,0,0))),
-                         pxor(mask,padd(s1,s2))));
-    
+    pstoret<float, Packet4f, ResAlignment>(
+        &res.x(),
+        padd<Packet4f>(
+            psub<Packet4f>(pmul<Packet4f>(a, vec4f_swizzle1(b, 3, 3, 3, 3)),
+                           pmul<Packet4f>(vec4f_swizzle1(a, 2, 0, 1, 0),
+                                          vec4f_swizzle1(b, 1, 2, 0, 0))),
+            pxor<Packet4f>(mask, padd(s1, s2))));
+
     return res;
   }
 };
diff --git a/bench/tensors/tensor_benchmarks_sycl.cc b/bench/tensors/tensor_benchmarks_sycl.cc
index 6f9f87179..bc68b234a 100644
--- a/bench/tensors/tensor_benchmarks_sycl.cc
+++ b/bench/tensors/tensor_benchmarks_sycl.cc
@@ -5,7 +5,7 @@
 
 #include "tensor_benchmarks.h"
 
-cl::sycl::gpu_selector selector;
+sycl::gpu_selector selector;
 Eigen::QueueInterface queue(selector);
 #define BM_FuncWithInput2DimsGPU(FUNC, D1, D2)                      \
   static void BM_##FUNC##_##D1##x##D2(int iters, int N) {           \
diff --git a/bench/tensors/tensor_contract_sycl_bench.cc b/bench/tensors/tensor_contract_sycl_bench.cc
index 8f2defe42..c9c93df63 100644
--- a/bench/tensors/tensor_contract_sycl_bench.cc
+++ b/bench/tensors/tensor_contract_sycl_bench.cc
@@ -306,7 +306,7 @@ void contractionABT(const Device& device_, TensorIndex num_iters, TensorIndex m_
 }
 
 int main() {
-  cl::sycl::gpu_selector selector;
+  sycl::gpu_selector selector;
   Eigen::QueueInterface queue(selector);
   Eigen::SyclDevice device(&queue);
   int64_t num_iters =20;
diff --git a/unsupported/Eigen/CXX11/Tensor b/unsupported/Eigen/CXX11/Tensor
index 2640f9565..90c43a3ef 100644
--- a/unsupported/Eigen/CXX11/Tensor
+++ b/unsupported/Eigen/CXX11/Tensor
@@ -67,6 +67,8 @@ typedef unsigned __int64 uint64_t;
   #include <iostream>
   #if defined(EIGEN_USE_HIP)
     #include <hip/hip_runtime.h>
+  #elif defined(EIGEN_USE_DPCPP)
+    #include <CL/sycl.hpp>
   #else
     #include <cuda_runtime.h>
   #endif
@@ -140,6 +142,13 @@ typedef unsigned __int64 uint64_t;
 #include "src/Tensor/TensorScanSycl.h"
 #endif
 
+#ifdef EIGEN_USE_DPCPP
+#include "src/Tensor/TensorReductionDpcpp.h"
+#include "src/Tensor/TensorContractionDpcpp.h"
+#include "src/Tensor/TensorScanDpcpp.h"
+#include "src/Tensor/TensorShufflingDpcpp.h"
+#endif
+
 #include "src/Tensor/TensorExecutor.h"
 #include "src/Tensor/TensorDevice.h"
 
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h b/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h
index 91a6f8d6c..29bed9aac 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h
@@ -127,7 +127,7 @@ struct TensorEvaluator<const TensorIndexTupleOp<ArgType>, Device>
   EIGEN_DEVICE_FUNC EvaluatorPointerType data() const { return NULL; }
 
 #ifdef EIGEN_USE_SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
@@ -278,7 +278,7 @@ struct TensorEvaluator<const TensorTupleReducerOp<ReduceOp, Dims, ArgType>, Devi
 
   EIGEN_DEVICE_FUNC EvaluatorPointerType data() const { return NULL; }
 #ifdef EIGEN_USE_SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
     m_orig_impl.bind(cgh);
   }
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h b/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h
index 72f072cf2..a15214e20 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h
@@ -95,12 +95,13 @@ struct TensorEvaluator<const TensorAssignOp<LeftArgType, RightArgType>, Device>
   typedef typename XprType::Index Index;
   typedef typename XprType::Scalar Scalar;
   typedef typename XprType::CoeffReturnType CoeffReturnType;
-  typedef typename PacketType<CoeffReturnType, Device>::type PacketReturnType;
+  typedef typename internal::traits<RightArgType>::XprTraits::Scalar InType;
+  typedef typename PacketType<const std::pair<CoeffReturnType, InType>, Device>::type PacketReturnType;
   typedef typename TensorEvaluator<RightArgType, Device>::Dimensions Dimensions;
   typedef StorageMemory<CoeffReturnType, Device> Storage;
   typedef typename Storage::Type EvaluatorPointerType;
 
-  static const int PacketSize = PacketType<CoeffReturnType, Device>::size;
+  static const int PacketSize = PacketType<const std::pair<CoeffReturnType, InType>, Device>::size;
   static const int NumDims = XprType::NumDims;
 
   enum {
@@ -168,10 +169,23 @@ struct TensorEvaluator<const TensorAssignOp<LeftArgType, RightArgType>, Device>
     m_rightImpl.cleanup();
   }
 
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void evalScalar(Index i) {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void evalScalar(Index i) const {
     m_leftImpl.coeffRef(i) = m_rightImpl.coeff(i);
   }
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void evalPacket(Index i) {
+
+  template<bool valid = std::is_same<bool, CoeffReturnType>::value>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename std::enable_if<valid>::type
+  evalPacket(Index i) const {
+
+    const int LhsStoreMode = TensorEvaluator<LeftArgType, Device>::IsAligned ? Aligned : Unaligned;
+    const int RhsLoadMode = TensorEvaluator<RightArgType, Device>::IsAligned ? Aligned : Unaligned;
+    auto packet_res = m_rightImpl.template packet<RhsLoadMode>(i);
+    internal::pstoret<Scalar, PacketReturnType, LhsStoreMode>(&(m_leftImpl.coeffRef(i)), packet_res);
+  }
+  
+  template<bool valid = std::is_same<bool, CoeffReturnType>::value>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename std::enable_if<!valid>::type
+  evalPacket(Index i) const {
 
     const int LhsStoreMode = TensorEvaluator<LeftArgType, Device>::IsAligned ? Aligned : Unaligned;
     const int RhsLoadMode = TensorEvaluator<RightArgType, Device>::IsAligned ? Aligned : Unaligned;
@@ -228,7 +242,7 @@ struct TensorEvaluator<const TensorAssignOp<LeftArgType, RightArgType>, Device>
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_leftImpl.bind(cgh);
     m_rightImpl.bind(cgh);
   }
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h b/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h
index fc75c8d9a..7a47182ed 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h
@@ -60,6 +60,12 @@ struct is_input_scalar<Sizes<Indices...> > {
   static const bool value = (Sizes<Indices...>::total_size == 1);
 };
 #endif
+#ifdef EIGEN_HAS_INDEX_LIST
+template <>
+struct is_input_scalar<Eigen::IndexList<Eigen::type2index<1>>> {
+  static const bool value = true;
+};
+#endif
 
 }  // end namespace internal
 
@@ -106,7 +112,7 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
   typedef typename PacketType<CoeffReturnType, Device>::type PacketReturnType;
   static const int PacketSize = PacketType<CoeffReturnType, Device>::size;
   protected: //  all the non-static fields must have the same access control, otherwise the TensorEvaluator wont be standard layout;
-  bool isCopy, nByOne, oneByN;
+  bool isCopy, isScalar, nByOne, oneByN;
   public:
   typedef StorageMemory<CoeffReturnType, Device> Storage;
   typedef typename Storage::Type EvaluatorPointerType;
@@ -121,6 +127,7 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
   };
 
   typedef typename internal::remove_const<Scalar>::type ScalarNoConst;
+  typedef internal::TensorIntDivisor<Index> IndexDivisor;
 
   // We do block based broadcasting using a trick with 2x tensor rank and 0
   // strides. See block method implementation for details.
@@ -140,7 +147,7 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
 
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TensorEvaluator(const XprType& op,
                                                         const Device& device)
-      : isCopy(false), nByOne(false), oneByN(false),
+      : isCopy(false), isScalar(false), nByOne(false), oneByN(false),
         m_device(device), m_broadcast(op.broadcast()), m_impl(op.expression(), device)
   {
 
@@ -150,12 +157,17 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
     EIGEN_STATIC_ASSERT((NumDims > 0), YOU_MADE_A_PROGRAMMING_MISTAKE);
     const InputDimensions& input_dims = m_impl.dimensions();
     isCopy = true;
+    isScalar = true;
     for (int i = 0; i < NumDims; ++i) {
       eigen_assert(input_dims[i] > 0);
       m_dimensions[i] = input_dims[i] * m_broadcast[i];
+      m_fastInputDims[i] = IndexDivisor(input_dims[i]);
       if (m_broadcast[i] != 1) {
         isCopy = false;
       }
+      if (input_dims[i] != 1) {
+        isScalar = false;
+      }
     }
 
     if (static_cast<int>(Layout) == static_cast<int>(ColMajor)) {
@@ -164,6 +176,7 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
       for (int i = 1; i < NumDims; ++i) {
         m_inputStrides[i] = m_inputStrides[i-1] * input_dims[i-1];
         m_outputStrides[i] = m_outputStrides[i-1] * m_dimensions[i-1];
+        m_fastStrides[i] = IndexDivisor(m_outputStrides[i]);
       }
     } else {
       m_inputStrides[NumDims-1] = 1;
@@ -171,6 +184,7 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
       for (int i = NumDims-2; i >= 0; --i) {
         m_inputStrides[i] = m_inputStrides[i+1] * input_dims[i+1];
         m_outputStrides[i] = m_outputStrides[i+1] * m_dimensions[i+1];
+        m_fastStrides[i] = IndexDivisor(m_outputStrides[i]);
       }
     }
 
@@ -211,6 +225,10 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
 
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Dimensions& dimensions() const { return m_dimensions; }
 
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const InputDimensions& inputDimensions() const { return m_impl.dimensions(); }
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const bool isCopyOrNot() const { return isCopy; }
+
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool evalSubExprsIfNeeded(EvaluatorPointerType) {
     m_impl.evalSubExprsIfNeeded(NULL);
     return true;
@@ -254,7 +272,7 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
     Index inputIndex = 0;
     EIGEN_UNROLL_LOOP
     for (int i = NumDims - 1; i > 0; --i) {
-      const Index idx = index / m_outputStrides[i];
+      const Index idx = index / m_fastStrides[i];
       if (internal::index_statically_eq<Broadcast>(i, 1)) {
         eigen_assert(idx < m_impl.dimensions()[i]);
         inputIndex += idx * m_inputStrides[i];
@@ -262,7 +280,13 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
         if (internal::index_statically_eq<InputDimensions>(i, 1)) {
           eigen_assert(idx % m_impl.dimensions()[i] == 0);
         } else {
-          inputIndex += (idx % m_impl.dimensions()[i]) * m_inputStrides[i];
+          if (m_impl.dimensions()[i] != 1) {
+            if (m_impl.dimensions()[i] == m_dimensions[i]) {
+              inputIndex += idx * m_inputStrides[i];
+            } else {
+              inputIndex += (idx - (idx / m_fastInputDims[i]) * m_impl.dimensions()[i]) * m_inputStrides[i]; 
+            }
+          }
         }
       }
       index -= idx * m_outputStrides[i];
@@ -274,7 +298,13 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
       if (internal::index_statically_eq<InputDimensions>(0, 1)) {
         eigen_assert(index % m_impl.dimensions()[0] == 0);
       } else {
-        inputIndex += (index % m_impl.dimensions()[0]);
+        if (m_impl.dimensions()[0] != 1) {
+          if (m_impl.dimensions()[0] == m_dimensions[0]) {
+            inputIndex += index * m_inputStrides[0];
+          } else {
+            inputIndex += (index- (index / m_fastInputDims[0]) * m_impl.dimensions()[0]); 
+          }
+        }
       }
     }
     return inputIndex;
@@ -289,7 +319,7 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
     Index inputIndex = 0;
     EIGEN_UNROLL_LOOP
     for (int i = 0; i < NumDims - 1; ++i) {
-      const Index idx = index / m_outputStrides[i];
+      const Index idx = index / m_fastStrides[i];
       if (internal::index_statically_eq<Broadcast>(i, 1)) {
         eigen_assert(idx < m_impl.dimensions()[i]);
         inputIndex += idx * m_inputStrides[i];
@@ -297,7 +327,13 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
         if (internal::index_statically_eq<InputDimensions>(i, 1)) {
           eigen_assert(idx % m_impl.dimensions()[i] == 0);
         } else {
-          inputIndex += (idx % m_impl.dimensions()[i]) * m_inputStrides[i];
+          if (m_impl.dimensions()[i] != 1) {
+            if (m_impl.dimensions()[i] == m_dimensions[i]) {
+              inputIndex += idx * m_inputStrides[i];
+            } else {
+              inputIndex += (idx - (idx / m_fastInputDims[i]) * m_impl.dimensions()[i]) * m_inputStrides[i]; 
+            }
+          }
         }
       }
       index -= idx * m_outputStrides[i];
@@ -309,7 +345,13 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
       if (internal::index_statically_eq<InputDimensions>(NumDims - 1, 1)) {
         eigen_assert(index % m_impl.dimensions()[NumDims - 1] == 0);
       } else {
-        inputIndex += (index % m_impl.dimensions()[NumDims - 1]);
+        if (m_impl.dimensions()[NumDims - 1] != 1) {
+          if (m_impl.dimensions()[NumDims - 1] == m_dimensions[NumDims - 1]) {
+            inputIndex += index;
+          } else {
+            inputIndex += (index - (index / m_fastInputDims[NumDims - 1]) * m_impl.dimensions()[NumDims - 1]); 
+          }
+        }
       }
     }
     return inputIndex;
@@ -320,10 +362,40 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
     return m_impl.coeff(indexRowMajor(index));
   }
 
+  // only use when not scalar and not copy.
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Index getInputIndex(Index index) const {
+    if (static_cast<int>(Layout) == static_cast<int>(ColMajor)) {
+      return indexColMajor(index);
+    }
+    else {
+      return indexRowMajor(index);
+    }
+  }
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType coeffInputIndex(Index index) const
+  {
+    return m_impl.coeff(index);
+  }
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Index getInputStride(Index index) const
+  {
+    return m_inputStrides[index];
+  }
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Index getOutputStride(Index index) const
+  {
+    return m_outputStrides[index];
+  }
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Index getBroadcast(Index index) const
+  {
+    return m_broadcast[index];
+  }
+
   template<int LoadMode>
   EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE PacketReturnType packet(Index index) const
   {
-    if (internal::is_input_scalar<typename internal::remove_all<InputDimensions>::type>::value) {
+    if (internal::is_input_scalar<typename internal::remove_all<InputDimensions>::type>::value || isScalar) {
       return internal::pset1<PacketReturnType>(m_impl.coeff(0));
     }
 
@@ -697,7 +769,7 @@ struct TensorEvaluator<const TensorBroadcastingOp<Broadcast, ArgType>, Device>
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(
-      cl::sycl::handler& cgh) const {
+      sycl::handler& cgh) const {
     m_impl.bind(cgh);
   }
 #endif
@@ -1085,6 +1157,8 @@ protected:
   Dimensions m_dimensions;
   array<Index, NumDims> m_outputStrides;
   array<Index, NumDims> m_inputStrides;
+  array<IndexDivisor, NumDims> m_fastInputDims;
+  array<IndexDivisor, NumDims> m_fastStrides;
   TensorEvaluator<ArgType, Device> m_impl;
 };
 
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h b/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h
index 5b28e706d..3d7ddd5e5 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h
@@ -372,7 +372,7 @@ struct TensorEvaluator<const TensorChippingOp<DimId, ArgType>, Device>
   }
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
@@ -453,13 +453,13 @@ struct TensorEvaluator<TensorChippingOp<DimId, ArgType>, Device>
     : Base(op, device)
     { }
 
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType& coeffRef(Index index)
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType& coeffRef(Index index) const
   {
     return this->m_impl.coeffRef(this->srcCoeff(index));
   }
 
   template <int StoreMode> EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
-  void writePacket(Index index, const PacketReturnType& x)
+  void writePacket(Index index, const PacketReturnType& x) const
   {
     EIGEN_STATIC_ASSERT((PacketSize > 1), YOU_MADE_A_PROGRAMMING_MISTAKE)
 
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorConcatenation.h b/unsupported/Eigen/CXX11/src/Tensor/TensorConcatenation.h
index 5968ff4b7..b4e8d6564 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorConcatenation.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorConcatenation.h
@@ -296,7 +296,7 @@ struct TensorEvaluator<const TensorConcatenationOp<Axis, LeftArgType, RightArgTy
 
   #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_leftImpl.bind(cgh);
     m_rightImpl.bind(cgh);
   }
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorContractionDpcpp.h b/unsupported/Eigen/CXX11/src/Tensor/TensorContractionDpcpp.h
new file mode 100644
index 000000000..5d9ab272c
--- /dev/null
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorContractionDpcpp.h
@@ -0,0 +1,1631 @@
+// This file is part of Eigen, a lightweight C++ template library for linear algebra.
+//
+// Mehdi Goli    Codeplay Software Ltd.
+// Ralph Potter  Codeplay Software Ltd.
+// Luke Iwanski  Codeplay Software Ltd.
+// Contact: <eigen@codeplay.com>
+//
+// This Source Code Form is subject to the terms of the Mozilla Public License v. 2.0. If a copy of the MPL was not
+// distributed with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+/*****************************************************************
+ * TensorContractionSycl.h
+ *
+ * \brief:
+ *  TensorContractionSycl.h, provides various tensor contraction kernel for SYCL backend
+ *
+ *****************************************************************/
+
+#if defined(EIGEN_USE_GPU) && !defined(EIGEN_CXX11_TENSOR_TENSOR_CONTRACTION_DPCPP_H)
+#define EIGEN_CXX11_TENSOR_TENSOR_CONTRACTION_DPCPP_H
+
+namespace Eigen {
+
+namespace internal {
+
+#ifndef EIGEN_SYCL_DISABLE_GEMV
+/*!
+ * \brief TVPanelSize, a template class used for setting the panel size required for launching General TensorVector
+ * contraction kernel on various hardware devices.
+ *
+ * \tparam Scalar: determines the element type of the tensor/vector
+ *
+ * \tparam StorageIndex  determines the Index type.
+ *
+ * \tparam NCWindow: determines the number of non-contracting element to be process by each work-group
+ *
+ * \tparam CFactor: determines the number of contracting element to be process by each thread
+ *
+ * \tparam NCFactor: determines the number of non-contracting element to be process by each thread
+ */
+template <typename Scalar, typename StorageIndex, StorageIndex NCWindow, StorageIndex CFactor, StorageIndex NCFactor>
+struct TVPanelSize {
+  // LocalThreadSizeC: determines total number of thread per workgroup for the contracting dimension
+  static EIGEN_CONSTEXPR StorageIndex LocalThreadSizeC = EIGEN_SYCL_LOCAL_THREAD_DIM0;
+  // LocalThreadSizeNC: determines total number of thread per workgroup for the non-contracting dimension
+  static EIGEN_CONSTEXPR StorageIndex LocalThreadSizeNC = EIGEN_SYCL_LOCAL_THREAD_DIM1;
+  // TileSizeDimNC: determines the tile size for the non-contracting dimension
+  static EIGEN_CONSTEXPR StorageIndex TileSizeDimNC = NCWindow / NCFactor;
+  // TileSizeDimC: determines the tile size for the contracting dimension
+  static EIGEN_CONSTEXPR StorageIndex TileSizeDimC = CFactor * LocalThreadSizeNC * LocalThreadSizeC;
+  // WorkLoadPerThreadNC : determines workload per thread for loading the non-contracting dimension
+  static EIGEN_CONSTEXPR StorageIndex WorkLoadPerThreadNC = TileSizeDimNC / LocalThreadSizeNC;
+  // WorkLoadPerThreadC: determines workload per thread for loading the non-contracting dimension
+  static EIGEN_CONSTEXPR StorageIndex WorkLoadPerThreadC = TileSizeDimC / LocalThreadSizeC;
+  // BC : determines if supporting bank conflict is required
+  static EIGEN_CONSTEXPR bool BC = false;
+};
+#endif
+
+/*!
+ * \brief TTPanelSize, a template class used for setting the panel size required for launching General Tensor Tensor
+ contraction kernel on various hardware devices.
+ *
+ * \tparam Scalar: determines the element type of the tensor
+ *
+ * \tparam StorageIndex: determines the Index type.
+ *
+ * \tparam REG_SIZE_M: determines workload per thread for loading the M dimension This can be varied based on the
+ available register on a chosen device(can be controlled by EIGEN_SYCL_REG_M macro).
+ *
+ * \tparam REG_SIZE_N: determines workload per thread for loading the N dimension This can be varied based on the
+ available register on a chosen device(can be controlled by EIGEN_SYCL_REG_N macro).
+ *
+ * \tparam TSDK: determines Tile size for dimension K. The packet size is assumed to be considered
+ */
+
+template <typename Scalar, typename StorageIndex, StorageIndex REG_SIZE_M, StorageIndex REG_SIZE_N, StorageIndex TSDK>
+struct TTPanelSize {
+  // TileSizeDimK: determines Tile size for dimension K. The packet size is assumed to be considered
+  static EIGEN_CONSTEXPR StorageIndex TileSizeDimK = TSDK;
+  // WorkLoadPerThreadM : determines workload per thread for loading the M dimension This can be varied based on the
+  // available register on a chosen device(can be controlled by EIGEN_SYCL_REG_M macro//
+#ifndef EIGEN_SYCL_REG_M
+  static EIGEN_CONSTEXPR StorageIndex WorkLoadPerThreadM = REG_SIZE_M;
+#else
+  static EIGEN_CONSTEXPR StorageIndex WorkLoadPerThreadM = EIGEN_SYCL_REG_M;
+#endif
+// WorkLoadPerThreadN : determines workload per thread for loading the N dimension This can be varied based on the
+// available register on a chosen device(can be controlled by EIGEN_SYCL_REG_N macro
+#ifndef EIGEN_SYCL_REG_N
+  static EIGEN_CONSTEXPR StorageIndex WorkLoadPerThreadN = REG_SIZE_N;
+#else
+  static EIGEN_CONSTEXPR StorageIndex WorkLoadPerThreadN = EIGEN_SYCL_REG_N;
+#endif
+  // LocalThreadSizeM: determines total number of thread per workgroup for the m dimension
+  static EIGEN_CONSTEXPR StorageIndex LocalThreadSizeM = EIGEN_SYCL_LOCAL_THREAD_DIM0;
+  // LocalThreadSizeN: determines total number of thread per workgroup for the n dimension
+  static EIGEN_CONSTEXPR StorageIndex LocalThreadSizeN = EIGEN_SYCL_LOCAL_THREAD_DIM1;
+  // TileSizeDimM: determines the tile size for the m dimension
+  static EIGEN_CONSTEXPR StorageIndex TileSizeDimM = LocalThreadSizeM * WorkLoadPerThreadM;
+  // TileSizeDimN: determines the tile size for the n dimension
+  static EIGEN_CONSTEXPR StorageIndex TileSizeDimN = LocalThreadSizeN * WorkLoadPerThreadN;
+  // LoadPerThreadLhs: determines workload per thread for loading Lhs Tensor. This must be divisable by packetsize
+  static EIGEN_CONSTEXPR StorageIndex LoadPerThreadLhs =
+      ((TileSizeDimK * WorkLoadPerThreadM * WorkLoadPerThreadN) / (TileSizeDimN));
+  // LoadPerThreadRhs: determines workload per thread for loading Rhs Tensor. This must be divisable by packetsize
+  static EIGEN_CONSTEXPR StorageIndex LoadPerThreadRhs =
+      ((TileSizeDimK * WorkLoadPerThreadM * WorkLoadPerThreadN) / (TileSizeDimM));
+  // BC : determines if supporting bank conflict is required
+  static EIGEN_CONSTEXPR bool BC = true;
+  // DoubleBuffer: determines if double buffering technique should be used (This can be disabled by
+  // EIGEN_SYCL_DISABLE_DOUBLE_BUFFER macro when the device doesnot have sufficient  local memory)
+  static EIGEN_CONSTEXPR bool DoubleBuffer =
+#ifdef EIGEN_SYCL_DISABLE_DOUBLE_BUFFER
+      false;
+#else
+      true;
+#endif
+};
+
+/* !
+ * \brief contraction_type: an enum class representing the Tensor Contraction implementation algorithm. This is used to
+ * specialize the contraction algorithm based on device support for dedicated local memory.
+ */
+enum class contraction_type { local, no_local };
+/* !
+ * \brief data_source an enum class determining the location of the data in a memory hierarchy (global, local, private).
+ */
+enum class data_source { global_mem, local_mem, private_mem };
+
+/*!
+ * \brief read, a template function used for loading the data from global
+ memory. This function is used to guarantee coalesced and vectorized load whenever possible
+ *
+ * \tparam PacketLoad: determines if the each element of this tensor block should be loaded in a packet mode
+ *
+ * \param is_coalesced_layout: determines whether or not the Tensor data in a memory can be access coalesced and
+ vectorized when possible. Coalesced memory access is a key factor in Kernel performance. When a tensor is 2d and the
+ contracting dimension is 1, it is always possible to accessed tensor data coalesced and vectorized. This is the case
+ when RHS(right hand side) Tensor is transposed or when LHS(left hand side) Tensor is not transposed.
+ *
+ * \tparam PacketType:  determines the type of packet
+ *
+ * \tparam TensorMapper: determines the input tensor mapper type
+ *
+ * \tparam StorageIndex: determines the Index type
+
+ * \param tensorMapper: is the input tensor
+ *
+ * \param NCIndex: is the non-contracting dim index
+ *
+ * \param CIndex is the contracting dim index
+ *
+ * \param ld: is the leading dimension of the flattened tensor
+ */
+template <bool PacketLoad, bool is_coalesced_layout, bool, typename PacketType, typename TensorMapper,
+          typename StorageIndex>
+static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename ::Eigen::internal::enable_if<PacketLoad, PacketType>::type read(
+    const TensorMapper &tensorMapper, const StorageIndex &NCIndex, const StorageIndex &CIndex, const StorageIndex &ld) {
+  const StorageIndex row = (is_coalesced_layout) ? NCIndex : CIndex;
+  const StorageIndex col = (is_coalesced_layout) ? CIndex : NCIndex;
+  return tensorMapper.get_tensor().template packet<Unaligned>(row + (col * ld));
+}
+
+/*!
+ * \brief read, special overload of read function, when the read access is not vectorized
+ *
+ * \tparam PacketLoad: determines if the each element of this tensor block should be loaded in a packet mode
+ *
+ * \param is_coalesced_layout: determines whether or not the Tensor data in a memory can be access coalesced and
+  vectorized when possible. Coalesced memory access is a key factor in Kernel performance. When a tensor is 2d and the
+  contracting dimension is 1, it is always possible to accessed tensor data coalesced and vectorized. This is the case
+  when RHS(right hand side) Tensor is transposed or when LHS(left hand side) Tensor is not transposed.
+ *
+ * \tparam PacketType: determines the type of packet
+ *
+ * \tparam TensorMapper: determines the input tensor mapper type
+ *
+ * \tparam StorageIndex: determines the Index type
+
+ * \param tensorMapper: is the input tensor
+ *
+ * \param NCIndex: is the non-contracting dim index
+ *
+ * \param CIndex: is the contracting dim index
+ */
+template <bool PacketLoad, bool, bool IsRhs, typename PacketType, typename TensorMapper, typename StorageIndex>
+static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename ::Eigen::internal::enable_if<!PacketLoad, PacketType>::type read(
+    const TensorMapper &tensorMapper, const StorageIndex &NCIndex, const StorageIndex &CIndex, const StorageIndex &) {
+  const StorageIndex row = (IsRhs) ? CIndex : NCIndex;
+  const StorageIndex col = (IsRhs) ? NCIndex : CIndex;
+  return tensorMapper(row, col);
+}
+
+/*!
+ * \brief write, a template function used for storing the data to local memory. This function is used to guarantee
+ * coalesced and vectorized store whenever possible.
+ *
+ * \tparam StorageIndex: determines the Index type
+ *
+ * \param ld is the leading dimension of the local memory. ld is a compile time value for the local memory
+ *
+ * \tparam data_source: an enum value representing if the location of the data in a memory hierarchy.
+ *
+ * \tparam PacketType:  determines the type of packet
+ *
+ * \tparam DataScalar: determines the output data type
+ *
+ * \param packet_data: the data to be written in the local memory
+ *
+ * \param ptr: a pointer to the local memory
+ *
+ * \param CIndex is the contracting dim index
+ */
+
+template <typename StorageIndex, StorageIndex ld, data_source dt, typename PacketType, typename DataScalar>
+static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+    typename ::Eigen::internal::enable_if<dt != data_source::global_mem, void>::type
+    write(PacketType &packet_data, DataScalar ptr) {
+  EIGEN_CONSTEXPR int PacketSize = Eigen::internal::unpacket_traits<PacketType>::size;
+  EIGEN_UNROLL_LOOP
+  for (int i = 0; i < PacketSize; i++) {
+    *ptr = PacketWrapper<PacketType, PacketSize>::scalarize(i, packet_data);
+    ptr += ld;
+  }
+}
+
+/*!
+ * \brief Overloading the write function for storing the data to global memory, when vectorization enabled This function
+ * is used to guarantee coalesced and vectorized store whenever possible.
+ *
+ * \tparam data_source: an enum value representing if the location of the data in a memory hierarchy.
+ *
+ * \tparam PacketType:  determines the type of packet
+ *
+ * \tparam DataScalar: determines the output data type
+ *
+ * \param packet_data: the data to be written in the local memory
+ *
+ * \param ptr: a pointer to the local memory
+ */
+
+template <data_source dt, typename PacketType, typename DataScalar>
+static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename ::Eigen::internal::enable_if<
+    Eigen::internal::unpacket_traits<PacketType>::size != 1 && dt == data_source::global_mem, void>::type
+write(PacketType &packet_data, DataScalar *ptr) {
+  ::Eigen::internal::pstoreu<DataScalar, PacketType>(ptr, packet_data);
+}
+
+/*!
+ * \brief Overloading the write function for storing the data to global memory, when vectorization is disabled.
+ *
+ * \tparam data_source: an enum value representing if the location of the data in a memory hierarchy.
+ *
+ * \tparam PacketType:  determines the type of packet
+ *
+ * \tparam DataScalar: determines the output data type
+ *
+ * \param packet_data: the data to be written in the local memory
+ *
+ * \param ptr: a pointer to the local memory
+ */
+template <data_source dt, typename PacketType, typename DataScalar>
+static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename ::Eigen::internal::enable_if<
+    Eigen::internal::unpacket_traits<PacketType>::size == 1 && dt == data_source::global_mem, void>::type
+write(PacketType &packet_data, DataScalar *ptr) {
+  *ptr = packet_data;
+}
+
+/*!
+ * \brief check_boundary: is used to check the edge condition for non-internal blocks.
+ *
+ * \tparam is_internal: determines if the block is internal
+ */
+template <bool is_internal>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool check_boundary(bool) {
+  return true;
+}
+
+/*!
+ * \brief check_boundary: specialization of the check_boundary for non-internal blocks.
+ *
+ * \param cond: true when the data is in range. Otherwise fals
+ */
+template <>
+EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool check_boundary<false>(bool cond) {
+  return cond;
+}
+
+/*!
+ * \brief BlockProperties is a template class that provides different characteristic of a block of each Tensor processed
+ * by each workgroup.
+ *
+ * \tparam is_transposed: iff true, determines whether or not the block of the Tensor is transposed
+ *
+ * \tparam packet_load_: determines if the each element of this tensor block should be loaded in a packet mode
+ *
+ * \tparam PacketType:  determines the type of packet
+ *
+ * \tparam OutType: determines the type of each element for this block of tensor. If packet load is true, it will be
+ * packetType; Otherwise it will be scalar Type
+ *
+ * \param elements_per_access determines the size of each element based on OutType
+ *
+ * \param is_coalesced_layout  determines whether or not the Tensor data in a memory can be access coalesced and
+ * vectorized when possible. Coalesced memory access is a key factor in Kernel performance. When a tensor is 2d and the
+ * contracting dimension is 1, it is always possible to accessed tensor data coalesced and vectorized. This is the case
+ * when RHS(right hand side) Tensor is transposed or when LHS(left hand side) Tensor is not transposed.
+ *
+ * \param nc_stride determines the stride of non-contracting dimension to access the next adjustment element within the
+ * Tensor Block for each workgroup
+ *
+ * \param c_stride  determines the stride of contracting dimension to access the next adjustment element within the
+ * Tensor Block for each workgroup
+ */
+template <bool is_transposed, bool is_rhs_, bool packet_load_, typename PacketType>
+struct BlockProperties {
+  static EIGEN_CONSTEXPR bool packet_load = packet_load_;
+  typedef typename Eigen::internal::unpacket_traits<PacketType>::type OutScalar;
+  static EIGEN_CONSTEXPR bool is_rhs = is_rhs_;
+  typedef typename Eigen::internal::conditional<packet_load, PacketType, OutScalar>::type OutType;
+  static EIGEN_CONSTEXPR int elements_per_access = Eigen::internal::unpacket_traits<OutType>::size;
+  static EIGEN_CONSTEXPR bool is_coalesced_layout = !(is_transposed ^ is_rhs);
+  static EIGEN_CONSTEXPR int nc_stride = (is_coalesced_layout ? elements_per_access : 1);
+  static EIGEN_CONSTEXPR int c_stride = (is_coalesced_layout ? 1 : elements_per_access);
+};
+
+/*!
+ * \brief ThreadProperties is a template class that provides each thread's properties within a workgroup.  Please see
+ * the sycl-1.2.1 specification (https://www.khronos.org/registry/SYCL/specs/sycl-1.2.1.pdf) for the workgroup,
+ * work-items
+ *
+ * \tparam StorageIndex: determines the StorageIndex Type
+ *
+ * \param linearLocalThreadId: determines the linearized location of a thread within a work-group
+ *
+ * \param kGroupId: determines the logical group id in a k dimension of the flattened tensor. It will be > 1 when
+ * tall/skinny algorithm is used
+ *
+ * \param mGroupOffset: determines the logical start position of all thread within a workgroup for the m dimension of
+ * the flattened tensor.
+ *
+ * \param kGroupOffset determines the logical start position of all thread within a workgroup for the k dimension of the
+ * flattened tensor. It will be > 1 when tall/skinny algorithm is used.
+ *
+ * \param mLocalOffset: determines the logical start position of each thread within a workgroup for the m dimension of a
+ * flattened tensor. The position determines the distance of each thread within the workgroup from each other
+ * independent from their global position.
+ *
+ * \param nLocalOffset: determines the logical start position of each thread within a workgroup for the n dimension of a
+ * flattened tensor. The position determines the distance of each thread within the workgroup from each other
+ * independent from their global position.
+ *
+ * \param mGlobalOffset: determines the logical start position of each thread a thread for the m dimension on a
+ * flattened tensor
+ *
+ * \param nGlobalOffset: determines the logical start position of each thread a thread for the n dimension on a
+ * flattened tensor
+ *
+ * \param kSize : determine the number of the k elements of the flattened Tensor to be processed by each thread for the
+ * given tensor block. This is !=K dimension of Flattened Tensor when Tall/Skinny matrix is used.
+ *
+ * \param is_internal : this will determined if the thread within the work-group computes an internal block of tensor or
+ * the edge blocks. When it is internal, there is no need to check the boundaries and all the if stantement can be
+ * resolve by compiler.
+ */
+template <typename StorageIndex>
+struct ThreadProperties {
+  const StorageIndex linearLocalThreadId;
+  const StorageIndex kGroupId;
+  const StorageIndex mGroupOffset;
+  const StorageIndex nGroupOffset;
+  const StorageIndex kGroupOffset;
+  const StorageIndex mLocalOffset;
+  const StorageIndex nLocalOffset;
+  const StorageIndex mGlobalOffset;
+  const StorageIndex nGlobalOffset;
+  StorageIndex kSize;
+  const bool is_internal;
+  // this is used to adjust the last block
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE ThreadProperties(
+      const StorageIndex linearLocalThreadId_, const StorageIndex kGroupId_, const StorageIndex mGroupOffset_,
+      const StorageIndex nGroupOffset_, const StorageIndex kGroupOffset_, const StorageIndex mLocalOffset_,
+      const StorageIndex nLocalOffset_, const StorageIndex mGlobalOffset_, const StorageIndex nGlobalOffset_,
+      StorageIndex kSize_, const bool is_internal_)
+      : linearLocalThreadId(linearLocalThreadId_),
+        kGroupId(kGroupId_),
+        mGroupOffset(mGroupOffset_),
+        nGroupOffset(nGroupOffset_),
+        kGroupOffset(kGroupOffset_),
+        mLocalOffset(mLocalOffset_),
+        nLocalOffset(nLocalOffset_),
+        mGlobalOffset(mGlobalOffset_),
+        nGlobalOffset(nGlobalOffset_),
+        kSize(kSize_),
+        is_internal(is_internal_) {}
+};
+
+/*!
+ * \brief TensorContractionKernelDpcpp is a template class that provides Tensor -Tensor contraction operation.
+ *
+ * \tparam OutScalar: determines the output scalar type
+ *
+ * \tparam LhsScalar: determines the left-hand-side scalar type
+ *
+ * \tparam RhsScalar: determines the right-hand-side scalar type
+ *
+ * \tparam OutAccessor: determines the sycl accessor type for out put (please see the sycl-1.2.1 specification
+ (https://www.khronos.org/registry/SYCL/specs/sycl-1.2.1.pdf) for accessor definition)
+ *
+ * \tparam LhsMapper determines the tensor contraction mapper type for left-hand-side matrix
+ *
+ * \tparam RhsMapper determines the tensor contraction mapper type for right-hand-side matrix
+ *
+ * \tparam StorageIndex: determines the StorageIndex Type
+ *
+ * \tparam Properties: determines the Contraction Panel properties
+ *
+ * \tparam TripleDim: determines the M, K, N dimensions for the flatten tensors in order to treat them as a matrix
+ *
+ * \tparam Vectorizable: determines whether or not the vectorization is enabled for the Eigen expression.
+ *
+ * \tparam input_mapper_properties : determine if the input tensors are matrix. If they are matrix, special memory
+ access is used to guarantee that always the memory access are coalesced.
+ *
+ * \tptaram IsFinal : determine if this is the final kernel. If so, the result will be written in a final output.
+ Otherwise, the result of contraction will be written iin a temporary buffer. This is the case when Tall/Skinny
+ contraction is used. So in this case, a final reduction step is required to compute final output.
+
+ * \tparam contraction_tp: it is an enum value representing whether the local memroy/no local memory implementation of
+ the algorithm to be used
+ *
+ * \param scratch: local memory containing tiles of LHS and RHS tensors for each work-group
+ *
+ * \param lhs: determines the left-hand-side flattened tensor (tensor mapper)
+ *
+ * \param rhs: determines the right-hand-side flattened tensor (tensor mapper)
+ *
+ * \param out_res: determines the output tensor containing the contraction result
+ *
+ * \param groupSizeM: a logical number determining the number of work-group for m dimension
+ *
+ * \param groupSizeN: a logical number determining the number of work-group for n dimension
+ *
+ * \param numTiles: determines total number of tiles on the k dimension
+ *
+ * \param TripleDim: determines the M, K, N dimensions for the flatten tensors in order to treat them as a matrix
+ */
+template <typename OutScalar, typename LhsScalar, typename RhsScalar, typename OutAccessor, typename LhsMapper,
+          typename RhsMapper, typename StorageIndex, typename Properties, typename TripleDim, bool Vectorizable,
+          typename input_mapper_properties, bool IsFinal, contraction_type contraction_tp>
+class TensorContractionKernelDpcpp {
+ public:
+  typedef typename Eigen::internal::Vectorise<OutScalar, Eigen::GpuDevice, Vectorizable>::PacketReturnType
+      PacketReturnType;
+  static EIGEN_CONSTEXPR int PacketSize =
+      Eigen::internal::Vectorise<OutScalar, Eigen::GpuDevice, Vectorizable>::PacketSize;
+  static EIGEN_CONSTEXPR bool is_lhs_transposed =
+      !::Eigen::internal::TensorContractionInputMapperTrait<LhsMapper>::inner_dim_contiguous;
+  static EIGEN_CONSTEXPR bool is_rhs_transposed =
+      !::Eigen::internal::TensorContractionInputMapperTrait<RhsMapper>::inner_dim_contiguous;
+
+  typedef BlockProperties<is_lhs_transposed, false, input_mapper_properties::is_lhs_matrix && Vectorizable,
+                          PacketReturnType>
+      LHSBlockProperties;
+
+  typedef BlockProperties<is_rhs_transposed, true, input_mapper_properties::is_rhs_matrix && Vectorizable,
+                          PacketReturnType>
+      RHSBlockProperties;
+
+  static EIGEN_CONSTEXPR StorageIndex NStride =
+      contraction_tp == contraction_type::local ? Properties::WorkLoadPerThreadN : RHSBlockProperties::nc_stride;
+
+  typedef sycl::accessor<OutScalar, 1, sycl::access::mode::read_write, sycl::access::target::local> Scratch;
+  typedef sycl::multi_ptr<OutScalar, sycl::access::address_space::local_space> local_ptr;
+  typedef OutScalar * /*sycl::multi_ptr<OutScalar, sycl::access::address_space::private_space>*/ private_ptr;
+  typedef
+      typename ::Eigen::internal::conditional<contraction_tp == contraction_type::local, local_ptr, private_ptr>::type
+          tile_ptr;
+  static EIGEN_CONSTEXPR StorageIndex LSDL = contraction_tp == contraction_type::local
+                                                 ? Properties::TileSizeDimM + Properties::BC
+                                                 : Properties::WorkLoadPerThreadM;
+  static EIGEN_CONSTEXPR StorageIndex LSDR = contraction_tp == contraction_type::local
+                                                 ? Properties::TileSizeDimN + Properties::BC
+                                                 : Properties::WorkLoadPerThreadN;
+  static EIGEN_CONSTEXPR StorageIndex LocalOffset = Properties::LocalThreadSizeM * Properties::LocalThreadSizeN;
+
+  /**
+   * \brief MemHolder this is a place holder struct for creating memory hierarchy in SYCL. Inside SYCL kernel it is not
+   * allowed to have dinamic memory allocation. while the local memory is created outside of the kernel and passed to
+   * the kenrel as an accessor, the private memory can only allowed to be allocated statically. Since we are abstracting
+   * the TiledMemory for both local and private memory, the MemHolder structs is used as a helper to abstract out
+   * different type of memory needed when local/no_local memory computation is called.
+   *
+   * \tparam contraction_type: it is an enum value representing whether the local memroy/no local memory implementation
+   of the algorithm to be used
+   * \tparam the private memory size
+   * \param ptr the tile memory pointer type
+   */
+  template <contraction_type, StorageIndex>
+  struct MemHolder {
+    tile_ptr ptr;
+    EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE MemHolder(local_ptr block_start_ptr) : ptr(block_start_ptr) {}
+  };
+  /**
+   * \brief specialization of memHolder class when no local memory kernel is used.
+   */
+  template <StorageIndex MemSize>
+  struct MemHolder<contraction_type::no_local, MemSize> {
+    OutScalar ptr[MemSize] = {OutScalar{0}};
+  };
+  /**
+   * \brief TiledMemory: contains required memory pointer for loading  each tile of the TensorContraction panel from
+   * global memory to local/private memory when local/no_local algorithm used.
+   *
+   * \param lhs_scratch_extract : determines the LHS tile memory. It is either private or local memory based on the
+   * selected contraction_type.
+   *
+   * \param rhs_scratch_extract : determines the RHS tile memory. It is either private or local memory based on the
+   * selected contraction_type.
+   *
+   * \param lhs_extract_index: determins the position of each thread on a local memory for lhs input. When private
+   * memory is used this is set to zero as this is not applicable in case of private memory.
+   *
+   * \param rhs_extract_index: determins the position of each thread on a local memory for rhs input. When private
+   * memory is used this is set to zero as this is not applicable in case of private memory.
+   *
+   * \param lhs_scratch_compute : determines the  location to load for computation for lhs_local memory. This is the
+   * same as lhs_scratch_extract for private memory.
+   *
+   * \param rhs_scratch_compute : determines the  location to load for computation for rhs_local memory. This is the
+   * same as rhs_scratch_extract for private memory.
+   */
+  struct TiledMemory {
+    MemHolder<contraction_tp, Properties::WorkLoadPerThreadM * Properties::TileSizeDimK> lhs_scratch_extract;
+    MemHolder<contraction_tp, Properties::WorkLoadPerThreadN * Properties::TileSizeDimK> rhs_scratch_extract;
+    tile_ptr lhs_scratch_ptr_compute;
+    tile_ptr rhs_scratch_ptr_compute;
+    const std::pair<StorageIndex, StorageIndex> lhs_extract_index;
+    const std::pair<StorageIndex, StorageIndex> rhs_extract_index;
+    template <contraction_type tp = contraction_tp>
+    EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+    TiledMemory(const ThreadProperties<StorageIndex> &, local_ptr,
+                typename ::Eigen::internal::enable_if<tp == contraction_type::no_local>::type * = 0)
+        : lhs_scratch_extract{},
+          rhs_scratch_extract{},
+          lhs_scratch_ptr_compute(lhs_scratch_extract.ptr),
+          rhs_scratch_ptr_compute(rhs_scratch_extract.ptr),
+          lhs_extract_index(std::pair<StorageIndex, StorageIndex>(StorageIndex{0}, StorageIndex{0})),
+          rhs_extract_index(std::pair<StorageIndex, StorageIndex>(StorageIndex{0}, StorageIndex{0})) {}
+
+    template <contraction_type tp = contraction_tp>
+    EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+    TiledMemory(const ThreadProperties<StorageIndex> &thread_properties, local_ptr block_start_ptr,
+                typename ::Eigen::internal::enable_if<tp == contraction_type::local>::type * = 0)
+        : lhs_scratch_extract{block_start_ptr},
+          rhs_scratch_extract{lhs_scratch_extract.ptr +
+                              ((Properties::DoubleBuffer + 1) * LSDL * Properties::TileSizeDimK)},
+          lhs_scratch_ptr_compute(lhs_scratch_extract.ptr + thread_properties.mLocalOffset),
+          rhs_scratch_ptr_compute(rhs_scratch_extract.ptr + thread_properties.nLocalOffset),
+          lhs_extract_index(
+              local_id_extract<LHSBlockProperties, Properties::TileSizeDimM>(thread_properties.linearLocalThreadId)),
+          rhs_extract_index(
+              local_id_extract<RHSBlockProperties, Properties::TileSizeDimN>(thread_properties.linearLocalThreadId)) {}
+  };
+
+  Scratch scratch;
+  const LhsMapper lhs;
+  const RhsMapper rhs;
+  OutAccessor out_res;
+  const StorageIndex groupSizeM;
+  const StorageIndex groupSizeN;
+  const StorageIndex numTiles;
+  const TripleDim triple_dim;
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TensorContractionKernelDpcpp(Scratch scratch_, const LhsMapper lhs_,
+                                                                const RhsMapper rhs_, OutAccessor out_res_,
+                                                                const StorageIndex groupSizeM_,
+                                                                const StorageIndex groupSizeN_,
+                                                                const StorageIndex numTiles_,
+                                                                const TripleDim triple_dim_)
+      : scratch(scratch_),
+        lhs(lhs_),
+        rhs(rhs_),
+        out_res(out_res_),
+        groupSizeM(groupSizeM_),
+        groupSizeN(groupSizeN_),
+        numTiles(numTiles_),
+        triple_dim(triple_dim_) {}
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TensorContractionKernelDpcpp(Scratch scratch_, const LhsMapper lhs_,
+                                                                const RhsMapper rhs_, OutAccessor out_res_,
+                                                                const StorageIndex groupSizeM_,
+                                                                const StorageIndex numTiles_,
+                                                                const TripleDim triple_dim_)
+      : TensorContractionKernelDpcpp(scratch_, lhs_, rhs_, out_res_, groupSizeM_, 1, numTiles_, triple_dim_) {}
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(sycl::nd_item<1> itemID) {
+    const StorageIndex linearLocalThreadId = itemID.get_local_id(0);
+    const StorageIndex nLocalThreadId = linearLocalThreadId / Properties::LocalThreadSizeM;
+    const StorageIndex mLocalThreadId = linearLocalThreadId % Properties::LocalThreadSizeM;
+    const StorageIndex mGroupId = itemID.get_group(0) % groupSizeM;
+    const StorageIndex tmp = itemID.get_group(0) / groupSizeM;
+    const StorageIndex nGroupId = IsFinal ? tmp : tmp % groupSizeN;
+    const StorageIndex kGroupId = IsFinal ? 0 : tmp / groupSizeN;
+    const StorageIndex mGroupOffset = mGroupId * Properties::TileSizeDimM;
+    const StorageIndex nGroupOffset = nGroupId * Properties::TileSizeDimN;
+    const StorageIndex mLocalOffset = PacketSize * mLocalThreadId;
+    const StorageIndex nLocalOffset = NStride * nLocalThreadId;
+    const StorageIndex mGlobalOffset = mGroupOffset + mLocalOffset;
+    const StorageIndex nGlobalOffset = nGroupOffset + nLocalOffset;
+
+    const StorageIndex kSizePerWG = IsFinal ? triple_dim.K : numTiles * Properties::TileSizeDimK;
+    StorageIndex kGroupOffset = kGroupId * kSizePerWG;
+    const bool is_internal = triple_dim.M - mGroupOffset >= Properties::TileSizeDimM &&
+                             triple_dim.N - nGroupOffset >= Properties::TileSizeDimN &&
+                             triple_dim.K - kGroupOffset >= kSizePerWG;
+    // this is used to adjust the last block
+    StorageIndex kSize = IsFinal ? triple_dim.K : std::min(kSizePerWG, triple_dim.K - kGroupOffset);
+    // This is used to find out the lats K offset so that kGroupOffset -kSize can compute the coffset for loading to
+    // tile
+    kGroupOffset += kSize;
+
+    auto thread_properties =
+        ThreadProperties<StorageIndex>(linearLocalThreadId, kGroupId, mGroupOffset, nGroupOffset, kGroupOffset,
+                                       mLocalOffset, nLocalOffset, mGlobalOffset, nGlobalOffset, kSize, is_internal);
+
+    auto out_ptr = out_res.get_pointer() + (IsFinal ? 0 : thread_properties.kGroupId * triple_dim.M * triple_dim.N);
+
+    (thread_properties.is_internal) ? compute_panel<true>(itemID, thread_properties, out_ptr)
+                                    : compute_panel<false>(itemID, thread_properties, out_ptr);
+  }
+  // The compute block computes the contraction operation private block for each thread and store the resutl in the
+  // privateRes memory of Each computation the compute block function is independent of local and no local concepts as
+  // it only compute the block on each thread's private memory space
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void compute_block_per_tile(OutScalar *lhs_block_ptr, OutScalar *rhs_block_ptr,
+                                                                    PacketReturnType *privateRes) {
+    StorageIndex idx = 0;
+    EIGEN_CONSTEXPR StorageIndex lhs_stride =
+        contraction_tp == contraction_type::local ? (PacketSize * Properties::LocalThreadSizeM) : 1;
+    EIGEN_UNROLL_LOOP
+    for (StorageIndex wLPTN = 0; wLPTN < Properties::WorkLoadPerThreadN; wLPTN++) {
+      auto rhsPacket = PacketReturnType{*(rhs_block_ptr + wLPTN)};
+      StorageIndex lhs_index = 0;
+      EIGEN_UNROLL_LOOP
+      for (StorageIndex wLPTM = 0; wLPTM < Properties::WorkLoadPerThreadM / PacketSize; wLPTM++) {
+        PacketReturnType lhsPack{};
+        Eigen::internal::PacketWrapper<PacketReturnType, PacketSize>::set_packet(lhsPack,
+                                                                                             lhs_block_ptr + lhs_index);
+        privateRes[idx] = ::Eigen::internal::pmadd(lhsPack, rhsPacket, privateRes[idx]);
+
+        lhs_index += lhs_stride;
+        idx++;
+      }
+    }
+  }
+  // The store function write the computed contraction operation in the private memory of each thread to the global
+  // memory. The store function is independent of local and no local concepts s that it can be abstract out in the base
+  // class.
+  template <bool is_internal_block, StorageIndex PrivateNStride, typename OutPtr>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void store(OutPtr *out_ptr, PacketReturnType *privateRes,
+                                                   StorageIndex mGlobalOffset, StorageIndex nGlobalOffset) {
+    auto chk_bound = [&](const StorageIndex &mIndex, const StorageIndex &nIndex) EIGEN_DEVICE_FUNC {
+      return (mIndex + PacketSize - 1 < triple_dim.M && nGlobalOffset + nIndex < triple_dim.N);
+    };
+    // when local memory is not used M and N are both accessed in a coalesced way. However, when local memory is
+    // available the k*N is transposed in the local to N*K therefore, each blocks operates on blockId*
+    // WorkLoadPerThreadN slice of N
+    EIGEN_CONSTEXPR StorageIndex GlobalNStride =
+        contraction_tp == contraction_type::local ? 1 : Properties::LocalThreadSizeN;
+    EIGEN_UNROLL_LOOP
+    for (StorageIndex wLPTN = 0; wLPTN < Properties::WorkLoadPerThreadN / PrivateNStride; wLPTN++) {
+      // output leading dimension
+      StorageIndex outputLD = 0;
+      // When local memory is used the PrivateNstride is always 1 because the coalesed access on N is loaded into Local
+      // memory and extracting from local to global is the same as no transposed version. However, when local memory is
+      // not used and RHS is transposed we packetize the load for RHS.
+      EIGEN_UNROLL_LOOP
+      for (StorageIndex nId = 0; nId < PrivateNStride; nId++) {
+        StorageIndex globalRow = mGlobalOffset;
+        EIGEN_UNROLL_LOOP
+        for (StorageIndex wLPTM = 0; wLPTM < Properties::WorkLoadPerThreadM / PacketSize; wLPTM++) {
+          PacketReturnType privetOut = privateRes[wLPTM];
+          if (check_boundary<is_internal_block>(chk_bound(globalRow, nId))) {
+            // Store the final results in C. The C matrix has always M as a first StorageIndex and N as a second
+            // StorageIndex Therefore it is always coalesced layout
+            write<data_source::global_mem>(privetOut, out_ptr + outputLD + globalRow);
+          } else {
+            EIGEN_UNROLL_LOOP
+            for (StorageIndex mId = 0; mId < PacketSize; mId++) {
+              StorageIndex mOffset = globalRow + mId;
+              if (mOffset < triple_dim.M && (nGlobalOffset + nId < triple_dim.N)) {
+                out_ptr[mOffset + outputLD] =
+                    Eigen::internal::PacketWrapper<PacketReturnType, PacketSize>::scalarize(mId, privetOut);
+              }
+            }
+          }
+          globalRow += (PacketSize * Properties::LocalThreadSizeM);
+        }
+        outputLD += triple_dim.M;
+        privateRes += Properties::WorkLoadPerThreadM / PacketSize;
+      }
+      out_ptr += (GlobalNStride * outputLD);
+
+      nGlobalOffset += (PrivateNStride * GlobalNStride);
+    }
+  }
+  // when no local memory is used the following extract_block will be enabled
+  template <typename InputBlockProperties, bool is_internal_block, typename Input, typename PrivateReg,
+            contraction_type contract_tp = contraction_tp>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+      typename ::Eigen::internal::enable_if<contract_tp == contraction_type::no_local>::type
+      extract_block(const Input &inpt, PrivateReg private_ptr, const std::pair<StorageIndex, StorageIndex> &,
+                    const StorageIndex &ncOffset, const StorageIndex cOffset) {
+    EIGEN_CONSTEXPR StorageIndex LocalThreadSizeNC =
+        InputBlockProperties::is_rhs ? Properties::LocalThreadSizeN : Properties::LocalThreadSizeM;
+    EIGEN_CONSTEXPR StorageIndex WorkLoadPerThreadNC =
+        InputBlockProperties::is_rhs ? Properties::WorkLoadPerThreadN : Properties::WorkLoadPerThreadM;
+    const StorageIndex &NC = InputBlockProperties::is_rhs ? triple_dim.N : triple_dim.M;
+
+    auto chk_bound = [&](const StorageIndex &CIndex, const StorageIndex &NCIndex) EIGEN_DEVICE_FUNC {
+      return ((CIndex + InputBlockProperties::c_stride - 1 < triple_dim.K) &&
+              (NCIndex + InputBlockProperties::nc_stride - 1 < NC));
+    };
+    const StorageIndex ld = InputBlockProperties::is_coalesced_layout ? NC : triple_dim.K;
+    StorageIndex cIndex = cOffset;
+
+    EIGEN_UNROLL_LOOP
+    for (StorageIndex cId = 0; cId < Properties::TileSizeDimK / InputBlockProperties::c_stride; cId++) {
+      StorageIndex ncIndex = ncOffset;
+      EIGEN_UNROLL_LOOP
+      for (StorageIndex ncId = 0; ncId < WorkLoadPerThreadNC / InputBlockProperties::nc_stride; ncId++) {
+        if (check_boundary<is_internal_block>(chk_bound(cIndex, ncIndex))) {
+          auto val =
+              read<InputBlockProperties::packet_load, InputBlockProperties::is_coalesced_layout,
+                   InputBlockProperties::is_rhs, typename InputBlockProperties::OutType>(inpt, ncIndex, cIndex, ld);
+
+          write<StorageIndex, (InputBlockProperties::is_coalesced_layout ? 1 : WorkLoadPerThreadNC),
+                data_source::private_mem>(val, private_ptr);
+        } else {
+          EIGEN_UNROLL_LOOP
+          for (StorageIndex i = 0; i < InputBlockProperties::elements_per_access; i++) {
+            const StorageIndex ncInd = ncIndex + (InputBlockProperties::is_coalesced_layout ? i : 0);
+            const StorageIndex cInd = cIndex + (InputBlockProperties::is_coalesced_layout ? 0 : i);
+            OutScalar val =
+                (ncInd < NC && cInd < triple_dim.K)
+                    ? read<false, InputBlockProperties::is_coalesced_layout, InputBlockProperties::is_rhs, OutScalar>(
+                          inpt, ncInd, cInd, ld)
+                    : OutScalar(0);
+            write<StorageIndex, (InputBlockProperties::is_coalesced_layout ? 1 : WorkLoadPerThreadNC),
+                  data_source::private_mem>(
+                val, private_ptr + (InputBlockProperties::is_coalesced_layout ? i : 0) +
+                         ((InputBlockProperties::is_coalesced_layout ? 0 : i) * WorkLoadPerThreadNC));
+          }
+        }
+
+        // if it is lhs we have to load it packetised when the packet size is > 1, because the output is coalesced. So
+        // even if M is not accessed in a coalesced mode, we have to load packet_size number of m per thread.
+        ncIndex = (!InputBlockProperties::is_rhs && InputBlockProperties::nc_stride == 1 && PacketSize != 1)
+                      ? ncOffset + (ncId + 1) % PacketSize + ((ncId + 1) / PacketSize) * LocalThreadSizeNC
+                      : (ncIndex + InputBlockProperties::nc_stride * LocalThreadSizeNC);
+        private_ptr += InputBlockProperties::nc_stride;
+      }
+      // the previous for loop ( private_ptr += (ncId * nc_stride)) has already moved ptr with one WorkLoadPerThreadNC
+      private_ptr += (InputBlockProperties::c_stride - 1) * WorkLoadPerThreadNC;
+      cIndex += InputBlockProperties::c_stride;
+    }
+  }
+  template <typename InputBlockProperties, StorageIndex TileSizeDimNC>
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE std::pair<StorageIndex, StorageIndex> local_id_extract(
+      const StorageIndex &linearLocalThreadId) {
+    const StorageIndex localThreadNC =
+        (InputBlockProperties::is_coalesced_layout)
+            ? linearLocalThreadId % (TileSizeDimNC / InputBlockProperties::nc_stride)
+            : linearLocalThreadId / (Properties::TileSizeDimK / InputBlockProperties::c_stride);
+    const StorageIndex localThreadC =
+        (InputBlockProperties::is_coalesced_layout)
+            ? linearLocalThreadId / (TileSizeDimNC / InputBlockProperties::nc_stride)
+            : linearLocalThreadId % (Properties::TileSizeDimK / InputBlockProperties::c_stride);
+    return std::pair<StorageIndex, StorageIndex>(localThreadNC, localThreadC);
+  }
+
+  template <bool db = Properties::DoubleBuffer, contraction_type ctp = contraction_tp>
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+      typename ::Eigen::internal::enable_if<db && ctp == contraction_type::local>::type
+      sync_mem(const sycl::nd_item<1> &, bool &db_offset) noexcept {
+    db_offset = !db_offset;
+  }
+
+  template <bool db = Properties::DoubleBuffer, contraction_type ctp = contraction_tp>
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+      typename ::Eigen::internal::enable_if<!db && ctp == contraction_type::local>::type
+      sync_mem(const sycl::nd_item<1> &itemID, bool &) noexcept {
+    sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+  }
+
+  template <contraction_type ctp = contraction_tp>
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+      typename ::Eigen::internal::enable_if<ctp == contraction_type::no_local>::type
+      sync_mem(const sycl::nd_item<1> &, bool &) noexcept {
+    return;
+  }
+
+  template <bool need_sync, contraction_type ctp = contraction_tp>
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+      typename ::Eigen::internal::enable_if<need_sync && ctp == contraction_type::no_local>::type
+      sync_thread(const sycl::nd_item<1> &
+#ifdef EIGEN_SYCL_ARM_GPU_CACHE_OPTIMISATION
+                      itemID
+#endif
+                  ) noexcept {
+#ifdef EIGEN_SYCL_ARM_GPU_CACHE_OPTIMISATION
+    sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+#else
+    return;
+#endif
+  }
+  template <bool need_sync, contraction_type ctp = contraction_tp>
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+      typename ::Eigen::internal::enable_if<need_sync && ctp == contraction_type::local>::type
+      sync_thread(const sycl::nd_item<1> &itemID) {
+    sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+  }
+  template <bool need_sync>
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename ::Eigen::internal::enable_if<!need_sync>::type sync_thread(
+      const sycl::nd_item<1> &) {
+    return;
+  }
+
+  template <bool is_internal_block>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void compute_tile_per_panel(const sycl::nd_item<1> &itemID,
+                                                                    ThreadProperties<StorageIndex> &thread_properties,
+                                                                    TiledMemory &tiled_input_block,
+                                                                    PacketReturnType *privateRes, bool &db_offset) {
+    // Tiling the Rhs block from global to local memory
+    extract_block<RHSBlockProperties, is_internal_block>(
+        rhs, tiled_input_block.rhs_scratch_extract.ptr + (db_offset * Properties::TileSizeDimK * LSDR),
+        tiled_input_block.rhs_extract_index,
+        contraction_tp == contraction_type::local ? thread_properties.nGroupOffset : thread_properties.nGlobalOffset,
+        thread_properties.kGroupOffset - thread_properties.kSize);
+
+    sync_thread<contraction_tp == contraction_type::no_local>(itemID);
+
+    // Tiling the Lhs block from global to local memory
+    extract_block<LHSBlockProperties, is_internal_block>(
+        lhs, tiled_input_block.lhs_scratch_extract.ptr + (db_offset * LSDL * Properties::TileSizeDimK),
+        tiled_input_block.lhs_extract_index,
+        contraction_tp == contraction_type::local ? thread_properties.mGroupOffset : thread_properties.mGlobalOffset,
+        thread_properties.kGroupOffset - thread_properties.kSize);
+
+    // sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+    sync_thread<contraction_tp == contraction_type::local>(itemID);
+    // switch to compute mede
+    StorageIndex lhs_offset = (db_offset * LSDL * Properties::TileSizeDimK);
+    StorageIndex rhs_offset = (db_offset * Properties::TileSizeDimK * LSDR);
+    // Loop over the values of a single tile
+    for (StorageIndex k = 0; k < Properties::TileSizeDimK; k++) {
+      compute_block_per_tile(tiled_input_block.lhs_scratch_ptr_compute + lhs_offset,
+                             tiled_input_block.rhs_scratch_ptr_compute + rhs_offset, privateRes);
+      lhs_offset += LSDL;
+      rhs_offset += LSDR;
+    }
+    // computing the K index for the next tile
+    thread_properties.kSize -= Properties::TileSizeDimK;
+    sync_mem(itemID, db_offset);
+  }
+
+  // when local memory is available the following compute_panel will be enabled
+  template <bool is_internal_block, typename OutPtr>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void compute_panel(const sycl::nd_item<1> &itemID,
+                                                           ThreadProperties<StorageIndex> &thread_properties,
+                                                           OutPtr out_ptr) {
+    auto tiled_input_block = TiledMemory{thread_properties, scratch.get_pointer()};
+    // Allocate register space
+    PacketReturnType privateRes[Properties::WorkLoadPerThreadM * Properties::WorkLoadPerThreadN / PacketSize] = {
+        PacketReturnType{0}};
+    bool db_offset = 0;
+
+    while (thread_properties.kSize >= Properties::TileSizeDimK) {
+      compute_tile_per_panel<is_internal_block>(itemID, thread_properties, tiled_input_block, privateRes, db_offset);
+    }
+    if (thread_properties.kSize > 0) {
+      compute_tile_per_panel<false>(itemID, thread_properties, tiled_input_block, privateRes, db_offset);
+    }
+
+    // Storing the final results in the output
+    store<is_internal_block,
+          contraction_tp == contraction_type::local ? static_cast<StorageIndex>(1) : RHSBlockProperties::nc_stride>(
+        out_ptr + thread_properties.nGlobalOffset * triple_dim.M, privateRes, thread_properties.mGlobalOffset,
+        thread_properties.nGlobalOffset);
+  }
+  // When local memory is available the following extract_block will be enabled
+  template <typename InputBlockProperties, bool is_internal_block, typename Input, typename Local,
+            contraction_type contract_tp = contraction_tp>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+      typename ::Eigen::internal::enable_if<contract_tp == contraction_type::local>::type
+      extract_block(const Input &inpt, Local local_ptr, const std::pair<StorageIndex, StorageIndex>& local_index,
+                    const StorageIndex &ncOffset, const StorageIndex cOffset) {
+    EIGEN_CONSTEXPR StorageIndex TileSizeDimNC =
+        InputBlockProperties::is_rhs ? Properties::TileSizeDimN : Properties::TileSizeDimM;
+    EIGEN_CONSTEXPR StorageIndex LoadPerThread =
+        InputBlockProperties::is_rhs ? Properties::LoadPerThreadRhs : Properties::LoadPerThreadLhs;
+    EIGEN_CONSTEXPR StorageIndex LSD = InputBlockProperties::is_rhs ? LSDR : LSDL;
+    static_assert(((LocalOffset % (TileSizeDimNC / InputBlockProperties::nc_stride) == 0) &&
+                   (LocalOffset % (Properties::TileSizeDimK / InputBlockProperties::c_stride) == 0)),
+                  " LocalOffset must be divisable by stride");
+    const StorageIndex &NC = InputBlockProperties::is_rhs ? triple_dim.N : triple_dim.M;
+    StorageIndex localThreadNC = local_index.first;
+    StorageIndex localThreadC = local_index.second;
+    auto chk_bound = [&](const StorageIndex &CIndex, const StorageIndex &NCIndex) EIGEN_DEVICE_FUNC {
+      return ((CIndex + InputBlockProperties::c_stride - 1 < triple_dim.K) &&
+              (NCIndex + InputBlockProperties::nc_stride - 1 < NC));
+    };
+    EIGEN_UNROLL_LOOP
+    for (StorageIndex lPT = 0; lPT < LoadPerThread / InputBlockProperties::elements_per_access; lPT++) {
+      const StorageIndex CIndex = cOffset + (InputBlockProperties::c_stride * localThreadC);
+      const StorageIndex NCIndex = ncOffset + (InputBlockProperties::nc_stride * localThreadNC);
+      const StorageIndex ld = InputBlockProperties::is_coalesced_layout ? NC : triple_dim.K;
+      if (check_boundary<is_internal_block>(chk_bound(CIndex, NCIndex))) {
+        auto val =
+            read<InputBlockProperties::packet_load, InputBlockProperties::is_coalesced_layout,
+                 InputBlockProperties::is_rhs, typename InputBlockProperties::OutType>(inpt, NCIndex, CIndex, ld);
+        write<StorageIndex, (InputBlockProperties::is_coalesced_layout ? 1 : LSD), data_source::local_mem>(
+            val, local_ptr + (InputBlockProperties::nc_stride * localThreadNC) +
+                     (InputBlockProperties::c_stride * localThreadC * LSD));
+      } else {
+        EIGEN_UNROLL_LOOP
+        for (StorageIndex i = 0; i < InputBlockProperties::elements_per_access; i++) {
+          const StorageIndex nCInd = NCIndex + (InputBlockProperties::is_coalesced_layout ? i : 0);
+          const StorageIndex cInd = CIndex + (InputBlockProperties::is_coalesced_layout ? 0 : i);
+          OutScalar val =
+              (nCInd < NC && cInd < triple_dim.K)
+                  ? read<false, InputBlockProperties::is_coalesced_layout, InputBlockProperties::is_rhs, OutScalar>(
+                        inpt, nCInd, cInd, ld)
+                  : OutScalar(0);
+
+          write<StorageIndex, (InputBlockProperties::is_coalesced_layout ? 1 : LSD), data_source::local_mem>(
+              val, local_ptr + (InputBlockProperties::nc_stride * localThreadNC) +
+                       (InputBlockProperties::is_coalesced_layout ? i : 0) +
+                       ((InputBlockProperties::c_stride * localThreadC +
+                         (InputBlockProperties::is_coalesced_layout ? 0 : i)) *
+                        LSD));
+        }
+      }
+      localThreadNC += (InputBlockProperties::is_coalesced_layout)
+                           ? LocalOffset % (TileSizeDimNC / InputBlockProperties::nc_stride)
+                           : LocalOffset / (Properties::TileSizeDimK / InputBlockProperties::c_stride);
+      localThreadC += (InputBlockProperties::is_coalesced_layout)
+                          ? LocalOffset / (TileSizeDimNC / InputBlockProperties::nc_stride)
+                          : LocalOffset % (Properties::TileSizeDimK / InputBlockProperties::c_stride);
+    }
+  }
+};
+
+#ifndef EIGEN_SYCL_DISABLE_GEMV
+
+/*!
+ * \brief GeneralVectorTensor is a template class that provides Tensor -vector contraction operation, which is a special
+ * case of Tensor Tensor contraction.
+ *
+ * \tparam OutScalar: determines the output scalar type
+ *
+ * \tparam OutAccessor: determines the sycl accessor type for out put (please see the sycl-1.2.1 specification
+ * (https://www.khronos.org/registry/SYCL/specs/sycl-1.2.1.pdf) for accessor definition)
+ *
+ * \tparam VectorMapper: determines the tensor contraction mapper for the vector input (can be lhs or rhs)
+ *
+ * \tparam TensorMapper: determines the tensor contraction mapper for the tensor input (can be lhs or rhs)
+ *
+ * \tparam StorageIndex: determines the StorageIndex Type
+ *
+ * \tparam Properties: determines the Contraction Panel properties
+ *
+ * \tparam KFactor: determines the number of elements in K dimension in a Tile
+ *
+ * \tparam Vectorizable: determines whether or not the vectorization is enabled for the Eigen expression.
+ *
+ * \tparam is_lhs_vec: determines whether lhs is a vector or rhs is a vector
+ *
+ * \tparam IsFinal: determine if this is the final kernel. If so, the result will be written in a final output.
+ * Otherwise, the result of contraction will be written iin a temporary buffer.
+ *
+ * \param scratch: determines the local memory containing the vector block for each work-group
+ *
+ * \param vec: determines the vector input (tensor mapper)
+ *
+ * \param mat: determines the tensor input (tensor mapper)
+ *
+ * \param out_res: determines the output vector containing the contraction result
+ *
+ * \param nonContractGroupSize: a logical number determining the number of work-group for non-contracting dimension
+ *
+ * \param nonContractDim: determines the size of non contracting dimension for the flattened tensor
+ *
+ * \param contractDim: determines the size of non contracting dimension for the flattened tensor
+ *
+ */
+template <typename OutScalar, typename OutAccessor, typename VectorMapper, typename TensorMapper, typename StorageIndex,
+          typename Properties, StorageIndex KFactor, bool Vectorizable, bool is_lhs_vec, bool IsFinal>
+struct GeneralVectorTensor {
+  typedef typename Eigen::internal::Vectorise<OutScalar, Eigen::GpuDevice, Vectorizable>::PacketReturnType
+      PacketReturnType;
+  static EIGEN_CONSTEXPR int PacketSize =
+      Eigen::internal::Vectorise<OutScalar, Eigen::GpuDevice, Vectorizable>::PacketSize;
+  typedef sycl::accessor<OutScalar, 1, sycl::access::mode::read_write, sycl::access::target::local> Scratch;
+
+  static EIGEN_CONSTEXPR StorageIndex OutScratchOffset =
+      KFactor * Properties::LocalThreadSizeC * Properties::LocalThreadSizeNC;
+
+  // Since the access layout for a vector can always be coalesced, when LHS is a vector, we pass false and false to make
+  // sure that the !^ is true When RHS is a vector, we pass true and true to make sure that the !^ is true.
+  typedef BlockProperties<is_lhs_vec ? false : true, is_lhs_vec ? false : true, Vectorizable, PacketReturnType>
+      VecBlockProperties;
+
+  Scratch scratch;
+  const VectorMapper vec;
+  const TensorMapper mat;
+  OutAccessor out_res;
+  const StorageIndex nonContractGroupSize;
+  const StorageIndex nonContractDim;
+  const StorageIndex contractDim;
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE GeneralVectorTensor(Scratch scratch_, const VectorMapper vec_,
+                                                            const TensorMapper mat_, OutAccessor out_res_,
+                                                            const StorageIndex nonContractGroupSize_,
+                                                            const StorageIndex nonContractDim_,
+                                                            const StorageIndex contractDim_)
+      : scratch(scratch_),
+        vec(vec_),
+        mat(mat_),
+        out_res(out_res_),
+        nonContractGroupSize(nonContractGroupSize_),
+        nonContractDim(nonContractDim_),
+        contractDim(contractDim_) {}
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(sycl::nd_item<1> itemID) {
+    auto scratch_ptr = scratch.get_pointer();
+    const StorageIndex linearLocalThreadId = itemID.get_local_id(0);
+    StorageIndex nonContractId = is_lhs_vec ? linearLocalThreadId / Properties::LocalThreadSizeC
+                                            : linearLocalThreadId % Properties::LocalThreadSizeNC;
+    StorageIndex contractId = is_lhs_vec ? linearLocalThreadId % Properties::LocalThreadSizeC
+                                         : linearLocalThreadId / Properties::LocalThreadSizeNC;
+    const StorageIndex cGroupSize = itemID.get_group_range(0) / nonContractGroupSize;
+    const StorageIndex nonContractGroupId =
+        is_lhs_vec ? itemID.get_group(0) / cGroupSize : itemID.get_group(0) % nonContractGroupSize;
+    const StorageIndex contractGroupId =
+        is_lhs_vec ? itemID.get_group(0) % cGroupSize : itemID.get_group(0) / nonContractGroupSize;
+    auto out_ptr = out_res.get_pointer() + (IsFinal ? 0 : contractGroupId * nonContractDim);
+
+    const StorageIndex nonContractGroupOffset = nonContractGroupId * Properties::TileSizeDimNC;
+    const StorageIndex contractGroupOffset = contractGroupId * Properties::TileSizeDimC;
+    auto outScratchIndex = nonContractId + contractId * Properties::LocalThreadSizeNC;
+    const StorageIndex globalNonContractDimOffset = nonContractGroupOffset + nonContractId;
+    const StorageIndex globalContractDimOffset = contractGroupOffset + contractId;
+    auto local_output = scratch_ptr + OutScratchOffset;
+    const bool is_internal = nonContractDim - nonContractGroupOffset >= Properties::TileSizeDimNC &&
+                             contractDim - contractGroupOffset >= Properties::TileSizeDimC;
+    is_internal
+        ? compute_panel<true>(itemID, vec, mat, local_output, out_ptr,
+#ifdef EIGEN_SYCL_LOCAL_MEM_UNSET_OR_ON
+                              scratch_ptr, contractGroupOffset,
+#endif
+                              nonContractGroupOffset, linearLocalThreadId, contractDim, nonContractDim, contractId,
+                              nonContractId, globalContractDimOffset, globalNonContractDimOffset, outScratchIndex)
+        : compute_panel<false>(itemID, vec, mat, local_output, out_ptr,
+#ifdef EIGEN_SYCL_LOCAL_MEM_UNSET_OR_ON
+                               scratch_ptr, contractGroupOffset,
+#endif
+                               nonContractGroupOffset, linearLocalThreadId, contractDim, nonContractDim, contractId,
+                               nonContractId, globalContractDimOffset, globalNonContractDimOffset, outScratchIndex);
+  }
+  template <bool is_internal_block, typename OutPtr>
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void compute_panel(
+      const sycl::nd_item<1> &itemID, const VectorMapper &vec, const TensorMapper &mat, OutScalar *local_output,
+      OutPtr out_ptr,
+#ifdef EIGEN_SYCL_LOCAL_MEM_UNSET_OR_ON
+      OutScalar *scratch_ptr, const StorageIndex contractGroupOffset,
+#endif
+      const StorageIndex nonContractGroupOffset, const StorageIndex linearLocalThreadId, StorageIndex contractDim,
+      StorageIndex nonContractDim, StorageIndex contractId, StorageIndex nonContractId,
+      StorageIndex globalContractDimOffset, StorageIndex globalNonContractDimOffset, StorageIndex outScratchIndex) {
+    OutScalar outScalar[Properties::WorkLoadPerThreadNC] = {OutScalar(0)};
+    // Reading the vector
+#ifdef EIGEN_SYCL_LOCAL_MEM_UNSET_OR_ON
+    const StorageIndex vectorOffset = contractGroupOffset + linearLocalThreadId;
+    extract_block<VecBlockProperties, is_internal_block, KFactor,
+                  Properties::LocalThreadSizeNC * Properties::LocalThreadSizeC>(vec, scratch_ptr, linearLocalThreadId,
+                                                                                vectorOffset, contractDim);
+
+    sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+    auto in_scratch_ptr = scratch_ptr + contractId;
+#endif
+
+    StorageIndex privateOffsetC = 0;
+    EIGEN_UNROLL_LOOP
+    for (StorageIndex i = 0; i < Properties::WorkLoadPerThreadC; i++) {
+      StorageIndex privateOffsetNC = 0;
+      bool contract_conds = ((globalContractDimOffset + privateOffsetC) < contractDim);
+#ifdef EIGEN_SYCL_LOCAL_MEM_UNSET_OR_ON
+      auto vecScalar = *in_scratch_ptr;
+#else
+      auto vecScalar = (check_boundary<is_internal_block>(contract_conds))
+                           ? vec(is_lhs_vec ? StorageIndex(0) : globalContractDimOffset + privateOffsetC,
+                                 is_lhs_vec ? globalContractDimOffset + privateOffsetC : StorageIndex(0))
+                           : OutScalar(0);
+#endif
+      EIGEN_UNROLL_LOOP
+      for (StorageIndex j = 0; j < Properties::WorkLoadPerThreadNC; j++) {
+        auto matScalar = (check_boundary<is_internal_block>(
+                             contract_conds && ((globalNonContractDimOffset + privateOffsetNC) < nonContractDim)))
+                             ? mat(is_lhs_vec ? globalContractDimOffset + privateOffsetC
+                                              : globalNonContractDimOffset + privateOffsetNC,
+                                   is_lhs_vec ? globalNonContractDimOffset + privateOffsetNC
+                                              : globalContractDimOffset + privateOffsetC)
+                             : OutScalar(0);
+
+        outScalar[j] = sycl::mad(matScalar, vecScalar, outScalar[j]);
+        privateOffsetNC += Properties::LocalThreadSizeNC;
+      }
+      privateOffsetC += Properties::LocalThreadSizeC;
+#ifdef EIGEN_SYCL_LOCAL_MEM_UNSET_OR_ON
+      in_scratch_ptr += Properties::LocalThreadSizeC;
+#endif
+    }
+
+    auto out_scratch_ptr = local_output + outScratchIndex;
+    // Each block of 16*16 element in shared memory should reduce to 16*1
+    EIGEN_UNROLL_LOOP
+    for (StorageIndex j = 0; j < Properties::WorkLoadPerThreadNC; j++) {
+      *out_scratch_ptr = outScalar[j];
+
+      out_scratch_ptr += (Properties::LocalThreadSizeNC * Properties::LocalThreadSizeC);
+    }
+    if (is_lhs_vec) {
+      nonContractId = linearLocalThreadId % Properties::LocalThreadSizeNC;
+      contractId = linearLocalThreadId / Properties::LocalThreadSizeNC;
+      outScratchIndex = nonContractId + contractId * Properties::LocalThreadSizeNC;
+    }
+
+    out_scratch_ptr = local_output + outScratchIndex;
+    EIGEN_UNROLL_LOOP
+    for (StorageIndex j = 0; j < Properties::WorkLoadPerThreadNC; j++) {
+      EIGEN_UNROLL_LOOP
+      for (StorageIndex offset = Properties::LocalThreadSizeC >> 1; offset > 0; offset >>= 1) {
+        sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+        if (contractId < offset) {
+          StorageIndex myNeigbourId = (Properties::LocalThreadSizeNC * offset);
+          *out_scratch_ptr += out_scratch_ptr[myNeigbourId];
+        }
+      }
+      // moving to next 16 by 16 block
+      out_scratch_ptr += (Properties::LocalThreadSizeNC * Properties::LocalThreadSizeC);
+    }
+
+    if (contractId == 0) {
+      out_scratch_ptr = local_output + nonContractId;
+      StorageIndex global_final_offset = nonContractGroupOffset + nonContractId;
+      out_ptr += global_final_offset;
+      EIGEN_UNROLL_LOOP
+      for (StorageIndex j = 0; j < Properties::WorkLoadPerThreadNC; j++) {
+        if (check_boundary<is_internal_block>(global_final_offset < nonContractDim)) {
+          auto res = *out_scratch_ptr;
+
+          *out_ptr = res;
+          out_ptr += Properties::LocalThreadSizeNC;
+        }
+        // moving to next 16 by 16 block to ge the next 16 reduced elements
+        out_scratch_ptr += (Properties::LocalThreadSizeNC * Properties::LocalThreadSizeC);
+        if (!(is_internal_block)) global_final_offset += Properties::LocalThreadSizeNC;
+      }
+    }
+  }
+
+  template <typename InputBlockProperties, bool is_internal_block, int CFactor, int GroupSize, typename Input,
+            typename Local>
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void extract_block(const Input &inpt, Local *local_ptr,
+                                                                  const StorageIndex &linearLocalThreadId,
+                                                                  const StorageIndex &cOffset, const StorageIndex &C) {
+    local_ptr += InputBlockProperties::c_stride * linearLocalThreadId;
+    StorageIndex cIndex = cOffset;
+    for (StorageIndex cId = 0; cId < CFactor / InputBlockProperties::c_stride; cId++) {
+      if (check_boundary<is_internal_block>(cIndex + InputBlockProperties::c_stride - 1 < C)) {
+        auto val = read<InputBlockProperties::packet_load, InputBlockProperties::is_coalesced_layout,
+                        InputBlockProperties::is_rhs, typename InputBlockProperties::OutType>(inpt, StorageIndex(0),
+                                                                                              cIndex, StorageIndex(1));
+        write<StorageIndex, 1, data_source::local_mem>(val, local_ptr);
+      } else {
+        EIGEN_UNROLL_LOOP
+        for (StorageIndex i = 0; i < InputBlockProperties::elements_per_access; i++) {
+          OutScalar val =
+              (cIndex + i < C)
+                  ? read<false, InputBlockProperties::is_coalesced_layout, InputBlockProperties::is_rhs, OutScalar>(
+                        inpt, StorageIndex(0), cIndex + i, StorageIndex(1))
+                  : OutScalar(0);
+          write<StorageIndex, 1, data_source::local_mem>(val, local_ptr + i);
+        }
+      }
+      local_ptr += InputBlockProperties::c_stride * GroupSize;
+      cIndex += InputBlockProperties::c_stride * GroupSize;
+    }
+  }
+};
+#endif
+
+#ifndef EIGEN_SYCL_DISABLE_SCALAR
+
+/*!
+ * \brief GeneralScalarContraction is a template class that provides the scalar value of Tensor -Tensor contraction
+ * operation, when all the dimensions are contracting dimensions. This Kernel reduces two tensors to an scalar
+ *
+ * \tparam OutScalar: determines the output scalar type
+ *
+ * \tparam LhsScalar: determines the left-hand-side scalar type
+ *
+ * \tparam RhsScalar: determines the right-hand-side scalar type
+ *
+ * \tparam OutAccessor: determines the sycl accessor type for out put (please see the sycl-1.2.1 specification
+ * (https://www.khronos.org/registry/SYCL/specs/sycl-1.2.1.pdf) for accessor definition)
+ *
+ * \tparam LhsMapper: determines the tensor contraction mapper type for left-hand-side matrix
+ *
+ * \tparam RhsMapper: determines the tensor contraction mapper type for right-hand-side matrix
+ *
+ * \tparam StorageIndex: determines the StorageIndex Type
+ *
+ * \tparam Vectorizable: determines whether or not the vectorization is enabled for the Eigen expression.
+ *
+ * \param scratch: local memory containing tiles of LHS and RHS tensors for each work-group
+ *
+ * \param lhs: determines the left-hand-side flattened tensor (tensor mapper)
+ *
+ * \param rhs: determines the right-hand-side flattened tensor (tensor mapper)
+ *
+ * \param out_res: determines the output tensor containing the contraction result
+ *
+ * \param rng: determins the total input data size
+ */
+template <typename OutScalar, typename LhsScalar, typename RhsScalar, typename OutAccessor, typename LhsMapper,
+          typename RhsMapper, typename StorageIndex, bool Vectorizable>
+struct GeneralScalarContraction {
+  typedef sycl::accessor<OutScalar, 1, sycl::access::mode::read_write, sycl::access::target::local> Scratch;
+  Scratch scratch;
+  const LhsMapper lhs;
+  const RhsMapper rhs;
+  OutAccessor out_res;
+  const StorageIndex rng;
+
+  EIGEN_DEVICE_FUNC
+  GeneralScalarContraction(Scratch scratch_, const LhsMapper lhs_, const RhsMapper rhs_, OutAccessor out_res_,
+                           const StorageIndex rng_)
+      : scratch(scratch_), lhs(lhs_), rhs(rhs_), out_res(out_res_), rng(rng_) {}
+
+  EIGEN_DEVICE_FUNC void operator()(sycl::nd_item<1> itemID) {
+    auto out_ptr = out_res.get_pointer();
+    auto scratch_ptr = scratch.get_pointer().get();
+
+    StorageIndex globalid = itemID.get_global_id(0);
+    StorageIndex localid = itemID.get_local_id(0);
+    OutScalar accumulator = OutScalar(0);
+    for (StorageIndex i = globalid; i < rng; i += itemID.get_global_range(0)) {
+      accumulator = sycl::mad(lhs(0, i), rhs(i, 0), accumulator);
+    }
+    auto out_scratch_ptr = scratch_ptr + localid;
+    *out_scratch_ptr = accumulator;
+    for (StorageIndex offset = itemID.get_local_range(0) >> 1; offset > 0; offset >>= 1) {
+      sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+      if (localid < offset) {
+        *out_scratch_ptr = (accumulator += out_scratch_ptr[offset]);
+      }
+    }
+    if (localid == 0) {
+      out_ptr[itemID.get_group(0)] = accumulator;
+    }
+  }
+};
+#endif
+
+}  // namespace internal
+
+template <typename Indices, typename LeftArgType, typename RightArgType, typename OutputKernelType>
+struct TensorEvaluator<const TensorContractionOp<Indices, LeftArgType, RightArgType, OutputKernelType>,
+                       Eigen::GpuDevice>
+    : public TensorContractionEvaluatorBase<TensorEvaluator<
+          const TensorContractionOp<Indices, LeftArgType, RightArgType, OutputKernelType>, Eigen::GpuDevice>> {
+  static_assert(std::is_same<OutputKernelType, const NoOpOutputKernel>::value,
+                "SYCL tensor contraction does not support output kernels.");
+
+  typedef Eigen::GpuDevice Device;
+
+  typedef TensorEvaluator<const TensorContractionOp<Indices, LeftArgType, RightArgType, OutputKernelType>, Device> Self;
+  typedef TensorContractionEvaluatorBase<Self> Base;
+  typedef TensorContractionOp<Indices, LeftArgType, RightArgType, OutputKernelType> XprType;
+  typedef typename internal::remove_const<typename XprType::Scalar>::type Scalar;
+  typedef typename XprType::Index StorageIndex;
+  typedef typename XprType::CoeffReturnType CoeffReturnType;
+  typedef typename PacketType<CoeffReturnType, Device>::type PacketReturnType;
+  typedef typename Base::Storage Storage;
+  typedef typename Base::EvaluatorPointerType EvaluatorPointerType;
+  struct TripleDim {
+    const StorageIndex M;
+    const StorageIndex N;
+    const StorageIndex K;
+    TripleDim(const StorageIndex M_, const StorageIndex N_, const StorageIndex K_) : M(M_), N(N_), K(K_) {}
+  };
+  enum {
+    Layout = TensorEvaluator<LeftArgType, Device>::Layout,
+    PacketAccess = (PacketType<CoeffReturnType, Device>::size > 1),
+    BlockAccess = false,
+  };
+
+  static EIGEN_CONSTEXPR int LDims = Base::LDims;
+  static EIGEN_CONSTEXPR int RDims = Base::RDims;
+  static EIGEN_CONSTEXPR int ContractDims = Base::ContractDims;
+
+  typedef array<StorageIndex, LDims> left_dim_mapper_t;
+  typedef array<StorageIndex, RDims> right_dim_mapper_t;
+
+  typedef array<StorageIndex, ContractDims> contract_t;
+  typedef array<StorageIndex, LDims - ContractDims> left_nocontract_t;
+  typedef array<StorageIndex, RDims - ContractDims> right_nocontract_t;
+
+  static const int NumDims = LDims + RDims - 2 * ContractDims;
+
+  typedef DSizes<StorageIndex, NumDims> Dimensions;
+
+  typedef TensorEvaluator<typename Base::EvalLeftArgType, Device> LeftEvaluator;
+  typedef TensorEvaluator<typename Base::EvalRightArgType, Device> RightEvaluator;
+  typedef typename Eigen::internal::remove_const<typename LeftEvaluator::CoeffReturnType>::type LhsScalar;
+  typedef typename Eigen::internal::remove_const<typename RightEvaluator::CoeffReturnType>::type RhsScalar;
+
+  typedef typename LeftEvaluator::Dimensions LeftDimensions;
+  typedef typename RightEvaluator::Dimensions RightDimensions;
+
+  template <bool lhs_inner_dim_contiguous, bool rhs_inner_dim_contiguous, bool rhs_inner_dim_reordered>
+  struct input_mapper_propertis {
+    static EIGEN_CONSTEXPR bool is_lhs_matrix = (LDims == 2 && ContractDims == 1) || lhs_inner_dim_contiguous;
+    static EIGEN_CONSTEXPR bool is_rhs_matrix =
+        (RDims == 2 && ContractDims == 1) || (rhs_inner_dim_contiguous && !rhs_inner_dim_reordered);
+  };
+
+  EIGEN_DEVICE_FUNC TensorEvaluator(const XprType &op, const Device &device) : Base(op, device) {}
+
+  // We need to redefine this method to make nvcc happy
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool evalSubExprsIfNeeded(typename Base::EvaluatorPointerType data) {
+    this->m_leftImpl.evalSubExprsIfNeeded(NULL);
+    this->m_rightImpl.evalSubExprsIfNeeded(NULL);
+    if (!data) {
+      this->m_result = this->m_device.get(
+          static_cast<Scalar *>(this->m_device.allocate_temp(this->dimensions().TotalSize() * sizeof(Scalar))));
+      data = this->m_result;
+    }
+    evalToSycl(data);
+    return (this->m_result != NULL);
+  }
+  const Eigen::GpuDevice &device() const { return this->m_device; }
+  void evalToSycl(typename Base::EvaluatorPointerType buffer) const {
+    if (this->m_lhs_inner_dim_contiguous) {
+      if (this->m_rhs_inner_dim_contiguous) {
+        if (this->m_rhs_inner_dim_reordered) {
+          evalTyped<true, true, true, Unaligned>(buffer);
+        } else {
+          evalTyped<true, true, false, Unaligned>(buffer);
+        }
+      } else {
+        if (this->m_rhs_inner_dim_reordered) {
+          evalTyped<true, false, true, Unaligned>(buffer);
+        } else {
+          evalTyped<true, false, false, Unaligned>(buffer);
+        }
+      }
+    } else {
+      if (this->m_rhs_inner_dim_contiguous) {
+        if (this->m_rhs_inner_dim_reordered) {
+          evalTyped<false, true, true, Unaligned>(buffer);
+        } else {
+          evalTyped<false, true, false, Unaligned>(buffer);
+        }
+      } else {
+        if (this->m_rhs_inner_dim_reordered) {
+          evalTyped<false, false, true, Unaligned>(buffer);
+        } else {
+          evalTyped<false, false, false, Unaligned>(buffer);
+        }
+      }
+    }
+  }
+
+  template <bool lhs_inner_dim_contiguous, bool rhs_inner_dim_contiguous, bool rhs_inner_dim_reordered, int Alignment>
+  void evalTyped(typename Base::EvaluatorPointerType buffer) const {
+    const auto triple_dim = TripleDim{this->m_i_size, this->m_j_size, this->m_k_size};
+    typedef internal::TensorContractionInputMapper<
+        LhsScalar, StorageIndex, internal::Lhs, LeftEvaluator, left_nocontract_t, contract_t,
+        PacketType<CoeffReturnType, Device>::size, lhs_inner_dim_contiguous, false, Unaligned, MakePointer>
+        LhsMapper;
+
+    typedef internal::TensorContractionInputMapper<RhsScalar, StorageIndex, internal::Rhs, RightEvaluator,
+                                                   right_nocontract_t, contract_t,
+                                                   PacketType<CoeffReturnType, Device>::size, rhs_inner_dim_contiguous,
+                                                   rhs_inner_dim_reordered, Unaligned, MakePointer>
+        RhsMapper;
+
+    // initialize data mappers
+    LhsMapper lhs(this->m_leftImpl, this->m_left_nocontract_strides, this->m_i_strides,
+                  this->m_left_contracting_strides, this->m_k_strides);
+
+    RhsMapper rhs(this->m_rightImpl, this->m_right_nocontract_strides, this->m_j_strides,
+                  this->m_right_contracting_strides, this->m_k_strides);
+
+#ifndef EIGEN_SYCL_DISABLE_SCALAR
+    if (triple_dim.M == 1 && triple_dim.N == 1) {
+      launchSC(buffer, lhs, rhs, triple_dim.K);
+    } else
+#endif
+#ifndef EIGEN_SYCL_DISABLE_GEMV
+        if (triple_dim.M != 1 && triple_dim.N == 1) {
+      LaunchVT<false>(buffer, rhs, lhs, triple_dim.M, triple_dim.K);
+    } else if (triple_dim.M == 1 && triple_dim.N != 1) {
+      LaunchVT<true>(buffer, lhs, rhs, triple_dim.N, triple_dim.K);
+    } else  // This is equivalent of if (m!=1 && n!=1)
+#endif
+    {
+      typedef input_mapper_propertis<lhs_inner_dim_contiguous, rhs_inner_dim_contiguous, rhs_inner_dim_reordered>
+          inpt_mapper_properties;
+        adjustTT<false, inpt_mapper_properties>(buffer, lhs, rhs, triple_dim);
+    }
+  }
+
+  template <bool skinny, typename input_mapper_properties, typename LhsMapper, typename RhsMapper>
+  void EIGEN_ALWAYS_INLINE adjustTT(EvaluatorPointerType buffer, const LhsMapper &lhs, const RhsMapper &rhs,
+                                    const TripleDim &triple_dim) const {
+#ifdef EIGEN_SYCL_LOCAL_MEM_UNSET_OR_ON
+    if (device().has_local_memory()) {
+      typedef internal::TTPanelSize<CoeffReturnType, StorageIndex, 4, 4, 16> PanelParameters;
+      launchTT<internal::contraction_type::local, skinny, input_mapper_properties, PanelParameters>(
+          buffer, lhs, rhs, triple_dim);
+    }
+#endif
+#ifdef EIGEN_SYCL_LOCAL_MEM_UNSET_OR_OFF
+    if (!(device().has_local_memory())) {
+      typedef internal::TTPanelSize<CoeffReturnType, StorageIndex, 4, 4, 4> PanelParameters;
+      launchTT<internal::contraction_type::no_local, skinny, input_mapper_properties, PanelParameters>(
+          buffer, lhs, rhs, triple_dim);
+    }
+#endif
+  }
+
+  template <internal::contraction_type ct, bool skinny, typename input_mapper_properties,
+            typename Properties, typename LhsMapper, typename RhsMapper>
+  void launchTT(EvaluatorPointerType buffer, const LhsMapper &lhs, const RhsMapper &rhs,
+                const TripleDim &triple_dim) const {
+    const StorageIndex roundUpM = Eigen::internal::roundUp(triple_dim.M, Properties::TileSizeDimM);
+    const StorageIndex roundUpN = Eigen::internal::roundUp(triple_dim.N, Properties::TileSizeDimN);
+    const StorageIndex groupSizeM = roundUpM / Properties::TileSizeDimM;
+    const StorageIndex groupSizeN = roundUpN / Properties::TileSizeDimN;
+
+    const StorageIndex roundUpK = Eigen::internal::roundUp(triple_dim.K, Properties::TileSizeDimK);
+    StorageIndex totalTilesK = roundUpK / Properties::TileSizeDimK;
+    StorageIndex groupSizeK =
+        skinny
+            ? std::max(std::min(totalTilesK,
+                                (StorageIndex)(device().getPowerOfTwo(device().getNumSyclMultiProcessors(), true) * 4) /
+                                    (groupSizeM * groupSizeN)),
+                       StorageIndex(1))
+            : StorageIndex(1);
+
+    const StorageIndex numTilesPerGroup = Eigen::internal::roundUp(totalTilesK, groupSizeK) / groupSizeK;
+
+    const StorageIndex totalGroupSize = groupSizeM * groupSizeN * groupSizeK;
+
+    const StorageIndex localRange = Properties::LocalThreadSizeM * Properties::LocalThreadSizeN;
+    const StorageIndex globalRange = totalGroupSize * localRange;
+
+    const StorageIndex scratchSize = (ct == internal::contraction_type::local)
+                                         ? ((Properties::DoubleBuffer + 1) *
+                                            (Properties::TileSizeDimM + Properties::BC) * (Properties::TileSizeDimK)) +
+                                               ((Properties::DoubleBuffer + 1) * (Properties::TileSizeDimK) *
+                                                (Properties::TileSizeDimN + Properties::BC))
+                                         : StorageIndex(1);
+
+    auto thread_range = sycl::nd_range<1>(sycl::range<1>(globalRange), sycl::range<1>(localRange));
+    if (groupSizeK == 1) {
+      typedef internal::TensorContractionKernelDpcpp<CoeffReturnType, LhsScalar, RhsScalar, EvaluatorPointerType,
+                                                            LhsMapper, RhsMapper, StorageIndex, Properties, TripleDim,
+                                                            PacketAccess, input_mapper_properties, true, ct>
+          ContractKernelName;
+      device().template binary_kernel_launcher<CoeffReturnType, ContractKernelName>(
+          lhs, rhs, buffer, thread_range, scratchSize, groupSizeM, groupSizeN, numTilesPerGroup, triple_dim);
+    } else {
+      typedef internal::TensorContractionKernelDpcpp<CoeffReturnType, LhsScalar, RhsScalar, EvaluatorPointerType,
+                                                            LhsMapper, RhsMapper, StorageIndex, Properties, TripleDim,
+                                                            PacketAccess, input_mapper_properties, false, ct>
+          ContractKernelName;
+      CoeffReturnType *temp_pointer = static_cast<CoeffReturnType *>(
+          device().allocate_temp(triple_dim.M * triple_dim.N * groupSizeK * sizeof(CoeffReturnType)));
+      EvaluatorPointerType tmp_global_accessor = device().get(temp_pointer);
+
+      device().template binary_kernel_launcher<CoeffReturnType, ContractKernelName>(
+          lhs, rhs, tmp_global_accessor, thread_range, scratchSize, groupSizeM, groupSizeN, numTilesPerGroup,
+          triple_dim);
+
+      typedef Eigen::internal::SumReducer<CoeffReturnType> Op;
+      auto op = Op();
+      typedef internal::SecondStepPartialReduction<CoeffReturnType, StorageIndex, EvaluatorPointerType,
+                                                               EvaluatorPointerType, Op>
+          ReductionKernel;
+
+      device().template unary_kernel_launcher<CoeffReturnType, ReductionKernel>(
+          tmp_global_accessor, buffer,
+          sycl::nd_range<1>(sycl::range<1>(StorageIndex(
+                                    Eigen::internal::roundUp(triple_dim.M * triple_dim.N, localRange))),
+                                sycl::range<1>(localRange)),
+          StorageIndex(1), op, StorageIndex(triple_dim.M * triple_dim.N), groupSizeK);
+
+      device().deallocate_temp(temp_pointer);
+    }
+  }
+
+#ifndef EIGEN_SYCL_DISABLE_GEMV
+  template <bool is_lhs_vec, typename VectorMapper, typename TensorMapper, typename StorageIndex>
+  void EIGEN_ALWAYS_INLINE LaunchVT(EvaluatorPointerType buffer, const VectorMapper &vec, const TensorMapper &mat,
+                                    StorageIndex NC, StorageIndex C) const {
+    const StorageIndex nonContractDim = NC;
+    EIGEN_CONSTEXPR StorageIndex NCFactor = 1;
+    EIGEN_CONSTEXPR StorageIndex CFactor = 1;
+    EIGEN_CONSTEXPR StorageIndex NCWindow = 16;
+    typedef Eigen::internal::TVPanelSize<CoeffReturnType, StorageIndex, NCWindow, CFactor, NCFactor>
+        Properties;
+    const StorageIndex roundUpC = Eigen::internal::roundUp(C, Properties::TileSizeDimC);
+    const StorageIndex cNumGroups = roundUpC / (Properties::LocalThreadSizeC * Properties::WorkLoadPerThreadC);
+    const StorageIndex roundUpNC = Eigen::internal::roundUp(nonContractDim, Properties::TileSizeDimNC);
+    const StorageIndex nCNumGroups = roundUpNC / (Properties::LocalThreadSizeNC * Properties::WorkLoadPerThreadNC);
+    const StorageIndex globalRange =
+        (roundUpNC / (Properties::WorkLoadPerThreadNC)) * (roundUpC / (Properties::WorkLoadPerThreadC));
+    const StorageIndex localRange = Properties::LocalThreadSizeNC * Properties::LocalThreadSizeC;
+    const StorageIndex scratchSize =
+        (Properties::WorkLoadPerThreadNC + CFactor) * Properties::LocalThreadSizeC * Properties::LocalThreadSizeNC;
+    auto thread_range = sycl::nd_range<1>(sycl::range<1>(globalRange), sycl::range<1>(localRange));
+    if (cNumGroups > 1) {
+      typedef Eigen::internal::GeneralVectorTensor<CoeffReturnType, EvaluatorPointerType, VectorMapper,
+                                                               TensorMapper, StorageIndex, Properties, CFactor, false,
+                                                               is_lhs_vec, false>
+          ContractKernelName;
+      CoeffReturnType *temp_pointer =
+          static_cast<CoeffReturnType *>(device().allocate_temp(nonContractDim * cNumGroups * sizeof(CoeffReturnType)));
+      EvaluatorPointerType tmp_global_accessor = device().get(temp_pointer);
+
+      device().template binary_kernel_launcher<CoeffReturnType, ContractKernelName>(
+          vec, mat, tmp_global_accessor, thread_range, scratchSize, nCNumGroups, nonContractDim, C);
+
+      typedef Eigen::internal::SumReducer<CoeffReturnType> Op;
+      typedef internal::SecondStepPartialReduction<CoeffReturnType, StorageIndex, EvaluatorPointerType,
+                                                               EvaluatorPointerType, Op>
+          ReductionKernel;
+
+      device().template unary_kernel_launcher<CoeffReturnType, ReductionKernel>(
+          tmp_global_accessor, buffer,
+          sycl::nd_range<1>(sycl::range<1>(Eigen::internal::roundUp(nonContractDim, localRange)),
+                                sycl::range<1>(localRange)),
+          StorageIndex(1), Op(), nonContractDim, cNumGroups);
+
+      device().deallocate_temp(temp_pointer);
+    } else {
+      typedef Eigen::internal::GeneralVectorTensor<CoeffReturnType, EvaluatorPointerType, VectorMapper,
+                                                               TensorMapper, StorageIndex, Properties, CFactor, false,
+                                                               is_lhs_vec, true>
+          ContractKernelName;
+      device().template binary_kernel_launcher<CoeffReturnType, ContractKernelName>(
+          vec, mat, buffer, thread_range, scratchSize, nCNumGroups, nonContractDim, C);
+    }
+  }
+#endif
+
+#ifndef EIGEN_SYCL_DISABLE_SCALAR
+  template <typename LhsMapper, typename RhsMapper>
+  EIGEN_ALWAYS_INLINE void launchSC(EvaluatorPointerType buffer, const LhsMapper &lhs, const RhsMapper &rhs,
+                                    StorageIndex K) const {
+    EIGEN_STATIC_ASSERT(!((EIGEN_SYCL_LOCAL_THREAD_DIM0 * EIGEN_SYCL_LOCAL_THREAD_DIM1) &
+                          (EIGEN_SYCL_LOCAL_THREAD_DIM0 * EIGEN_SYCL_LOCAL_THREAD_DIM1 - 1)),
+                        "The Local thread size must be a power of 2 for the reduction "
+                        "operation");
+    EIGEN_CONSTEXPR StorageIndex local_range = EIGEN_SYCL_LOCAL_THREAD_DIM0 * EIGEN_SYCL_LOCAL_THREAD_DIM1;
+
+    // Here we force the code not to be more than 2-step reduction: Our empirical research shows that if each thread
+    // reduces at least 512 elementss individually, we get better performance.
+    const StorageIndex num_work_group = ((K + (512 * local_range - 1)) / (512 * local_range) > 1 ? local_range : 1);
+    const StorageIndex global_range = num_work_group * local_range;
+
+    typedef Eigen::internal::GeneralScalarContraction<
+        CoeffReturnType, LhsScalar, RhsScalar, EvaluatorPointerType, LhsMapper, RhsMapper, StorageIndex, false>
+        ContractKernelName;
+    auto thread_range = sycl::nd_range<1>(sycl::range<1>(global_range), sycl::range<1>(local_range));
+    if (num_work_group > 1) {
+      CoeffReturnType *temp_pointer =
+          static_cast<CoeffReturnType *>(device().allocate_temp(num_work_group * sizeof(CoeffReturnType)));
+      EvaluatorPointerType tmp_global_accessor = device().get(temp_pointer);
+      device().template binary_kernel_launcher<CoeffReturnType, ContractKernelName>(lhs, rhs, tmp_global_accessor,
+                                                                                    thread_range, local_range, K);
+      typedef Eigen::internal::SumReducer<CoeffReturnType> Op;
+      typedef internal::SecondStepFullReducer<CoeffReturnType, Op, EvaluatorPointerType,
+                                                          EvaluatorPointerType, StorageIndex>
+          GenericRKernel;
+      device().template unary_kernel_launcher<CoeffReturnType, GenericRKernel>(
+          tmp_global_accessor, buffer,
+          sycl::nd_range<1>(sycl::range<1>(local_range), sycl::range<1>(local_range)), local_range, Op());
+
+      device().deallocate_temp(temp_pointer);
+    } else {
+      device().template binary_kernel_launcher<CoeffReturnType, ContractKernelName>(lhs, rhs, buffer, thread_range,
+                                                                                    local_range, K);
+    }
+  }
+#endif
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void cleanup() {
+    this->m_leftImpl.cleanup();
+    this->m_rightImpl.cleanup();
+
+    if (this->m_result) {
+      this->m_device.deallocate_temp(this->m_result);
+      this->m_result = nullptr;
+    }
+  }
+  // The placeholder accessors must bound to a command group handler for SYCL
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
+    this->m_leftImpl.bind(cgh);
+    this->m_rightImpl.bind(cgh);
+    this->m_result.bind(cgh);
+  }
+};
+}  // namespace Eigen
+#endif  // EIGEN_CXX11_TENSOR_TENSOR_CONTRACTION_DPCPP_H
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h b/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h
index 9ab900b4a..b9eb65b55 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h
@@ -61,7 +61,7 @@ struct CoeffLoader {
 
   #ifdef EIGEN_USE_SYCL
   // The placeholder accessors require to be bound to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_tensor.bind(cgh);
   }
   #endif
@@ -97,7 +97,7 @@ struct CoeffLoader<Tensor, true, MakePointer_> {
 
   #ifdef EIGEN_USE_SYCL
   // The placeholder accessors require to be bound to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_data.bind(cgh);
   }
   #endif
@@ -250,7 +250,7 @@ class SimpleTensorContractionMapper {
 
   #ifdef EIGEN_USE_SYCL
   // The placeholder accessors require to be bound to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_tensor.bind(cgh);
   }
   #endif
@@ -500,7 +500,7 @@ class TensorContractionSubMapper {
 
   #ifdef EIGEN_USE_SYCL
   // The placeholder accessors require to be bound to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_base_mapper.bind(cgh);
   }
   #endif
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorContractionSycl.h b/unsupported/Eigen/CXX11/src/Tensor/TensorContractionSycl.h
index a6ca1777a..e0287899d 100755
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorContractionSycl.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorContractionSycl.h
@@ -472,9 +472,9 @@ class TensorContractionKernel {
   static EIGEN_CONSTEXPR StorageIndex NStride =
       contraction_tp == contraction_type::local ? Properties::WorkLoadPerThreadN : RHSBlockProperties::nc_stride;
 
-  typedef cl::sycl::accessor<OutScalar, 1, cl::sycl::access::mode::read_write, cl::sycl::access::target::local> Scratch;
-  typedef cl::sycl::multi_ptr<OutScalar, cl::sycl::access::address_space::local_space> local_ptr;
-  typedef OutScalar * /*cl::sycl::multi_ptr<OutScalar, cl::sycl::access::address_space::private_space>*/ private_ptr;
+  typedef sycl::accessor<OutScalar, 1, sycl::access::mode::read_write, sycl::access::target::local> Scratch;
+  typedef sycl::multi_ptr<OutScalar, sycl::access::address_space::local_space> local_ptr;
+  typedef OutScalar * /*sycl::multi_ptr<OutScalar, sycl::access::address_space::private_space>*/ private_ptr;
   typedef
       typename ::Eigen::internal::conditional<contraction_tp == contraction_type::local, local_ptr, private_ptr>::type
           tile_ptr;
@@ -596,7 +596,7 @@ class TensorContractionKernel {
                                                                 const TripleDim triple_dim_)
       : TensorContractionKernel(scratch_, lhs_, rhs_, out_res_, groupSizeM_, 1, numTiles_, triple_dim_) {}
 
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(cl::sycl::nd_item<1> itemID) {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(sycl::nd_item<1> itemID) {
     const StorageIndex linearLocalThreadId = itemID.get_local_id(0);
     const StorageIndex nLocalThreadId = linearLocalThreadId / Properties::LocalThreadSizeM;
     const StorageIndex mLocalThreadId = linearLocalThreadId % Properties::LocalThreadSizeM;
@@ -784,34 +784,34 @@ class TensorContractionKernel {
   template <bool db = Properties::DoubleBuffer, contraction_type ctp = contraction_tp>
   static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
       typename ::Eigen::internal::enable_if<db && ctp == contraction_type::local>::type
-      sync_mem(const cl::sycl::nd_item<1> &, bool &db_offset) noexcept {
+      sync_mem(const sycl::nd_item<1> &, bool &db_offset) noexcept {
     db_offset = !db_offset;
   }
 
   template <bool db = Properties::DoubleBuffer, contraction_type ctp = contraction_tp>
   static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
       typename ::Eigen::internal::enable_if<!db && ctp == contraction_type::local>::type
-      sync_mem(const cl::sycl::nd_item<1> &itemID, bool &) noexcept {
-    itemID.barrier(cl::sycl::access::fence_space::local_space);
+      sync_mem(const sycl::nd_item<1> &itemID, bool &) noexcept {
+    itemID.barrier(sycl::access::fence_space::local_space);
   }
 
   template <contraction_type ctp = contraction_tp>
   static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
       typename ::Eigen::internal::enable_if<ctp == contraction_type::no_local>::type
-      sync_mem(const cl::sycl::nd_item<1> &, bool &) noexcept {
+      sync_mem(const sycl::nd_item<1> &, bool &) noexcept {
     return;
   }
 
   template <bool need_sync, contraction_type ctp = contraction_tp>
   static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
       typename ::Eigen::internal::enable_if<need_sync && ctp == contraction_type::no_local>::type
-      sync_thread(const cl::sycl::nd_item<1> &
+      sync_thread(const sycl::nd_item<1> &
 #ifdef EIGEN_SYCL_ARM_GPU_CACHE_OPTIMISATION
                       itemID
 #endif
                   ) noexcept {
 #ifdef EIGEN_SYCL_ARM_GPU_CACHE_OPTIMISATION
-    itemID.barrier(cl::sycl::access::fence_spacce::local_space);
+    itemID.barrier(sycl::access::fence_spacce::local_space);
 #else
     return;
 #endif
@@ -819,17 +819,17 @@ class TensorContractionKernel {
   template <bool need_sync, contraction_type ctp = contraction_tp>
   static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
       typename ::Eigen::internal::enable_if<need_sync && ctp == contraction_type::local>::type
-      sync_thread(const cl::sycl::nd_item<1> &itemID) {
-    itemID.barrier(cl::sycl::access::fence_space::local_space);
+      sync_thread(const sycl::nd_item<1> &itemID) {
+    itemID.barrier(sycl::access::fence_space::local_space);
   }
   template <bool need_sync>
   static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename ::Eigen::internal::enable_if<!need_sync>::type sync_thread(
-      const cl::sycl::nd_item<1> &) {
+      const sycl::nd_item<1> &) {
     return;
   }
 
   template <bool is_internal_block>
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void compute_tile_per_panel(const cl::sycl::nd_item<1> &itemID,
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void compute_tile_per_panel(const sycl::nd_item<1> &itemID,
                                                                     ThreadProperties<StorageIndex> &thread_properties,
                                                                     TiledMemory &tiled_input_block,
                                                                     PacketReturnType *privateRes, bool &db_offset) {
@@ -849,7 +849,7 @@ class TensorContractionKernel {
         contraction_tp == contraction_type::local ? thread_properties.mGroupOffset : thread_properties.mGlobalOffset,
         thread_properties.kGroupOffset - thread_properties.kSize);
 
-    // itemID.barrier(cl::sycl::access::fence_space::local_space);
+    // itemID.barrier(sycl::access::fence_space::local_space);
     sync_thread<contraction_tp == contraction_type::local>(itemID);
     // switch to compute mede
     StorageIndex lhs_offset = (db_offset * LSDL * Properties::TileSizeDimK);
@@ -868,7 +868,7 @@ class TensorContractionKernel {
 
   // when local memory is available the following compute_panel will be enabled
   template <bool is_internal_block, typename OutPtr>
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void compute_panel(const cl::sycl::nd_item<1> &itemID,
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void compute_panel(const sycl::nd_item<1> &itemID,
                                                            ThreadProperties<StorageIndex> &thread_properties,
                                                            OutPtr out_ptr) {
     auto tiled_input_block = TiledMemory{thread_properties, scratch.get_pointer()};
@@ -1003,7 +1003,7 @@ struct GeneralVectorTensor {
       PacketReturnType;
   static EIGEN_CONSTEXPR int PacketSize =
       Eigen::TensorSycl::internal::Vectorise<OutScalar, Eigen::SyclDevice, Vectorizable>::PacketSize;
-  typedef cl::sycl::accessor<OutScalar, 1, cl::sycl::access::mode::read_write, cl::sycl::access::target::local> Scratch;
+  typedef sycl::accessor<OutScalar, 1, sycl::access::mode::read_write, sycl::access::target::local> Scratch;
 
   static EIGEN_CONSTEXPR StorageIndex OutScratchOffset =
       KFactor * Properties::LocalThreadSizeC * Properties::LocalThreadSizeNC;
@@ -1034,7 +1034,7 @@ struct GeneralVectorTensor {
         nonContractDim(nonContractDim_),
         contractDim(contractDim_) {}
 
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(cl::sycl::nd_item<1> itemID) {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(sycl::nd_item<1> itemID) {
     auto scratch_ptr = scratch.get_pointer();
     const StorageIndex linearLocalThreadId = itemID.get_local_id(0);
     StorageIndex nonContractId = is_lhs_vec ? linearLocalThreadId / Properties::LocalThreadSizeC
@@ -1072,7 +1072,7 @@ struct GeneralVectorTensor {
   }
   template <bool is_internal_block, typename OutPtr>
   static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void compute_panel(
-      const cl::sycl::nd_item<1> &itemID, const VectorMapper &vec, const TensorMapper &mat, OutScalar *local_output,
+      const sycl::nd_item<1> &itemID, const VectorMapper &vec, const TensorMapper &mat, OutScalar *local_output,
       OutPtr out_ptr,
 #ifdef EIGEN_SYCL_LOCAL_MEM_UNSET_OR_ON
       OutScalar *scratch_ptr, const StorageIndex contractGroupOffset,
@@ -1088,7 +1088,7 @@ struct GeneralVectorTensor {
                   Properties::LocalThreadSizeNC * Properties::LocalThreadSizeC>(vec, scratch_ptr, linearLocalThreadId,
                                                                                 vectorOffset, contractDim);
 
-    itemID.barrier(cl::sycl::access::fence_space::local_space);
+    itemID.barrier(sycl::access::fence_space::local_space);
     auto in_scratch_ptr = scratch_ptr + contractId;
 #endif
 
@@ -1115,7 +1115,7 @@ struct GeneralVectorTensor {
                                               : globalContractDimOffset + privateOffsetC)
                              : OutScalar(0);
 
-        outScalar[j] = cl::sycl::mad(matScalar, vecScalar, outScalar[j]);
+        outScalar[j] = sycl::mad(matScalar, vecScalar, outScalar[j]);
         privateOffsetNC += Properties::LocalThreadSizeNC;
       }
       privateOffsetC += Properties::LocalThreadSizeC;
@@ -1143,7 +1143,7 @@ struct GeneralVectorTensor {
     for (StorageIndex j = 0; j < Properties::WorkLoadPerThreadNC; j++) {
       EIGEN_UNROLL_LOOP
       for (StorageIndex offset = Properties::LocalThreadSizeC >> 1; offset > 0; offset >>= 1) {
-        itemID.barrier(cl::sycl::access::fence_space::local_space);
+        itemID.barrier(sycl::access::fence_space::local_space);
         if (contractId < offset) {
           StorageIndex myNeigbourId = (Properties::LocalThreadSizeNC * offset);
           *out_scratch_ptr += out_scratch_ptr[myNeigbourId];
@@ -1239,7 +1239,7 @@ struct GeneralVectorTensor {
 template <typename OutScalar, typename LhsScalar, typename RhsScalar, typename OutAccessor, typename LhsMapper,
           typename RhsMapper, typename StorageIndex, bool Vectorizable>
 struct GeneralScalarContraction {
-  typedef cl::sycl::accessor<OutScalar, 1, cl::sycl::access::mode::read_write, cl::sycl::access::target::local> Scratch;
+  typedef sycl::accessor<OutScalar, 1, sycl::access::mode::read_write, sycl::access::target::local> Scratch;
   Scratch scratch;
   const LhsMapper lhs;
   const RhsMapper rhs;
@@ -1251,7 +1251,7 @@ struct GeneralScalarContraction {
                            const StorageIndex rng_)
       : scratch(scratch_), lhs(lhs_), rhs(rhs_), out_res(out_res_), rng(rng_) {}
 
-  EIGEN_DEVICE_FUNC void operator()(cl::sycl::nd_item<1> itemID) {
+  EIGEN_DEVICE_FUNC void operator()(sycl::nd_item<1> itemID) {
     auto out_ptr = out_res.get_pointer();
     auto scratch_ptr = scratch.get_pointer().get();
 
@@ -1259,12 +1259,12 @@ struct GeneralScalarContraction {
     StorageIndex localid = itemID.get_local_id(0);
     OutScalar accumulator = OutScalar(0);
     for (StorageIndex i = globalid; i < rng; i += itemID.get_global_range(0)) {
-      accumulator = cl::sycl::mad(lhs(0, i), rhs(i, 0), accumulator);
+      accumulator = sycl::mad(lhs(0, i), rhs(i, 0), accumulator);
     }
     auto out_scratch_ptr = scratch_ptr + localid;
     *out_scratch_ptr = accumulator;
     for (StorageIndex offset = itemID.get_local_range(0) >> 1; offset > 0; offset >>= 1) {
-      itemID.barrier(cl::sycl::access::fence_space::local_space);
+      itemID.barrier(sycl::access::fence_space::local_space);
       if (localid < offset) {
         *out_scratch_ptr = (accumulator += out_scratch_ptr[offset]);
       }
@@ -1496,7 +1496,7 @@ struct TensorEvaluator<const TensorContractionOp<Indices, LeftArgType, RightArgT
                                                 (Properties::TileSizeDimN + Properties::BC))
                                          : StorageIndex(1);
 
-    auto thread_range = cl::sycl::nd_range<1>(cl::sycl::range<1>(globalRange), cl::sycl::range<1>(localRange));
+    auto thread_range = sycl::nd_range<1>(sycl::range<1>(globalRange), sycl::range<1>(localRange));
     if (groupSizeK == 1) {
       typedef TensorSycl::internal::TensorContractionKernel<CoeffReturnType, LhsScalar, RhsScalar, EvaluatorPointerType,
                                                             LhsMapper, RhsMapper, StorageIndex, Properties, TripleDim,
@@ -1525,9 +1525,9 @@ struct TensorEvaluator<const TensorContractionOp<Indices, LeftArgType, RightArgT
 
       device().template unary_kernel_launcher<CoeffReturnType, ReductionKernel>(
           tmp_global_accessor, buffer,
-          cl::sycl::nd_range<1>(cl::sycl::range<1>(StorageIndex(
+          sycl::nd_range<1>(sycl::range<1>(StorageIndex(
                                     Eigen::TensorSycl::internal::roundUp(triple_dim.M * triple_dim.N, localRange))),
-                                cl::sycl::range<1>(localRange)),
+                                sycl::range<1>(localRange)),
           StorageIndex(1), op, StorageIndex(triple_dim.M * triple_dim.N), groupSizeK);
 
       device().deallocate_temp(temp_pointer);
@@ -1553,7 +1553,7 @@ struct TensorEvaluator<const TensorContractionOp<Indices, LeftArgType, RightArgT
     const StorageIndex localRange = Properties::LocalThreadSizeNC * Properties::LocalThreadSizeC;
     const StorageIndex scratchSize =
         (Properties::WorkLoadPerThreadNC + CFactor) * Properties::LocalThreadSizeC * Properties::LocalThreadSizeNC;
-    auto thread_range = cl::sycl::nd_range<1>(cl::sycl::range<1>(globalRange), cl::sycl::range<1>(localRange));
+    auto thread_range = sycl::nd_range<1>(sycl::range<1>(globalRange), sycl::range<1>(localRange));
     if (cNumGroups > 1) {
       typedef Eigen::TensorSycl::internal::GeneralVectorTensor<CoeffReturnType, EvaluatorPointerType, VectorMapper,
                                                                TensorMapper, StorageIndex, Properties, CFactor, false,
@@ -1573,8 +1573,8 @@ struct TensorEvaluator<const TensorContractionOp<Indices, LeftArgType, RightArgT
 
       device().template unary_kernel_launcher<CoeffReturnType, ReductionKernel>(
           tmp_global_accessor, buffer,
-          cl::sycl::nd_range<1>(cl::sycl::range<1>(Eigen::TensorSycl::internal::roundUp(nonContractDim, localRange)),
-                                cl::sycl::range<1>(localRange)),
+          sycl::nd_range<1>(sycl::range<1>(Eigen::TensorSycl::internal::roundUp(nonContractDim, localRange)),
+                                sycl::range<1>(localRange)),
           StorageIndex(1), Op(), nonContractDim, cNumGroups);
 
       device().deallocate_temp(temp_pointer);
@@ -1607,7 +1607,7 @@ struct TensorEvaluator<const TensorContractionOp<Indices, LeftArgType, RightArgT
     typedef Eigen::TensorSycl::internal::GeneralScalarContraction<
         CoeffReturnType, LhsScalar, RhsScalar, EvaluatorPointerType, LhsMapper, RhsMapper, StorageIndex, false>
         ContractKernelName;
-    auto thread_range = cl::sycl::nd_range<1>(cl::sycl::range<1>(global_range), cl::sycl::range<1>(local_range));
+    auto thread_range = sycl::nd_range<1>(sycl::range<1>(global_range), sycl::range<1>(local_range));
     if (num_work_group > 1) {
       CoeffReturnType *temp_pointer =
           static_cast<CoeffReturnType *>(device().allocate_temp(num_work_group * sizeof(CoeffReturnType)));
@@ -1620,7 +1620,7 @@ struct TensorEvaluator<const TensorContractionOp<Indices, LeftArgType, RightArgT
           GenericRKernel;
       device().template unary_kernel_launcher<CoeffReturnType, GenericRKernel>(
           tmp_global_accessor, buffer,
-          cl::sycl::nd_range<1>(cl::sycl::range<1>(local_range), cl::sycl::range<1>(local_range)), local_range, Op());
+          sycl::nd_range<1>(sycl::range<1>(local_range), sycl::range<1>(local_range)), local_range, Op());
 
       device().deallocate_temp(temp_pointer);
     } else {
@@ -1640,7 +1640,7 @@ struct TensorEvaluator<const TensorContractionOp<Indices, LeftArgType, RightArgT
     }
   }
   // The placeholder accessors must bound to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     this->m_leftImpl.bind(cgh);
     this->m_rightImpl.bind(cgh);
     this->m_result.bind(cgh);
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h b/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h
index 44493906d..7e1972386 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h
@@ -25,6 +25,7 @@ struct traits<TensorConversionOp<TargetType, XprType> >
 {
   // Type promotion to handle the case where the types of the lhs and the rhs are different.
   typedef TargetType Scalar;
+  typedef traits<XprType> XprTraits;
   typedef typename traits<XprType>::StorageKind StorageKind;
   typedef typename traits<XprType>::Index Index;
   typedef typename XprType::Nested Nested;
@@ -325,7 +326,7 @@ struct TensorEvaluator<const TensorConversionOp<TargetType, ArgType>, Device>
   enum {
     IsAligned         = false,
     PacketAccess      =
-    #ifndef EIGEN_USE_SYCL
+    #if !defined(EIGEN_USE_SYCL) && !defined(EIGEN_USE_DPCPP)
                         true,
     #else
                         TensorEvaluator<ArgType, Device>::PacketAccess &
@@ -444,7 +445,7 @@ struct TensorEvaluator<const TensorConversionOp<TargetType, ArgType>, Device>
   const TensorEvaluator<ArgType, Device>& impl() const { return m_impl; }
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h b/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h
index 92003c766..7f9bdc25b 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h
@@ -33,7 +33,7 @@ template <typename Evaluator, typename CoeffReturnType, typename KernelType, typ
           typename Kernel_accessor, typename Buffer_accessor>
 struct EigenConvolutionKernel<Evaluator, CoeffReturnType, KernelType, Index, InputDims, Kernel_accessor,
                               Buffer_accessor, convolution_type::CONV1D> {
-  typedef cl::sycl::accessor<CoeffReturnType, 1, cl::sycl::access::mode::read_write, cl::sycl::access::target::local>
+  typedef sycl::accessor<CoeffReturnType, 1, sycl::access::mode::read_write, sycl::access::target::local>
       Local_accessor;
   Local_accessor local_acc;
   Evaluator device_evaluator;
@@ -41,11 +41,11 @@ struct EigenConvolutionKernel<Evaluator, CoeffReturnType, KernelType, Index, Inp
   Buffer_accessor buffer_acc;
   internal::IndexMapper<Index, InputDims, 1, Evaluator::Layout> indexMapper;
   const size_t kernelSize;
-  const cl::sycl::range<2> input_range;
+  const sycl::range<2> input_range;
   EigenConvolutionKernel(Local_accessor local_acc_, Evaluator device_evaluator_, Kernel_accessor kernel_filter_,
                          Buffer_accessor buffer_acc_,
                          internal::IndexMapper<Index, InputDims, 1, Evaluator::Layout> indexMapper_,
-                         const size_t kernelSize_, const cl::sycl::range<2> input_range_)
+                         const size_t kernelSize_, const sycl::range<2> input_range_)
       : local_acc(local_acc_),
         device_evaluator(device_evaluator_),
         kernel_filter(kernel_filter_),
@@ -58,7 +58,7 @@ struct EigenConvolutionKernel<Evaluator, CoeffReturnType, KernelType, Index, Inp
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool boundary_check(const BooleanDim2 boolean_check) {
     return (boolean_check[0] && boolean_check[1]);
   }
-  void operator()(cl::sycl::nd_item<2> itemID) {
+  void operator()(sycl::nd_item<2> itemID) {
     auto buffer_ptr = buffer_acc.get_pointer();
     auto kernel_ptr = kernel_filter.get_pointer();
     // the required row to be calculated for the for each plane in shered memory
@@ -78,7 +78,7 @@ struct EigenConvolutionKernel<Evaluator, CoeffReturnType, KernelType, Index, Inp
               : CoeffReturnType(0);
     }
 
-    itemID.barrier(cl::sycl::access::fence_space::local_space);
+    itemID.barrier(sycl::access::fence_space::local_space);
 
     // calculate the convolution // output start x
     const size_t first_output_start = itemID.get_group(0) * (itemID.get_local_range()[0]);
@@ -100,19 +100,19 @@ template <typename Evaluator, typename CoeffReturnType, typename KernelType, typ
           typename Kernel_accessor, typename Buffer_accessor>
 struct EigenConvolutionKernel<Evaluator, CoeffReturnType, KernelType, Index, InputDims, Kernel_accessor,
                               Buffer_accessor, convolution_type::CONV2D> {
-  typedef cl::sycl::accessor<CoeffReturnType, 1, cl::sycl::access::mode::read_write, cl::sycl::access::target::local>
+  typedef sycl::accessor<CoeffReturnType, 1, sycl::access::mode::read_write, sycl::access::target::local>
       Local_accessor;
   Local_accessor local_acc;
   Evaluator device_evaluator;
   Kernel_accessor kernel_filter;
   Buffer_accessor buffer_acc;
   internal::IndexMapper<Index, InputDims, 2, Evaluator::Layout> indexMapper;
-  const cl::sycl::range<2> kernel_size;
-  const cl::sycl::range<3> input_range;
+  const sycl::range<2> kernel_size;
+  const sycl::range<3> input_range;
   EigenConvolutionKernel(Local_accessor local_acc_, Evaluator device_evaluator_, Kernel_accessor kernel_filter_,
                          Buffer_accessor buffer_acc_,
                          internal::IndexMapper<Index, InputDims, 2, Evaluator::Layout> indexMapper_,
-                         const cl::sycl::range<2> kernel_size_, const cl::sycl::range<3> input_range_)
+                         const sycl::range<2> kernel_size_, const sycl::range<3> input_range_)
       : local_acc(local_acc_),
         device_evaluator(device_evaluator_),
         kernel_filter(kernel_filter_),
@@ -125,17 +125,17 @@ struct EigenConvolutionKernel<Evaluator, CoeffReturnType, KernelType, Index, Inp
     return (boolean_check[0] && boolean_check[1] && boolean_check[2]);
   }
 
-  void operator()(cl::sycl::nd_item<3> itemID) {
+  void operator()(sycl::nd_item<3> itemID) {
     auto buffer_ptr = buffer_acc.get_pointer();
     auto kernel_ptr = kernel_filter.get_pointer();
     // the required row to be calculated for the for each plane in shered memory
-    const auto num_input = cl::sycl::range<2>{
-        (cl::sycl::range<2>(itemID.get_local_range()[0], itemID.get_local_range()[1]) + kernel_size - 1)};
+    const auto num_input = sycl::range<2>{
+        (sycl::range<2>(itemID.get_local_range()[0], itemID.get_local_range()[1]) + kernel_size - 1)};
 
     const size_t plane_input_offset = indexMapper.mapGpuInputPlaneToTensorInputOffset(itemID.get_global_id(2));
     const size_t plane_kernel_offset = itemID.get_local_id(2) * num_input[1];
 
-    const auto input_offset = cl::sycl::range<2>{itemID.get_group(0) * itemID.get_local_range()[0],
+    const auto input_offset = sycl::range<2>{itemID.get_group(0) * itemID.get_local_range()[0],
                                                  itemID.get_group(1) * itemID.get_local_range()[1]};
       
     // fill the local memory
@@ -154,10 +154,10 @@ struct EigenConvolutionKernel<Evaluator, CoeffReturnType, KernelType, Index, Inp
       }
     }
 
-    itemID.barrier(cl::sycl::access::fence_space::local_space);
+    itemID.barrier(sycl::access::fence_space::local_space);
 
     // output offset start for each thread
-    const auto output_offset = cl::sycl::range<2>{itemID.get_group(0) * itemID.get_local_range()[0],
+    const auto output_offset = sycl::range<2>{itemID.get_group(0) * itemID.get_local_range()[0],
                                                   itemID.get_group(1) * itemID.get_local_range()[1]};
 
     if (boundary_check(itemID.get_global_id() < input_range)) {
@@ -185,21 +185,21 @@ template <typename Evaluator, typename CoeffReturnType, typename KernelType, typ
           typename Kernel_accessor, typename Buffer_accessor>
 struct EigenConvolutionKernel<Evaluator, CoeffReturnType, KernelType, Index, InputDims, Kernel_accessor,
                               Buffer_accessor, convolution_type::CONV3D> {
-  typedef cl::sycl::accessor<CoeffReturnType, 1, cl::sycl::access::mode::read_write, cl::sycl::access::target::local>
+  typedef sycl::accessor<CoeffReturnType, 1, sycl::access::mode::read_write, sycl::access::target::local>
       Local_accessor;
   Local_accessor local_acc;
   Evaluator device_evaluator;
   Kernel_accessor kernel_filter;
   Buffer_accessor buffer_acc;
   internal::IndexMapper<Index, InputDims, 3, Evaluator::Layout> indexMapper;
-  const cl::sycl::range<3> kernel_size;
-  const cl::sycl::range<3> input_range;
+  const sycl::range<3> kernel_size;
+  const sycl::range<3> input_range;
   const size_t numP;
 
   EigenConvolutionKernel(Local_accessor local_acc_, Evaluator device_evaluator_, Kernel_accessor kernel_filter_,
                          Buffer_accessor buffer_acc_,
                          internal::IndexMapper<Index, InputDims, 3, Evaluator::Layout> indexMapper_,
-                         const cl::sycl::range<3> kernel_size_, const cl::sycl::range<3> input_range_,
+                         const sycl::range<3> kernel_size_, const sycl::range<3> input_range_,
                          const size_t numP_)
       : local_acc(local_acc_),
         device_evaluator(device_evaluator_),
@@ -213,15 +213,15 @@ struct EigenConvolutionKernel<Evaluator, CoeffReturnType, KernelType, Index, Inp
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool boundary_check(const BooleanDim3 boolean_check) {
     return (boolean_check[0] && boolean_check[1] && boolean_check[2]);
   }
-  void operator()(cl::sycl::nd_item<3> itemID) {
+  void operator()(sycl::nd_item<3> itemID) {
     auto buffer_ptr = buffer_acc.get_pointer();
     auto kernel_ptr = kernel_filter.get_pointer();
-    const auto num_input = cl::sycl::range<3>{itemID.get_local_range() + kernel_size - 1};
+    const auto num_input = sycl::range<3>{itemID.get_local_range() + kernel_size - 1};
 
-    const auto input_offset = cl::sycl::range<3>{itemID.get_group().get_id() * itemID.get_local_range()};
+    const auto input_offset = sycl::range<3>{itemID.get_group().get_id() * itemID.get_local_range()};
 
     const auto output_offset =
-          cl::sycl::range<3>{itemID.get_group().get_id() * itemID.get_local_range() + itemID.get_local_id()};
+          sycl::range<3>{itemID.get_group().get_id() * itemID.get_local_range() + itemID.get_local_id()};
 
     for (size_t p = 0; p < numP; p++) {
       /// fill the shared memory
@@ -242,7 +242,7 @@ struct EigenConvolutionKernel<Evaluator, CoeffReturnType, KernelType, Index, Inp
           }
         }
       }
-      itemID.barrier(cl::sycl::access::fence_space::local_space);
+      itemID.barrier(sycl::access::fence_space::local_space);
 
       // calculate the convolution
 
@@ -266,7 +266,7 @@ struct EigenConvolutionKernel<Evaluator, CoeffReturnType, KernelType, Index, Inp
         buffer_ptr[tensor_index] = result;
       }
 
-      itemID.barrier(cl::sycl::access::fence_space::local_space);
+      itemID.barrier(sycl::access::fence_space::local_space);
     }
   }
 };
@@ -392,8 +392,8 @@ struct TensorEvaluator<const TensorConvolutionOp<Indices, InputArgType, KernelAr
         const size_t numX = dimensions()[m_indices[0]];
         const size_t numP = dimensions().TotalSize() / numX;
         const auto input_dim = std::array<size_t, 2>{numX, numP};
-        auto global_range = cl::sycl::range<2>{};
-        auto local_range = cl::sycl::range<2>{};
+        auto global_range = sycl::range<2>{};
+        auto local_range = sycl::range<2>{};
         const size_t kernel_size = m_kernelImpl.dimensions().TotalSize();
 
         m_device.parallel_for_setup(input_dim, global_range, local_range);
@@ -408,23 +408,23 @@ struct TensorEvaluator<const TensorConvolutionOp<Indices, InputArgType, KernelAr
             ConvKernel;
 
         m_device.template binary_kernel_launcher<CoeffReturnType, ConvKernel>(
-            m_inputImpl, m_kernel, data, cl::sycl::nd_range<2>(global_range, local_range), local_memory_size,
-            indexMapper, kernel_size, cl::sycl::range<2>(input_dim[0], input_dim[1]));
+            m_inputImpl, m_kernel, data, sycl::nd_range<2>(global_range, local_range), local_memory_size,
+            indexMapper, kernel_size, sycl::range<2>(input_dim[0], input_dim[1]));
         break;
       }
 
       case 2: {
         auto kernel_index = std::array<size_t, 2>{static_cast<int>(Layout) == static_cast<int>(ColMajor) ? 0 : 1,
                                                   static_cast<int>(Layout) == static_cast<int>(ColMajor) ? 1 : 0};
-        auto kernel_size = cl::sycl::range<2>{(size_t)m_kernelImpl.dimensions()[kernel_index[0]],
+        auto kernel_size = sycl::range<2>{(size_t)m_kernelImpl.dimensions()[kernel_index[0]],
                                               (size_t)m_kernelImpl.dimensions()[kernel_index[1]]};
         const size_t numX = dimensions()[m_indices[kernel_index[0]]];
         const size_t numY = dimensions()[m_indices[kernel_index[1]]];
         const size_t numP = dimensions().TotalSize() / (numX * numY);
         auto input_dim = std::array<size_t, 3>{numX, numY, numP};
 
-        auto global_range = cl::sycl::range<3>{};
-        auto local_range = cl::sycl::range<3>{};
+        auto global_range = sycl::range<3>{};
+        auto local_range = sycl::range<3>{};
 
         m_device.parallel_for_setup(input_dim, global_range, local_range);
 
@@ -439,8 +439,8 @@ struct TensorEvaluator<const TensorConvolutionOp<Indices, InputArgType, KernelAr
                                        typename KernelStorage::Type, EvaluatorPointerType, convolution_type::CONV2D>
             ConvKernel;
         m_device.template binary_kernel_launcher<CoeffReturnType, ConvKernel>(
-            m_inputImpl, m_kernel, data, cl::sycl::nd_range<3>(global_range, local_range), local_memory_size,
-            indexMapper, kernel_size, cl::sycl::range<3>{input_dim[0], input_dim[1], input_dim[2]});
+            m_inputImpl, m_kernel, data, sycl::nd_range<3>(global_range, local_range), local_memory_size,
+            indexMapper, kernel_size, sycl::range<3>{input_dim[0], input_dim[1], input_dim[2]});
         break;
       }
 
@@ -449,7 +449,7 @@ struct TensorEvaluator<const TensorConvolutionOp<Indices, InputArgType, KernelAr
                                                   static_cast<int>(Layout) == static_cast<int>(ColMajor) ? 1 : 1,
                                                   static_cast<int>(Layout) == static_cast<int>(ColMajor) ? 2 : 0};
 
-        auto kernel_size = cl::sycl::range<3>{(size_t)m_kernelImpl.dimensions()[kernel_index[0]],
+        auto kernel_size = sycl::range<3>{(size_t)m_kernelImpl.dimensions()[kernel_index[0]],
                                               (size_t)m_kernelImpl.dimensions()[kernel_index[1]],
                                               (size_t)m_kernelImpl.dimensions()[kernel_index[2]]};
 
@@ -467,8 +467,8 @@ struct TensorEvaluator<const TensorConvolutionOp<Indices, InputArgType, KernelAr
 
         internal::IndexMapper<Index, InputDims, 3, Layout> indexMapper(m_inputImpl.dimensions(), kernel_dims, indices);
 
-        auto global_range = cl::sycl::range<3>{};
-        auto local_range = cl::sycl::range<3>{};
+        auto global_range = sycl::range<3>{};
+        auto local_range = sycl::range<3>{};
 
         m_device.parallel_for_setup(input_dim, global_range, local_range);
         auto local_memory_range = (local_range + kernel_size - 1);
@@ -479,8 +479,8 @@ struct TensorEvaluator<const TensorConvolutionOp<Indices, InputArgType, KernelAr
                                        typename KernelStorage::Type, EvaluatorPointerType, convolution_type::CONV3D>
             ConvKernel;
         m_device.template binary_kernel_launcher<CoeffReturnType, ConvKernel>(
-            m_inputImpl, m_kernel, data, cl::sycl::nd_range<3>(global_range, local_range), local_memory_size,
-            indexMapper, kernel_size, cl::sycl::range<3>(input_dim[0], input_dim[1], input_dim[2]), numP);
+            m_inputImpl, m_kernel, data, sycl::nd_range<3>(global_range, local_range), local_memory_size,
+            indexMapper, kernel_size, sycl::range<3>(input_dim[0], input_dim[1], input_dim[2]), numP);
         break;
       }
 
@@ -518,7 +518,7 @@ struct TensorEvaluator<const TensorConvolutionOp<Indices, InputArgType, KernelAr
                           TensorOpCost(0, 0, convolve_compute_cost, vectorized, PacketSize));
   }
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_kernelImpl.bind(cgh);
     m_inputImpl.bind(cgh);
     m_buf.bind(cgh);
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorCustomOp.h b/unsupported/Eigen/CXX11/src/Tensor/TensorCustomOp.h
index 476b2282a..292b2855d 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorCustomOp.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorCustomOp.h
@@ -24,6 +24,7 @@ template<typename CustomUnaryFunc, typename XprType>
 struct traits<TensorCustomUnaryOp<CustomUnaryFunc, XprType> >
 {
   typedef typename XprType::Scalar Scalar;
+  typedef traits<XprType> XprTraits;
   typedef typename XprType::StorageKind StorageKind;
   typedef typename XprType::Index Index;
   typedef typename XprType::Nested Nested;
@@ -151,7 +152,7 @@ struct TensorEvaluator<const TensorCustomUnaryOp<CustomUnaryFunc, XprType>, Devi
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_result.bind(cgh);
   }
 #endif
@@ -324,7 +325,7 @@ struct TensorEvaluator<const TensorCustomBinaryOp<CustomBinaryFunc, LhsXprType,
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_result.bind(cgh);
   }
 #endif
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceGpu.h b/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceGpu.h
index 7f3394438..54fc519c1 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceGpu.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceGpu.h
@@ -273,7 +273,7 @@ struct GpuDevice {
   }
 
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void synchronize() const {
-#ifndef EIGEN_GPU_COMPILE_PHASE
+#if defined(EIGEN_GPUCC) && !defined(EIGEN_GPU_COMPILE_PHASE)
     gpuError_t err = gpuStreamSynchronize(stream_->stream());
     if (err != gpuSuccess) {
       std::cerr << "Error detected in GPU stream: "
@@ -281,6 +281,12 @@ struct GpuDevice {
                 << std::endl;
       gpu_assert(err == gpuSuccess);
     }
+#elif EIGEN_USE_DPCPP
+#ifdef EIGEN_EXCEPTIONS
+    stream_->stream()->wait_and_throw();
+#else
+    stream_->stream()->wait();
+#endif
 #else
     gpu_assert(false && "The default device should be used instead to generate kernel code");
 #endif
@@ -298,6 +304,13 @@ struct GpuDevice {
   EIGEN_STRONG_INLINE int sharedMemPerBlock() const {
     return stream_->deviceProperties().sharedMemPerBlock;
   }
+
+  #ifdef EIGEN_USE_DPCPP
+  EIGEN_STRONG_INLINE size_t getNearestPowerOfTwoWorkGroupSize() const {
+    return getPowerOfTwo(stream()->get_device().template get_info<sycl::info::device::max_work_group_size>(), false);
+  }
+  #endif
+
   EIGEN_STRONG_INLINE int majorDeviceVersion() const {
     return stream_->deviceProperties().major;
   }
@@ -320,9 +333,142 @@ struct GpuDevice {
 #endif
   }
 
+#ifdef EIGEN_USE_DPCPP
+template<typename Index>
+  EIGEN_STRONG_INLINE void parallel_for_setup(Index n, Index &tileSize, Index &rng, Index &GRange) const {
+    tileSize = static_cast<Index>(stream()->get_device(). template get_info<sycl::info::device::max_work_group_size>());
+    if(stream()->get_device().is_cpu()){ // intel doesn't allow to use max workgroup size
+      tileSize=std::min(static_cast<Index>(256), static_cast<Index>(tileSize));
+    }
+    rng = n;
+    if (rng==0) rng=static_cast<Index>(1);
+    GRange=rng;
+    if (tileSize>GRange) tileSize=GRange;
+    else if(GRange>tileSize){
+      Index xMode =  static_cast<Index>(GRange % tileSize);
+      if (xMode != 0) GRange += static_cast<Index>(tileSize - xMode);
+    }
+  }
+
+  template <typename OutScalar, typename sycl_kernel, typename InPtr,
+          typename Range, typename Index, typename... T>
+  EIGEN_ALWAYS_INLINE void nullary_kernel_launcher(const InPtr &inptr,
+                                                 Range thread_range,
+                                                 Index scratchSize,
+                                                 T... var) const {
+    auto kernel_functor = [=](sycl::handler &cgh) {
+    typedef sycl::accessor<OutScalar, 1,
+                               sycl::access::mode::read_write,
+                               sycl::access::target::local>
+        LocalAccessor;
+
+    LocalAccessor scratch(sycl::range<1>(scratchSize), cgh);
+    cgh.parallel_for<sycl_kernel>(
+          thread_range, sycl_kernel(scratch, inptr, var...));
+    };
+    sycl::event e;
+    EIGEN_SYCL_TRY_CATCH(e = stream_->stream()->submit(kernel_functor));
+    async_synchronize(e);
+  }
+
+  EIGEN_STRONG_INLINE void async_synchronize(sycl::event e) const {
+    set_latest_event(e);
+  }
+
+  template <typename OutScalar, typename sycl_kernel, typename InPtr,
+            typename OutPtr, typename Range, typename Index, typename... T>
+  EIGEN_ALWAYS_INLINE void unary_kernel_launcher(const InPtr &inptr,
+                                                 OutPtr &outptr,
+                                                 Range thread_range,
+                                                 Index scratchSize,
+                                                 T... var) const {
+    auto kernel_functor = [=](sycl::handler &cgh) {
+    typedef sycl::accessor<OutScalar, 1,
+                               sycl::access::mode::read_write,
+                               sycl::access::target::local>
+        LocalAccessor;
+
+    LocalAccessor scratch(sycl::range<1>(scratchSize), cgh);
+    cgh.parallel_for<sycl_kernel>(
+          thread_range, sycl_kernel(scratch, inptr, outptr, var...));
+    };
+    sycl::event e;
+    EIGEN_SYCL_TRY_CATCH(e = stream_->stream()->submit(kernel_functor));
+    async_synchronize(e);
+  }
+
+  template <typename OutScalar, typename sycl_kernel, typename Lhs,
+            typename Rhs, typename OutPtr, typename Range, typename Index,
+            typename... T>
+  EIGEN_ALWAYS_INLINE void binary_kernel_launcher(const Lhs &lhs,
+                                                  const Rhs &rhs, OutPtr outptr,
+                                                  Range thread_range,
+                                                  Index scratchSize,
+                                                  T... var) const {
+    auto kernel_functor = [=](sycl::handler &cgh) {
+    typedef sycl::accessor<OutScalar, 1,
+                               sycl::access::mode::read_write,
+                               sycl::access::target::local>
+        LocalAccessor;
+
+    LocalAccessor scratch(sycl::range<1>(scratchSize), cgh);
+    cgh.parallel_for<sycl_kernel>(
+          thread_range, sycl_kernel(scratch, lhs, rhs, outptr, var...));
+    };
+    sycl::event e;
+    EIGEN_SYCL_TRY_CATCH(e = stream_->stream()->submit(kernel_functor));
+    async_synchronize(e);
+  }
+
+  EIGEN_STRONG_INLINE size_t getPowerOfTwo(size_t wGSize, bool roundUp) const {
+    if (roundUp) --wGSize;
+    wGSize |= (wGSize >> 1);
+    wGSize |= (wGSize >> 2);
+    wGSize |= (wGSize >> 4);
+    wGSize |= (wGSize >> 8);
+    wGSize |= (wGSize >> 16);
+#if EIGEN_ARCH_x86_64 || EIGEN_ARCH_ARM64 || EIGEN_OS_WIN64
+    wGSize |= (wGSize >> 32);
+#endif
+    return ((!roundUp) ? (wGSize - (wGSize >> 1)) : ++wGSize);
+  }
+
+  EIGEN_STRONG_INLINE unsigned long getNumSyclMultiProcessors() const {
+     return stream_->deviceProperties().multiProcessorCount;
+  }
+
+  EIGEN_STRONG_INLINE bool has_local_memory() const {
+#if !defined(EIGEN_SYCL_LOCAL_MEM) && defined(EIGEN_SYCL_NO_LOCAL_MEM)
+    return false;
+#elif defined(EIGEN_SYCL_LOCAL_MEM) && !defined(EIGEN_SYCL_NO_LOCAL_MEM)
+    return true;
+#else
+    return stream_->deviceProperties().local_mem_type ==
+           sycl::info::local_mem_type::local;
+#endif
+  }
+
+#endif
+
  private:
   const StreamInterface* stream_;
   int max_blocks_;
+#ifdef EIGEN_USE_DPCPP
+#ifdef EIGEN_SYCL_STORE_LATEST_EVENT
+  mutable std::mutex event_mutex_;
+  mutable std::unordered_map<std::thread::id, sycl::event> latest_events_;
+#endif
+
+  EIGEN_STRONG_INLINE void set_latest_event(sycl::event e) const {
+#ifdef EIGEN_SYCL_STORE_LATEST_EVENT
+    std::lock_guard<std::mutex> lock(event_mutex_);
+    latest_events_[std::this_thread::get_id()] = e;
+#else
+    EIGEN_UNUSED_VARIABLE(e);
+#endif
+  }
+
+#endif // EIGEN_USE_DPCPP
 };
 
 #if defined(EIGEN_HIPCC)
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h b/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h
index df591c21d..b2090406e 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h
@@ -23,39 +23,39 @@ namespace internal {
 
 /// Cache all the device information needed
 struct SyclDeviceInfo {
-  SyclDeviceInfo(cl::sycl::queue queue)
+  SyclDeviceInfo(sycl::queue queue)
       : local_mem_type(
             queue.get_device()
-                .template get_info<cl::sycl::info::device::local_mem_type>()),
+                .template get_info<sycl::info::device::local_mem_type>()),
         max_work_item_sizes(
             queue.get_device()
                 .template get_info<
-                    cl::sycl::info::device::max_work_item_sizes>()),
+                    sycl::info::device::max_work_item_sizes>()),
         max_mem_alloc_size(
             queue.get_device()
                 .template get_info<
-                    cl::sycl::info::device::max_mem_alloc_size>()),
+                    sycl::info::device::max_mem_alloc_size>()),
         max_compute_units(queue.get_device()
                               .template get_info<
-                                  cl::sycl::info::device::max_compute_units>()),
+                                  sycl::info::device::max_compute_units>()),
         max_work_group_size(
             queue.get_device()
                 .template get_info<
-                    cl::sycl::info::device::max_work_group_size>()),
+                    sycl::info::device::max_work_group_size>()),
         local_mem_size(
             queue.get_device()
-                .template get_info<cl::sycl::info::device::local_mem_size>()),
+                .template get_info<sycl::info::device::local_mem_size>()),
         platform_name(queue.get_device()
                           .get_platform()
-                          .template get_info<cl::sycl::info::platform::name>()),
+                          .template get_info<sycl::info::platform::name>()),
         device_name(queue.get_device()
-                        .template get_info<cl::sycl::info::device::name>()),
+                        .template get_info<sycl::info::device::name>()),
         device_vendor(
             queue.get_device()
-                .template get_info<cl::sycl::info::device::vendor>()) {}
+                .template get_info<sycl::info::device::vendor>()) {}
 
-  cl::sycl::info::local_mem_type local_mem_type;
-  cl::sycl::id<3> max_work_item_sizes;
+  sycl::info::local_mem_type local_mem_type;
+  sycl::id<3> max_work_item_sizes;
   unsigned long max_mem_alloc_size;
   unsigned long max_compute_units;
   unsigned long max_work_group_size;
@@ -73,20 +73,20 @@ typedef TensorSycl::internal::buffer_data_type_t buffer_scalar_t;
 // can consume SPIR or SPIRV can use the Eigen SYCL backend and consequently
 // TensorFlow via the Eigen SYCL Backend.
 EIGEN_STRONG_INLINE auto get_sycl_supported_devices()
-    -> decltype(cl::sycl::device::get_devices()) {
+    -> decltype(sycl::device::get_devices()) {
 #ifdef EIGEN_SYCL_USE_DEFAULT_SELECTOR
-  return {cl::sycl::device(cl::sycl::default_selector())};
+  return {sycl::device(sycl::default_selector())};
 #else
-  std::vector<cl::sycl::device> supported_devices;
-  auto platform_list = cl::sycl::platform::get_platforms();
+  std::vector<sycl::device> supported_devices;
+  auto platform_list = sycl::platform::get_platforms();
   for (const auto &platform : platform_list) {
     auto device_list = platform.get_devices();
     auto platform_name =
-        platform.template get_info<cl::sycl::info::platform::name>();
+        platform.template get_info<sycl::info::platform::name>();
     std::transform(platform_name.begin(), platform_name.end(),
                    platform_name.begin(), ::tolower);
     for (const auto &device : device_list) {
-      auto vendor = device.template get_info<cl::sycl::info::device::vendor>();
+      auto vendor = device.template get_info<sycl::info::device::vendor>();
       std::transform(vendor.begin(), vendor.end(), vendor.begin(), ::tolower);
       bool unsupported_condition =
           (device.is_cpu() && platform_name.find("amd") != std::string::npos &&
@@ -104,10 +104,10 @@ EIGEN_STRONG_INLINE auto get_sycl_supported_devices()
 
 class QueueInterface {
  public:
-  /// Creating device by using cl::sycl::selector or cl::sycl::device.
+  /// Creating device by using sycl::selector or sycl::device.
   template <typename DeviceOrSelector>
   explicit QueueInterface(
-      const DeviceOrSelector &dev_or_sel, cl::sycl::async_handler handler,
+      const DeviceOrSelector &dev_or_sel, sycl::async_handler handler,
       unsigned num_threads = std::thread::hardware_concurrency())
       : m_queue(dev_or_sel, handler),
 #ifdef EIGEN_SYCL_USE_PROGRAM_CLASS
@@ -117,7 +117,7 @@ class QueueInterface {
         m_device_info(m_queue) {
 #ifdef EIGEN_SYCL_USE_PROGRAM_CLASS
     m_prog.build_with_kernel_type<DeviceOrSelector>();
-    auto f = [&](cl::sycl::handler &cgh) {
+    auto f = [&](sycl::handler &cgh) {
       cgh.single_task<DeviceOrSelector>(m_prog.get_kernel<DeviceOrSelector>(),
                                         [=]() {})
     };
@@ -130,18 +130,18 @@ class QueueInterface {
       const DeviceOrSelector &dev_or_sel,
       unsigned num_threads = std::thread::hardware_concurrency())
       : QueueInterface(dev_or_sel,
-                       [this](cl::sycl::exception_list l) {
+                       [this](sycl::exception_list l) {
                          this->exception_caught_ = this->sycl_async_handler(l);
                        },
                        num_threads) {}
 
 #ifdef EIGEN_SYCL_USE_PROGRAM_CLASS
-  EIGEN_STRONG_INLINE cl::sycl::program &program() const { return m_prog; }
+  EIGEN_STRONG_INLINE sycl::program &program() const { return m_prog; }
 #endif
 
   /// Attach an existing buffer to the pointer map, Eigen will not reuse it
   EIGEN_STRONG_INLINE void *attach_buffer(
-      cl::sycl::buffer<buffer_scalar_t, 1> &buf) const {
+      sycl::buffer<buffer_scalar_t, 1> &buf) const {
     std::lock_guard<std::mutex> lock(pmapper_mutex_);
     return static_cast<void *>(pMapper.add_pointer(buf));
   }
@@ -202,13 +202,13 @@ class QueueInterface {
   }
   template <typename data_t>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TensorSycl::internal::RangeAccess<
-      cl::sycl::access::mode::read_write, data_t>
+      sycl::access::mode::read_write, data_t>
   get(data_t *data) const {
-    return get_range_accessor<cl::sycl::access::mode::read_write, data_t>(data);
+    return get_range_accessor<sycl::access::mode::read_write, data_t>(data);
   }
   template <typename data_t>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE data_t *get(
-      TensorSycl::internal::RangeAccess<cl::sycl::access::mode::read_write,
+      TensorSycl::internal::RangeAccess<sycl::access::mode::read_write,
                                         data_t>
           data) const {
     return static_cast<data_t *>(data.get_virtual_pointer());
@@ -222,7 +222,7 @@ class QueueInterface {
     TensorSycl::internal::SYCLfree(p, pMapper);
 #endif
   }
-  template <cl::sycl::access::mode AcMd, typename T>
+  template <sycl::access::mode AcMd, typename T>
   EIGEN_STRONG_INLINE void deallocate_temp(
       const TensorSycl::internal::RangeAccess<AcMd, T> &p) const {
     deallocate_temp(p.get_virtual_pointer());
@@ -250,23 +250,23 @@ class QueueInterface {
   EIGEN_STRONG_INLINE void memcpyHostToDevice(
       void *dst, const void *src, size_t n,
       std::function<void()> callback) const {
-    static const auto write_mode = cl::sycl::access::mode::discard_write;
-    static const auto global_access = cl::sycl::access::target::global_buffer;
-    typedef cl::sycl::accessor<buffer_scalar_t, 1, write_mode, global_access>
+    static const auto write_mode = sycl::access::mode::discard_write;
+    static const auto global_access = sycl::access::target::global_buffer;
+    typedef sycl::accessor<buffer_scalar_t, 1, write_mode, global_access>
         write_accessor;
     if (n == 0) {
       if (callback) callback();
       return;
     }
     n /= sizeof(buffer_scalar_t);
-    auto f = [&](cl::sycl::handler &cgh) {
+    auto f = [&](sycl::handler &cgh) {
       write_accessor dst_acc = get_range_accessor<write_mode>(cgh, dst, n);
       buffer_scalar_t const *ptr = static_cast<buffer_scalar_t const *>(src);
       auto non_deleter = [](buffer_scalar_t const *) {};
       std::shared_ptr<const buffer_scalar_t> s_ptr(ptr, non_deleter);
       cgh.copy(s_ptr, dst_acc);
     };
-    cl::sycl::event e;
+    sycl::event e;
     EIGEN_SYCL_TRY_CATCH(e = m_queue.submit(f));
     synchronize_and_callback(e, callback);
   }
@@ -278,23 +278,23 @@ class QueueInterface {
   EIGEN_STRONG_INLINE void memcpyDeviceToHost(
       void *dst, const void *src, size_t n,
       std::function<void()> callback) const {
-    static const auto read_mode = cl::sycl::access::mode::read;
-    static const auto global_access = cl::sycl::access::target::global_buffer;
-    typedef cl::sycl::accessor<buffer_scalar_t, 1, read_mode, global_access>
+    static const auto read_mode = sycl::access::mode::read;
+    static const auto global_access = sycl::access::target::global_buffer;
+    typedef sycl::accessor<buffer_scalar_t, 1, read_mode, global_access>
         read_accessor;
     if (n == 0) {
       if (callback) callback();
       return;
     }
     n /= sizeof(buffer_scalar_t);
-    auto f = [&](cl::sycl::handler &cgh) {
+    auto f = [&](sycl::handler &cgh) {
       read_accessor src_acc = get_range_accessor<read_mode>(cgh, src, n);
       buffer_scalar_t *ptr = static_cast<buffer_scalar_t *>(dst);
       auto non_deleter = [](buffer_scalar_t *) {};
       std::shared_ptr<buffer_scalar_t> s_ptr(ptr, non_deleter);
       cgh.copy(src_acc, s_ptr);
     };
-    cl::sycl::event e;
+    sycl::event e;
     EIGEN_SYCL_TRY_CATCH(e = m_queue.submit(f));
     synchronize_and_callback(e, callback);
   }
@@ -303,18 +303,18 @@ class QueueInterface {
   /// No callback is required here as both arguments are on the device
   /// and SYCL can handle the dependency.
   EIGEN_STRONG_INLINE void memcpy(void *dst, const void *src, size_t n) const {
-    static const auto read_mode = cl::sycl::access::mode::read;
-    static const auto write_mode = cl::sycl::access::mode::discard_write;
+    static const auto read_mode = sycl::access::mode::read;
+    static const auto write_mode = sycl::access::mode::discard_write;
     if (n == 0) {
       return;
     }
     n /= sizeof(buffer_scalar_t);
-    auto f = [&](cl::sycl::handler &cgh) {
+    auto f = [&](sycl::handler &cgh) {
       auto src_acc = get_range_accessor<read_mode>(cgh, src, n);
       auto dst_acc = get_range_accessor<write_mode>(cgh, dst, n);
       cgh.copy(src_acc, dst_acc);
     };
-    cl::sycl::event e;
+    sycl::event e;
     EIGEN_SYCL_TRY_CATCH(e = m_queue.submit(f));
     async_synchronize(e);
   }
@@ -323,19 +323,19 @@ class QueueInterface {
   /// No callback is required here as both arguments are on the device
   /// and SYCL can handle the dependency.
   EIGEN_STRONG_INLINE void memset(void *data, int c, size_t n) const {
-    static const auto write_mode = cl::sycl::access::mode::discard_write;
+    static const auto write_mode = sycl::access::mode::discard_write;
     if (n == 0) {
       return;
     }
     n /= sizeof(buffer_scalar_t);
-    auto f = [&](cl::sycl::handler &cgh) {
+    auto f = [&](sycl::handler &cgh) {
       auto dst_acc = get_range_accessor<write_mode>(cgh, data, n);
       // The cast to uint8_t is here to match the behaviour of the standard
       // memset. The cast to buffer_scalar_t is needed to match the type of the
       // accessor (in case buffer_scalar_t is not uint8_t)
       cgh.fill(dst_acc, static_cast<buffer_scalar_t>(static_cast<uint8_t>(c)));
     };
-    cl::sycl::event e;
+    sycl::event e;
     EIGEN_SYCL_TRY_CATCH(e = m_queue.submit(f));
     async_synchronize(e);
   }
@@ -347,11 +347,11 @@ class QueueInterface {
   /// NOTE: Inside a kernel the range accessor will always be indexed from the
   /// start of the buffer, so the offset in the accessor is only used by
   /// methods like handler::copy and will not be available inside a kernel.
-  template <cl::sycl::access::mode AcMd, typename T>
+  template <sycl::access::mode AcMd, typename T>
   EIGEN_STRONG_INLINE TensorSycl::internal::RangeAccess<AcMd, T>
   get_range_accessor(const void *ptr) const {
-    static const auto global_access = cl::sycl::access::target::global_buffer;
-    static const auto is_place_holder = cl::sycl::access::placeholder::true_t;
+    static const auto global_access = sycl::access::target::global_buffer;
+    static const auto is_place_holder = sycl::access::placeholder::true_t;
     typedef TensorSycl::internal::RangeAccess<AcMd, T> ret_type;
     typedef const TensorSycl::internal::buffer_data_type_t *internal_ptr_t;
 
@@ -364,27 +364,27 @@ class QueueInterface {
     const auto typed_size = original_buffer.get_size() / sizeof(T);
     auto buffer = original_buffer.template reinterpret<
         typename Eigen::internal::remove_const<T>::type>(
-        cl::sycl::range<1>(typed_size));
+        sycl::range<1>(typed_size));
     const ptrdiff_t size = buffer.get_count() - typed_offset;
     eigen_assert(size >= 0);
-    typedef cl::sycl::accessor<typename Eigen::internal::remove_const<T>::type,
+    typedef sycl::accessor<typename Eigen::internal::remove_const<T>::type,
                                1, AcMd, global_access, is_place_holder>
         placeholder_accessor_t;
     const auto start_ptr = static_cast<internal_ptr_t>(ptr) - offset;
-    return ret_type(placeholder_accessor_t(buffer, cl::sycl::range<1>(size),
-                                           cl::sycl::id<1>(typed_offset)),
+    return ret_type(placeholder_accessor_t(buffer, sycl::range<1>(size),
+                                           sycl::id<1>(typed_offset)),
                     static_cast<size_t>(typed_offset),
                     reinterpret_cast<std::intptr_t>(start_ptr));
   }
 
   /// Get a range accessor to the virtual pointer's device memory with a
   /// specified size.
-  template <cl::sycl::access::mode AcMd, typename Index>
-  EIGEN_STRONG_INLINE cl::sycl::accessor<
-      buffer_scalar_t, 1, AcMd, cl::sycl::access::target::global_buffer>
-  get_range_accessor(cl::sycl::handler &cgh, const void *ptr,
+  template <sycl::access::mode AcMd, typename Index>
+  EIGEN_STRONG_INLINE sycl::accessor<
+      buffer_scalar_t, 1, AcMd, sycl::access::target::global_buffer>
+  get_range_accessor(sycl::handler &cgh, const void *ptr,
                      const Index n_bytes) const {
-    static const auto global_access = cl::sycl::access::target::global_buffer;
+    static const auto global_access = sycl::access::target::global_buffer;
     eigen_assert(n_bytes >= 0);
     std::lock_guard<std::mutex> lock(pmapper_mutex_);
     auto buffer = pMapper.get_buffer(ptr);
@@ -392,24 +392,24 @@ class QueueInterface {
     eigen_assert(offset >= 0);
     eigen_assert(offset + n_bytes <= buffer.get_size());
     return buffer.template get_access<AcMd, global_access>(
-        cgh, cl::sycl::range<1>(n_bytes), cl::sycl::id<1>(offset));
+        cgh, sycl::range<1>(n_bytes), sycl::id<1>(offset));
   }
 
   /// Creation of sycl accessor for a buffer. This function first tries to find
   /// the buffer in the buffer_map. If found it gets the accessor from it, if
   /// not, the function then adds an entry by creating a sycl buffer for that
   /// particular pointer.
-  template <cl::sycl::access::mode AcMd>
-  EIGEN_STRONG_INLINE cl::sycl::accessor<
-      buffer_scalar_t, 1, AcMd, cl::sycl::access::target::global_buffer>
-  get_sycl_accessor(cl::sycl::handler &cgh, const void *ptr) const {
+  template <sycl::access::mode AcMd>
+  EIGEN_STRONG_INLINE sycl::accessor<
+      buffer_scalar_t, 1, AcMd, sycl::access::target::global_buffer>
+  get_sycl_accessor(sycl::handler &cgh, const void *ptr) const {
     std::lock_guard<std::mutex> lock(pmapper_mutex_);
     return pMapper.get_buffer(ptr)
-        .template get_access<AcMd, cl::sycl::access::target::global_buffer>(
+        .template get_access<AcMd, sycl::access::target::global_buffer>(
             cgh);
   }
 
-  EIGEN_STRONG_INLINE cl::sycl::buffer<buffer_scalar_t, 1> get_sycl_buffer(
+  EIGEN_STRONG_INLINE sycl::buffer<buffer_scalar_t, 1> get_sycl_buffer(
       const void *ptr) const {
     std::lock_guard<std::mutex> lock(pmapper_mutex_);
     return pMapper.get_buffer(ptr);
@@ -428,24 +428,24 @@ class QueueInterface {
                                                   Range thread_range,
                                                   Index scratchSize,
                                                   T... var) const {
-    auto kernel_functor = [=](cl::sycl::handler &cgh) {
+    auto kernel_functor = [=](sycl::handler &cgh) {
       // binding the placeholder accessors to a commandgroup handler
       lhs.bind(cgh);
       rhs.bind(cgh);
       outptr.bind(cgh);
-      typedef cl::sycl::accessor<OutScalar, 1,
-                                 cl::sycl::access::mode::read_write,
-                                 cl::sycl::access::target::local>
+      typedef sycl::accessor<OutScalar, 1,
+                                 sycl::access::mode::read_write,
+                                 sycl::access::target::local>
           LocalAccessor;
 
-      LocalAccessor scratch(cl::sycl::range<1>(scratchSize), cgh);
+      LocalAccessor scratch(sycl::range<1>(scratchSize), cgh);
       cgh.parallel_for(
 #ifdef EIGEN_SYCL_USE_PROGRAM_CLASS
           program().template get_kernel<sycl_kernel>(),
 #endif
           thread_range, sycl_kernel(scratch, lhs, rhs, outptr, var...));
     };
-    cl::sycl::event e;
+    sycl::event e;
     EIGEN_SYCL_TRY_CATCH(e = m_queue.submit(kernel_functor));
     async_synchronize(e);
   }
@@ -457,23 +457,23 @@ class QueueInterface {
                                                  Range thread_range,
                                                  Index scratchSize,
                                                  T... var) const {
-    auto kernel_functor = [=](cl::sycl::handler &cgh) {
+    auto kernel_functor = [=](sycl::handler &cgh) {
       // binding the placeholder accessors to a commandgroup handler
       inptr.bind(cgh);
       outptr.bind(cgh);
-      typedef cl::sycl::accessor<OutScalar, 1,
-                                 cl::sycl::access::mode::read_write,
-                                 cl::sycl::access::target::local>
+      typedef sycl::accessor<OutScalar, 1,
+                                 sycl::access::mode::read_write,
+                                 sycl::access::target::local>
           LocalAccessor;
 
-      LocalAccessor scratch(cl::sycl::range<1>(scratchSize), cgh);
+      LocalAccessor scratch(sycl::range<1>(scratchSize), cgh);
       cgh.parallel_for(
 #ifdef EIGEN_SYCL_USE_PROGRAM_CLASS
           program().template get_kernel<sycl_kernel>(),
 #endif
           thread_range, sycl_kernel(scratch, inptr, outptr, var...));
     };
-    cl::sycl::event e;
+    sycl::event e;
     EIGEN_SYCL_TRY_CATCH(e = m_queue.submit(kernel_functor));
     async_synchronize(e);
   }
@@ -484,22 +484,22 @@ class QueueInterface {
                                                  Range thread_range,
                                                  Index scratchSize,
                                                  T... var) const {
-    auto kernel_functor = [=](cl::sycl::handler &cgh) {
+    auto kernel_functor = [=](sycl::handler &cgh) {
       // binding the placeholder accessors to a commandgroup handler
       inptr.bind(cgh);
-      typedef cl::sycl::accessor<OutScalar, 1,
-                                 cl::sycl::access::mode::read_write,
-                                 cl::sycl::access::target::local>
+      typedef sycl::accessor<OutScalar, 1,
+                                 sycl::access::mode::read_write,
+                                 sycl::access::target::local>
           LocalAccessor;
 
-      LocalAccessor scratch(cl::sycl::range<1>(scratchSize), cgh);
+      LocalAccessor scratch(sycl::range<1>(scratchSize), cgh);
       cgh.parallel_for(
 #ifdef EIGEN_SYCL_USE_PROGRAM_CLASS
           program().template get_kernel<sycl_kernel>(),
 #endif
           thread_range, sycl_kernel(scratch, inptr, var...));
     };
-    cl::sycl::event e;
+    sycl::event e;
     EIGEN_SYCL_TRY_CATCH(e = m_queue.submit(kernel_functor));
     async_synchronize(e);
   }
@@ -514,7 +514,7 @@ class QueueInterface {
   }
 
 
-  EIGEN_STRONG_INLINE void async_synchronize(cl::sycl::event e) const {
+  EIGEN_STRONG_INLINE void async_synchronize(sycl::event e) const {
     set_latest_event(e);
 #ifndef EIGEN_SYCL_ASYNC_EXECUTION
     synchronize();
@@ -543,8 +543,8 @@ class QueueInterface {
   /// threads per block for sycl kernels
   template <typename Index>
   EIGEN_STRONG_INLINE void parallel_for_setup(
-      const std::array<Index, 2> &input_dim, cl::sycl::range<2> &global_range,
-      cl::sycl::range<2> &local_range) const {
+      const std::array<Index, 2> &input_dim, sycl::range<2> &global_range,
+      sycl::range<2> &local_range) const {
     std::array<Index, 2> input_range = input_dim;
     Index max_workgroup_Size =
         static_cast<Index>(getNearestPowerOfTwoWorkGroupSize());
@@ -582,8 +582,8 @@ class QueueInterface {
   /// threads per block for sycl kernels
   template <typename Index>
   EIGEN_STRONG_INLINE void parallel_for_setup(
-      const std::array<Index, 3> &input_dim, cl::sycl::range<3> &global_range,
-      cl::sycl::range<3> &local_range) const {
+      const std::array<Index, 3> &input_dim, sycl::range<3> &global_range,
+      sycl::range<3> &local_range) const {
     std::array<Index, 3> input_range = input_dim;
     Index max_workgroup_Size =
         static_cast<Index>(getNearestPowerOfTwoWorkGroupSize());
@@ -639,7 +639,7 @@ class QueueInterface {
     return true;
 #else
     return m_device_info.local_mem_type ==
-           cl::sycl::info::local_mem_type::local;
+           sycl::info::local_mem_type::local;
 #endif
   }
 
@@ -655,7 +655,7 @@ class QueueInterface {
     return m_device_info.max_work_group_size;
   }
 
-  EIGEN_STRONG_INLINE cl::sycl::id<3> maxWorkItemSizes() const {
+  EIGEN_STRONG_INLINE sycl::id<3> maxWorkItemSizes() const {
     return m_device_info.max_work_item_sizes;
   }
 
@@ -705,7 +705,7 @@ class QueueInterface {
     return ((!roundUp) ? (wGSize - (wGSize >> 1)) : ++wGSize);
   }
 
-  EIGEN_STRONG_INLINE cl::sycl::queue &sycl_queue() const { return m_queue; }
+  EIGEN_STRONG_INLINE sycl::queue &sycl_queue() const { return m_queue; }
 
   // This function checks if the runtime recorded an error for the
   // underlying stream device.
@@ -716,13 +716,13 @@ class QueueInterface {
     return !exception_caught_;
   }
 
-  EIGEN_STRONG_INLINE cl::sycl::event get_latest_event() const {
+  EIGEN_STRONG_INLINE sycl::event get_latest_event() const {
 #ifdef EIGEN_SYCL_STORE_LATEST_EVENT
     std::lock_guard<std::mutex> lock(event_mutex_);
     return latest_events_[std::this_thread::get_id()];
 #else
     eigen_assert(false);
-    return cl::sycl::event();
+    return sycl::event();
 #endif
   }
 
@@ -735,7 +735,7 @@ class QueueInterface {
   }
 
  protected:
-  EIGEN_STRONG_INLINE void set_latest_event(cl::sycl::event e) const {
+  EIGEN_STRONG_INLINE void set_latest_event(sycl::event e) const {
 #ifdef EIGEN_SYCL_STORE_LATEST_EVENT
     std::lock_guard<std::mutex> lock(event_mutex_);
     latest_events_[std::this_thread::get_id()] = e;
@@ -744,15 +744,15 @@ class QueueInterface {
 #endif
   }
 
-  void synchronize_and_callback(cl::sycl::event e,
+  void synchronize_and_callback(sycl::event e,
                                 const std::function<void()> &callback) const {
     set_latest_event(e);
     if (callback) {
       auto callback_ = [=]() {
 #ifdef EIGEN_EXCEPTIONS
-        cl::sycl::event(e).wait_and_throw();
+        sycl::event(e).wait_and_throw();
 #else
-        cl::sycl::event(e).wait();
+        sycl::event(e).wait();
 #endif
         callback();
       };
@@ -766,7 +766,7 @@ class QueueInterface {
     }
   }
 
-  bool sycl_async_handler(cl::sycl::exception_list exceptions) const {
+  bool sycl_async_handler(sycl::exception_list exceptions) const {
     bool exception_caught = false;
     for (const auto &e : exceptions) {
       if (e) {
@@ -784,7 +784,7 @@ class QueueInterface {
 
 #ifdef EIGEN_SYCL_STORE_LATEST_EVENT
   mutable std::mutex event_mutex_;
-  mutable std::unordered_map<std::thread::id, cl::sycl::event> latest_events_;
+  mutable std::unordered_map<std::thread::id, sycl::event> latest_events_;
 #endif
 
   /// std::map is the container used to make sure that we create only one buffer
@@ -796,9 +796,9 @@ class QueueInterface {
   mutable std::unordered_set<void *> scratch_buffers;
 #endif
   /// sycl queue
-  mutable cl::sycl::queue m_queue;
+  mutable sycl::queue m_queue;
 #ifdef EIGEN_SYCL_USE_PROGRAM_CLASS
-  mutable cl::sycl::program m_prog;
+  mutable sycl::program m_prog;
 #endif
 
   /// The thread pool is used to wait on events and call callbacks
@@ -826,22 +826,22 @@ struct SyclDevice : public SyclDeviceBase {
       : SyclDeviceBase(queue_stream) {}
 
   // this is the accessor used to construct the evaluator
-  template <cl::sycl::access::mode AcMd, typename T>
+  template <sycl::access::mode AcMd, typename T>
   EIGEN_STRONG_INLINE TensorSycl::internal::RangeAccess<AcMd, T>
   get_range_accessor(const void *ptr) const {
     return queue_stream()->template get_range_accessor<AcMd, T>(ptr);
   }
 
   // get sycl accessor
-  template <cl::sycl::access::mode AcMd>
-  EIGEN_STRONG_INLINE cl::sycl::accessor<
-      buffer_scalar_t, 1, AcMd, cl::sycl::access::target::global_buffer>
-  get_sycl_accessor(cl::sycl::handler &cgh, const void *ptr) const {
+  template <sycl::access::mode AcMd>
+  EIGEN_STRONG_INLINE sycl::accessor<
+      buffer_scalar_t, 1, AcMd, sycl::access::target::global_buffer>
+  get_sycl_accessor(sycl::handler &cgh, const void *ptr) const {
     return queue_stream()->template get_sycl_accessor<AcMd>(cgh, ptr);
   }
 
   /// Accessing the created sycl device buffer for the device pointer
-  EIGEN_STRONG_INLINE cl::sycl::buffer<buffer_scalar_t, 1> get_sycl_buffer(
+  EIGEN_STRONG_INLINE sycl::buffer<buffer_scalar_t, 1> get_sycl_buffer(
       const void *ptr) const {
     return queue_stream()->get_sycl_buffer(ptr);
   }
@@ -858,8 +858,8 @@ struct SyclDevice : public SyclDeviceBase {
   /// threads per block for sycl kernels
   template <typename Index>
   EIGEN_STRONG_INLINE void parallel_for_setup(
-      const std::array<Index, 2> &input_dim, cl::sycl::range<2> &global_range,
-      cl::sycl::range<2> &local_range) const {
+      const std::array<Index, 2> &input_dim, sycl::range<2> &global_range,
+      sycl::range<2> &local_range) const {
     queue_stream()->parallel_for_setup(input_dim, global_range, local_range);
   }
 
@@ -867,8 +867,8 @@ struct SyclDevice : public SyclDeviceBase {
   /// threads per block for sycl kernels
   template <typename Index>
   EIGEN_STRONG_INLINE void parallel_for_setup(
-      const std::array<Index, 3> &input_dim, cl::sycl::range<3> &global_range,
-      cl::sycl::range<3> &local_range) const {
+      const std::array<Index, 3> &input_dim, sycl::range<3> &global_range,
+      sycl::range<3> &local_range) const {
     queue_stream()->parallel_for_setup(input_dim, global_range, local_range);
   }
 
@@ -889,7 +889,7 @@ struct SyclDevice : public SyclDeviceBase {
   EIGEN_STRONG_INLINE void deallocate_temp(void *buffer) const {
     queue_stream()->deallocate_temp(buffer);
   }
-  template <cl::sycl::access::mode AcMd, typename T>
+  template <sycl::access::mode AcMd, typename T>
   EIGEN_STRONG_INLINE void deallocate_temp(
       const TensorSycl::internal::RangeAccess<AcMd, T> &buffer) const {
     queue_stream()->deallocate_temp(buffer);
@@ -900,13 +900,13 @@ struct SyclDevice : public SyclDeviceBase {
 
   template <typename data_t>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TensorSycl::internal::RangeAccess<
-      cl::sycl::access::mode::read_write, data_t>
+      sycl::access::mode::read_write, data_t>
   get(data_t *data) const {
     return queue_stream()->get(data);
   }
   template <typename data_t>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE data_t *get(
-      TensorSycl::internal::RangeAccess<cl::sycl::access::mode::read_write,
+      TensorSycl::internal::RangeAccess<sycl::access::mode::read_write,
                                         data_t>
           data) const {
     return queue_stream()->get(data);
@@ -914,7 +914,7 @@ struct SyclDevice : public SyclDeviceBase {
 
   /// attach existing buffer
   EIGEN_STRONG_INLINE void *attach_buffer(
-      cl::sycl::buffer<buffer_scalar_t, 1> &buf) const {
+      sycl::buffer<buffer_scalar_t, 1> &buf) const {
     return queue_stream()->attach_buffer(buf);
   }
   /// detach buffer
@@ -952,11 +952,11 @@ struct SyclDevice : public SyclDeviceBase {
     queue_stream()->memset(data, c, n);
   }
   /// returning the sycl queue
-  EIGEN_STRONG_INLINE cl::sycl::queue &sycl_queue() const {
+  EIGEN_STRONG_INLINE sycl::queue &sycl_queue() const {
     return queue_stream()->sycl_queue();
   }
 #ifdef EIGEN_SYCL_USE_PROGRAM_CLASS
-  EIGEN_STRONG_INLINE cl::sycl::program &program() const {
+  EIGEN_STRONG_INLINE sycl::program &program() const {
     return queue_stream()->program();
   }
 #endif
@@ -974,7 +974,7 @@ struct SyclDevice : public SyclDeviceBase {
   EIGEN_STRONG_INLINE unsigned long maxSyclThreadsPerBlock() const {
     return queue_stream()->maxSyclThreadsPerBlock();
   }
-  EIGEN_STRONG_INLINE cl::sycl::id<3> maxWorkItemSizes() const {
+  EIGEN_STRONG_INLINE sycl::id<3> maxWorkItemSizes() const {
     return queue_stream()->maxWorkItemSizes();
   }
   EIGEN_STRONG_INLINE unsigned long maxSyclThreadsPerMultiProcessor() const {
@@ -1000,10 +1000,10 @@ struct SyclDevice : public SyclDeviceBase {
     queue_stream()->synchronize();
   }
   EIGEN_STRONG_INLINE void async_synchronize(
-      cl::sycl::event e = cl::sycl::event()) const {
+      sycl::event e = sycl::event()) const {
     queue_stream()->async_synchronize(e);
   }
-  EIGEN_STRONG_INLINE cl::sycl::event get_latest_event() const {
+  EIGEN_STRONG_INLINE sycl::event get_latest_event() const {
     return queue_stream()->get_latest_event();
   }
 
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorDimensions.h b/unsupported/Eigen/CXX11/src/Tensor/TensorDimensions.h
index 132458a20..f0f1e832a 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorDimensions.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorDimensions.h
@@ -466,7 +466,7 @@ struct sizes_match_below_dim {
 template <typename Dims1, typename Dims2, ptrdiff_t n>
 struct sizes_match_below_dim<Dims1, Dims2, n, n> {
   static EIGEN_DEVICE_FUNC  EIGEN_STRONG_INLINE bool run(Dims1& dims1, Dims2& dims2) {
-    return (array_get<n-1>(dims1) == array_get<n-1>(dims2)) &
+    return (array_get<n-1>(dims1) == array_get<n-1>(dims2)) &&
         sizes_match_below_dim<Dims1, Dims2, n-1, n-1>::run(dims1, dims2);
   }
 };
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorEvalTo.h b/unsupported/Eigen/CXX11/src/Tensor/TensorEvalTo.h
index 4689b0230..5cf29ddc3 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorEvalTo.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorEvalTo.h
@@ -134,11 +134,6 @@ struct TensorEvaluator<const TensorEvalToOp<ArgType, MakePointer_>, Device>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TensorEvaluator(const XprType& op, const Device& device)
       : m_impl(op.expression(), device), m_buffer(device.get(op.buffer())), m_expression(op.expression()){}
 
-
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE ~TensorEvaluator() {
-  }
-
-
   EIGEN_DEVICE_FUNC const Dimensions& dimensions() const { return m_impl.dimensions(); }
 
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool evalSubExprsIfNeeded(EvaluatorPointerType scalar) {
@@ -157,10 +152,10 @@ struct TensorEvaluator<const TensorEvalToOp<ArgType, MakePointer_>, Device>
   }
 #endif
 
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void evalScalar(Index i) {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void evalScalar(Index i) const {
     m_buffer[i] = m_impl.coeff(i);
   }
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void evalPacket(Index i) {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void evalPacket(Index i) const {
     internal::pstoret<CoeffReturnType, PacketReturnType, Aligned>(m_buffer + i, m_impl.template packet<TensorEvaluator<ArgType, Device>::IsAligned ? Aligned : Unaligned>(i));
   }
 
@@ -217,7 +212,7 @@ struct TensorEvaluator<const TensorEvalToOp<ArgType, MakePointer_>, Device>
   ArgType expression() const { return m_expression; }
   #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
     m_buffer.bind(cgh);
   }
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h b/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h
index d4532b72c..0fe7fb2d2 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h
@@ -24,6 +24,7 @@ namespace Eigen {
   */
 
 // Generic evaluator
+
 template<typename Derived, typename Device>
 struct TensorEvaluator
 {
@@ -41,7 +42,6 @@ struct TensorEvaluator
   // NumDimensions is -1 for variable dim tensors
   static const int NumCoords = internal::traits<Derived>::NumDimensions > 0 ?
                                internal::traits<Derived>::NumDimensions : 0;
-
   enum {
     IsAligned          = Derived::IsAligned,
     PacketAccess       = (PacketType<CoeffReturnType, Device>::size > 1),
@@ -96,7 +96,7 @@ struct TensorEvaluator
     return m_data[index];
   }
 
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType& coeffRef(Index index) {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType& coeffRef(Index index) const {
     eigen_assert(m_data != NULL);
     return m_data[index];
   }
@@ -120,8 +120,7 @@ struct TensorEvaluator
   }
 
   template <int StoreMode> EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
-  void writePacket(Index index, const PacketReturnType& x)
-  {
+  void writePacket(Index index, const PacketReturnType& x) const {
     return internal::pstoret<Scalar, PacketReturnType, StoreMode>(m_data + index, x);
   }
 
@@ -135,7 +134,7 @@ struct TensorEvaluator
   }
 
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType&
-  coeffRef(const array<DenseIndex, NumCoords>& coords) {
+  coeffRef(const array<DenseIndex, NumCoords>& coords) const {
     eigen_assert(m_data != NULL);
     if (static_cast<int>(Layout) == static_cast<int>(ColMajor)) {
       return m_data[m_dims.IndexOfColMajor(coords)];
@@ -182,7 +181,7 @@ struct TensorEvaluator
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_data.bind(cgh);
   }
 #endif
@@ -214,7 +213,7 @@ Eigen::half loadConstant(const Eigen::half* address) {
 #endif
 #ifdef EIGEN_USE_SYCL
 // overload of load constant should be implemented here based on range access
-template <cl::sycl::access::mode AcMd, typename T>
+template <sycl::access::mode AcMd, typename T>
 T &loadConstant(const Eigen::TensorSycl::internal::RangeAccess<AcMd, T> &address) {
   return *address;
 }
@@ -337,7 +336,7 @@ struct TensorEvaluator<const Derived, Device>
   EIGEN_DEVICE_FUNC EvaluatorPointerType data() const { return m_data; }
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_data.bind(cgh);
   }
 #endif
@@ -374,7 +373,7 @@ struct TensorEvaluator<const TensorCwiseNullaryOp<NullaryOp, ArgType>, Device>
   enum {
     IsAligned = true,
     PacketAccess = internal::functor_traits<NullaryOp>::PacketAccess
-    #ifdef EIGEN_USE_SYCL
+    #if  defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
     &&  (PacketType<CoeffReturnType, Device>::size >1)
     #endif
     ,
@@ -424,7 +423,7 @@ struct TensorEvaluator<const TensorCwiseNullaryOp<NullaryOp, ArgType>, Device>
 
 #ifdef EIGEN_USE_SYCL
    // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_argImpl.bind(cgh);
   }
 #endif
@@ -465,8 +464,9 @@ struct TensorEvaluator<const TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>
   typedef typename XprType::Scalar Scalar;
   typedef typename internal::remove_const<Scalar>::type ScalarNoConst;
   typedef typename internal::traits<XprType>::Scalar CoeffReturnType;
-  typedef typename PacketType<CoeffReturnType, Device>::type PacketReturnType;
-  static const int PacketSize = PacketType<CoeffReturnType, Device>::size;
+  typedef typename ArgType::Scalar InType;
+  typedef typename PacketType<const std::pair<CoeffReturnType, InType>, Device>::type PacketReturnType;
+  static const int PacketSize = PacketType<const std::pair<CoeffReturnType, InType>, Device>::size;
   typedef typename TensorEvaluator<ArgType, Device>::Dimensions Dimensions;
   typedef StorageMemory<CoeffReturnType, Device> Storage;
   typedef typename Storage::Type EvaluatorPointerType;
@@ -510,7 +510,7 @@ struct TensorEvaluator<const TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>
   template<int LoadMode>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE PacketReturnType packet(Index index) const
   {
-    return m_functor.packetOp(m_argImpl.template packet<LoadMode>(index));
+    return m_functor.template packetOp<PacketReturnType>(m_argImpl.template packet<LoadMode>(index));
   }
 
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TensorOpCost costPerCoeff(bool vectorized) const {
@@ -536,7 +536,7 @@ struct TensorEvaluator<const TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const{
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const{
     m_argImpl.bind(cgh);
   }
 #endif
@@ -584,8 +584,9 @@ struct TensorEvaluator<const TensorCwiseBinaryOp<BinaryOp, LeftArgType, RightArg
   typedef typename XprType::Index Index;
   typedef typename XprType::Scalar Scalar;
   typedef typename internal::traits<XprType>::Scalar CoeffReturnType;
-  typedef typename PacketType<CoeffReturnType, Device>::type PacketReturnType;
-  static const int PacketSize = PacketType<CoeffReturnType, Device>::size;
+  typedef typename internal::traits<LeftArgType>::Scalar InType;
+  typedef typename PacketType<const std::pair<CoeffReturnType, InType>, Device>::type PacketReturnType;
+  static const int PacketSize = PacketType<const std::pair<CoeffReturnType, InType>, Device>::size;
   typedef typename TensorEvaluator<LeftArgType, Device>::Dimensions Dimensions;
   typedef StorageMemory<CoeffReturnType, Device> Storage;
   typedef typename Storage::Type EvaluatorPointerType;
@@ -643,7 +644,7 @@ struct TensorEvaluator<const TensorCwiseBinaryOp<BinaryOp, LeftArgType, RightArg
   template<int LoadMode>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE PacketReturnType packet(Index index) const
   {
-    return m_functor.packetOp(m_leftImpl.template packet<LoadMode>(index), m_rightImpl.template packet<LoadMode>(index));
+    return m_functor.template packetOp<PacketReturnType>(m_leftImpl.template packet<LoadMode>(index), m_rightImpl.template packet<LoadMode>(index));
   }
 
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TensorOpCost
@@ -675,7 +676,7 @@ struct TensorEvaluator<const TensorCwiseBinaryOp<BinaryOp, LeftArgType, RightArg
 
   #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_leftImpl.bind(cgh);
     m_rightImpl.bind(cgh);
   }
@@ -789,7 +790,7 @@ struct TensorEvaluator<const TensorCwiseTernaryOp<TernaryOp, Arg1Type, Arg2Type,
 
 #ifdef EIGEN_USE_SYCL
    // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_arg1Impl.bind(cgh);
     m_arg2Impl.bind(cgh);
     m_arg3Impl.bind(cgh);
@@ -803,6 +804,72 @@ struct TensorEvaluator<const TensorCwiseTernaryOp<TernaryOp, Arg1Type, Arg2Type,
   TensorEvaluator<Arg3Type, Device> m_arg3Impl;
 };
 
+namespace {
+
+template <typename Expression, typename Device>
+typename std::enable_if<!internal::IsTensorBroadcastingOp<Expression>::value, bool>::type
+isScalarOrNot(const TensorEvaluator<Expression, Device>& expr) {
+  return false;
+}
+
+// If not BroadcastingOp, return -1.
+template <typename Expression, typename Device>
+typename std::enable_if<!internal::IsTensorBroadcastingOp<Expression>::value, int>::type
+getUniqueBroadcastDimensionSizeIsOne(const TensorEvaluator<Expression, Device>& expr) {
+  return -1;
+}
+
+template <typename Expression, typename Index, typename Device>
+typename std::enable_if<!internal::IsTensorBroadcastingOp<Expression>::value, Index>::type
+getInputStride(const TensorEvaluator<Expression, Device>& expr, int position) {
+  return 0;
+}
+
+template <typename Expression, typename Device>
+typename std::enable_if<internal::IsTensorBroadcastingOp<Expression>::value, bool>::type
+isScalarOrNot(const TensorEvaluator<Expression, Device>& expr) {
+  auto& input_dims = expr.inputDimensions();
+  bool is_scalar = true;
+  EIGEN_UNROLL_LOOP
+  for(int i = 0; i < TensorEvaluator<Expression, Device>::NumDims; ++i) {
+    if (input_dims[i] != 1) {
+      is_scalar = false;
+    }
+  }
+  return is_scalar;
+}
+
+// Get the only one broadcast dimension which size is one in BroadcastingOp. If conditions are not met, return -1.
+template <typename Expression, typename Device>
+typename std::enable_if<internal::IsTensorBroadcastingOp<Expression>::value, int>::type
+getUniqueBroadcastDimensionSizeIsOne(const TensorEvaluator<Expression, Device>& expr) {
+  auto& input_dims = expr.inputDimensions();
+  int position = -1;
+  static const int NumDims = TensorEvaluator<Expression, Device>::NumDims;
+  EIGEN_UNROLL_LOOP
+  for(int i = 0; i < NumDims; ++i) {
+    if (input_dims[i] == 1 && expr.getBroadcast(i) != 1) {
+      if (position == -1) {
+        position = i;
+      } else {
+        return -1;
+      }
+    }
+  }
+  return position;
+}
+
+template <typename Expression, typename Index, typename Device>
+typename std::enable_if<internal::IsTensorBroadcastingOp<Expression>::value, Index>::type
+getInputStride(const TensorEvaluator<Expression, Device>& expr, int position) {
+  if (position == -1) {
+    return 0;
+  }
+  return expr.getInputStride(position);
+}
+
+}; // namespace
+
 
 // -------------------- SelectOp --------------------
 
@@ -838,6 +905,15 @@ struct TensorEvaluator<const TensorSelectOp<IfArgType, ThenArgType, ElseArgType>
     EIGEN_STATIC_ASSERT((static_cast<int>(TensorEvaluator<IfArgType, Device>::Layout) == static_cast<int>(TensorEvaluator<ElseArgType, Device>::Layout)), YOU_MADE_A_PROGRAMMING_MISTAKE);
     eigen_assert(dimensions_match(m_condImpl.dimensions(), m_thenImpl.dimensions()));
     eigen_assert(dimensions_match(m_thenImpl.dimensions(), m_elseImpl.dimensions()));
+    isScalar = isScalarOrNot<IfArgType, Device>(m_condImpl);
+    // Get the only one broadcast dimension which size is one in BroadcastingOp.
+    // If conditions are not met, return -1.
+    position = getUniqueBroadcastDimensionSizeIsOne<IfArgType, Device>(m_condImpl);
+
+    Index inputStride = getInputStride<IfArgType, Index, Device>(m_condImpl, position);
+    if (inputStride != 0) {
+      inputDivisor = internal::TensorIntDivisor<Index>(inputStride);
+    }
   }
 
   typedef typename XprType::Index Index;
@@ -915,18 +991,84 @@ struct TensorEvaluator<const TensorSelectOp<IfArgType, ThenArgType, ElseArgType>
   {
     return m_condImpl.coeff(index) ? m_thenImpl.coeff(index) : m_elseImpl.coeff(index);
   }
+
+  template <typename Expression = IfArgType>
+  EIGEN_DEVICE_FUNC typename std::enable_if<internal::IsTensorBroadcastingOp<Expression>::value,
+                                            internal::Selector<PacketSize>>::type
+  getSelector(Index index) const
+  {
+    internal::Selector<PacketSize> select;
+    if (isScalar) {
+      select.select[0] = m_condImpl.coeffInputIndex(0);
+      EIGEN_UNROLL_LOOP
+      for (Index i = 1; i < PacketSize; ++i) {
+        select.select[i] = select.select[0];
+      }
+    } else if (m_condImpl.isCopyOrNot() || position == -1) {
+      EIGEN_UNROLL_LOOP
+      for (Index i = 0; i < PacketSize; ++i) {
+        select.select[i] = m_condImpl.coeff(index+i);
+      }
+    } else {
+      Index inputIndex = m_condImpl.getInputIndex(index);
+      Index inputStride = m_condImpl.getInputStride(position);
+      Index broadcast = m_condImpl.getBroadcast(position);
+      Index batchIndex = inputIndex / inputDivisor;
+      Index inputOffset = batchIndex * inputStride;
+      Index sliceIndex = inputIndex - inputOffset;
+      Index scale, pre_position;
+      static const int Layout = static_cast<int>(TensorEvaluator<IfArgType, Device>::Layout);
+      static const int NumDims = static_cast<int>(TensorEvaluator<IfArgType, Device>::NumDims);
+      if (Layout == static_cast<int>(ColMajor)) {
+        pre_position = (position == NumDims - 1 ? NumDims - 1 : position + 1);
+      } else {
+        pre_position = (position == 0 ? 0 : position - 1);
+      }
+      Index outputStride = m_condImpl.getOutputStride(pre_position);
+      scale = (index - batchIndex * outputStride) / inputDivisor;
+      EIGEN_UNROLL_LOOP
+      for (Index i = 0; i < PacketSize-1; ++i) {
+        select.select[i] = m_condImpl.coeffInputIndex(inputIndex);
+        if (sliceIndex + 1 < inputStride) {
+          ++inputIndex;
+          ++sliceIndex;
+        } else {
+          sliceIndex = 0;
+          if (scale + 1 < broadcast) {
+            inputIndex = inputOffset;
+            ++scale;
+          } else {
+            ++inputIndex;
+            inputOffset += inputStride;
+            scale = 0;
+          }
+        }
+      }
+      select.select[PacketSize-1] = m_condImpl.coeffInputIndex(inputIndex);
+    }
+    return select;
+  }
+
+  template <typename Expression = IfArgType>
+  EIGEN_DEVICE_FUNC typename std::enable_if<!internal::IsTensorBroadcastingOp<Expression>::value,
+                                            internal::Selector<PacketSize>>::type
+  getSelector(Index index) const {
+    internal::Selector<PacketSize> select;
+    EIGEN_UNROLL_LOOP
+    for (Index i = 0; i < PacketSize; ++i) {
+      select.select[i] = m_condImpl.coeff(index+i);
+    }
+    return select;
+  }
+
   template<int LoadMode>
   EIGEN_DEVICE_FUNC PacketReturnType packet(Index index) const
   {
-     internal::Selector<PacketSize> select;
-     EIGEN_UNROLL_LOOP
-     for (Index i = 0; i < PacketSize; ++i) {
-       select.select[i] = m_condImpl.coeff(index+i);
-     }
+     internal::Selector<PacketSize> select = getSelector(index);
+
      return internal::pblend(select,
                              m_thenImpl.template packet<LoadMode>(index),
                              m_elseImpl.template packet<LoadMode>(index));
-
   }
 
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TensorOpCost
@@ -966,13 +1108,21 @@ struct TensorEvaluator<const TensorSelectOp<IfArgType, ThenArgType, ElseArgType>
 
 #ifdef EIGEN_USE_SYCL
  // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_condImpl.bind(cgh);
     m_thenImpl.bind(cgh);
     m_elseImpl.bind(cgh);
   }
 #endif
  private:
+  // When m_condImpl is BroadcastingOp, represent whether the tensor of m_condImpl is scalar.
+  bool isScalar;
+  // When m_condImpl is BroadcastingOp, represent the only broadcast dimension which size is 1.
+  // If conditions are not met, position is -1.
+  int position;
+  // When position is not -1, represent fast divisor for input strides
+  // corresponding to position.
+  internal::TensorIntDivisor<Index> inputDivisor;
   TensorEvaluator<IfArgType, Device> m_condImpl;
   TensorEvaluator<ThenArgType, Device> m_thenImpl;
   TensorEvaluator<ElseArgType, Device> m_elseImpl;
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h b/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h
index c52fb77dc..18788d3fa 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h
@@ -76,8 +76,13 @@ struct ExpressionHasTensorBroadcastingOp<
  * Default strategy: the expression is evaluated sequentially with a single cpu
  * thread, without vectorization and block evaluation.
  */
+#if EIGEN_HAS_CXX11
 template <typename Expression, typename Device, bool Vectorizable,
           TiledEvaluation Tiling>
+#else
+template <typename Expression, typename Device, bool Vectorizable,
+          TiledEvaluation::TiledEvaluation Tiling>
+#endif
 class TensorExecutor {
  public:
   typedef typename Expression::Index StorageIndex;
@@ -325,17 +330,22 @@ template <typename Expression, bool Vectorizable, TiledEvaluation Tiling>
 class TensorExecutor<Expression, ThreadPoolDevice, Vectorizable, Tiling> {
  public:
   typedef typename Expression::Index StorageIndex;
-
+  // TODO(itex): Reopen ThreadPoolDevice dedicated vectorization path when it's ready.
+#ifdef EIGEN_USE_DPCPP
+  static const bool vectorized = false;
+#else
+  static const bool vectorized = Vectorizable;
+#endif
   static EIGEN_STRONG_INLINE void run(const Expression& expr,
                          const ThreadPoolDevice& device) {
     typedef TensorEvaluator<Expression, ThreadPoolDevice> Evaluator;
-    typedef EvalRange<Evaluator, StorageIndex, Vectorizable> EvalRange;
+    typedef EvalRange<Evaluator, StorageIndex, vectorized> EvalRange;
 
     Evaluator evaluator(expr, device);
     const bool needs_assign = evaluator.evalSubExprsIfNeeded(nullptr);
     if (needs_assign) {
       const StorageIndex size = array_prod(evaluator.dimensions());
-      device.parallelFor(size, evaluator.costPerCoeff(Vectorizable),
+      device.parallelFor(size, evaluator.costPerCoeff(vectorized),
                          EvalRange::alignBlockSize,
                          [&evaluator](StorageIndex firstIdx, StorageIndex lastIdx) {
                            EvalRange::run(&evaluator, firstIdx, lastIdx);
@@ -354,6 +364,12 @@ class TensorExecutor<Expression, ThreadPoolDevice, Vectorizable,
   typedef typename remove_const<Scalar>::type ScalarNoConst;
 
   static const int NumDims = traits<Expression>::NumDimensions;
+  // TODO: Reopen ThreadPoolDevice dedicated vectorization path when it's ready.
+#ifdef EIGEN_USE_DPCPP
+  static const bool vectorized = false;
+#else
+  static const bool vectorized = Vectorizable;
+#endif
 
   typedef TensorEvaluator<Expression, ThreadPoolDevice> Evaluator;
   typedef TensorBlockMapper<NumDims, Evaluator::Layout, IndexType> BlockMapper;
@@ -372,7 +388,7 @@ class TensorExecutor<Expression, ThreadPoolDevice, Vectorizable,
     if (needs_assign) {
       const TilingContext tiling =
           internal::GetTensorExecutorTilingContext<Evaluator, BlockMapper,
-                                                   Vectorizable>(evaluator);
+                                                   vectorized>(evaluator);
 
       auto eval_block = [&device, &evaluator, &tiling](IndexType firstBlockIdx,
                                                        IndexType lastBlockIdx) {
@@ -615,6 +631,86 @@ EIGEN_STRONG_INLINE void TensorExecutor<Expression, GpuDevice, Vectorizable, Til
 }
 
 #endif  // EIGEN_GPUCC
+
+#ifdef EIGEN_USE_DPCPP
+
+template <typename Evaluator>
+struct ExecExprFunctorKernel {
+  typedef typename Evaluator::Index Index;
+  typedef typename Evaluator::Scalar Scalar;
+  Evaluator evaluator;
+  const Index range;
+#ifndef EIGEN_DONT_VECTORIZE_SYCL
+  static const bool vectorized = Eigen::PacketType<Scalar, GpuDevice>::vectorized;
+#else
+  static const bool vectorized = false;
+#endif
+  template <typename Scratch>
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE ExecExprFunctorKernel(
+      const Scratch, Evaluator evaluator_, const Index range_)
+      : evaluator(evaluator_), range(range_) {}
+
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE void operator()(
+      sycl::nd_item<1> itemID) const {
+    compute(itemID);
+  }
+  template <bool is_vec = Evaluator::PacketAccess & vectorized>
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE typename std::enable_if<!is_vec>::type
+  compute(const sycl::nd_item<1>& itemID) const {
+    Index gId = static_cast<Index>(itemID.get_global_linear_id());
+    Index total_threads = itemID.get_global_range(0);
+    for (Index i = gId; i < range; i += total_threads) {
+      evaluator.evalScalar(i);
+    }
+  }
+  template <bool is_vec = Evaluator::PacketAccess & vectorized>
+  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE typename std::enable_if<is_vec>::type
+  compute(const sycl::nd_item<1>& itemID) const {
+    const Index vectorizedRange =
+        (range / Evaluator::PacketSize) * Evaluator::PacketSize;
+    Index gId = static_cast<Index>(itemID.get_global_linear_id());
+    Index total_threads = itemID.get_global_range(0);
+    const Index step = Evaluator::PacketSize * itemID.get_global_range(0);
+    const Index start = Evaluator::PacketSize * gId;
+    for (Index i = start; i < vectorizedRange; i += step) {
+      evaluator.evalPacket(i);
+    }
+    gId += vectorizedRange;
+    for (Index i = gId; i < range; i += total_threads) {
+      evaluator.evalScalar(i);
+    }
+  }
+};
+
+/*static*/
+template <typename Expression, bool Vectorizable, TiledEvaluation Tiling>
+EIGEN_STRONG_INLINE void TensorExecutor<Expression, GpuDevice, Vectorizable, Tiling>::run(
+  const Expression& expr, const GpuDevice& dev) {
+  typedef Eigen::TensorEvaluator<Expression, GpuDevice> Evaluator;
+  Evaluator evaluator(expr, dev);
+  const bool needs_assign = evaluator.evalSubExprsIfNeeded(NULL);
+  if (needs_assign) {
+    Index range, GRange, tileSize;
+    Index total_size = ::Eigen::internal::array_prod(evaluator.dimensions());
+    total_size = (total_size == 0) ? 1 : total_size;
+    const int PacketSize = evaluator.PacketSize;
+    Index vectorizable_threads = static_cast<Index>(total_size / PacketSize);
+    dev.parallel_for_setup(vectorizable_threads, tileSize, range, GRange);
+    range = total_size;
+
+    dev.template nullary_kernel_launcher<
+        typename Evaluator::CoeffReturnType,
+        ExecExprFunctorKernel<Evaluator> >(
+        evaluator,
+        sycl::nd_range<1>(sycl::range<1>(GRange),
+                              sycl::range<1>(tileSize)),
+        Index(1), range);
+  }
+  evaluator.cleanup();
+}
+
+#endif // EIGEN_USE_DPCPP
+
 #endif  // EIGEN_USE_GPU
 
 // SYCL Executor policy
@@ -624,19 +720,19 @@ template <typename Evaluator>
 struct ExecExprFunctorKernel {
   typedef typename Evaluator::Index Index;
   Evaluator evaluator;
-  const Index range;
+  const int range;
   template <typename Scratch>
   EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE ExecExprFunctorKernel(
-      const Scratch, Evaluator evaluator_, const Index range_)
+      const Scratch, Evaluator evaluator_, const int range_)
       : evaluator(evaluator_), range(range_) {}
 
   EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE void operator()(
-      cl::sycl::nd_item<1> itemID) {
+      sycl::nd_item<1> itemID) {
     compute(itemID);
   }
   template <bool is_vec = Evaluator::PacketAccess>
   EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE typename std::enable_if<!is_vec>::type
-  compute(const cl::sycl::nd_item<1>& itemID) {
+  compute(const sycl::nd_item<1>& itemID) {
     Index gId = static_cast<Index>(itemID.get_global_linear_id());
     Index total_threads = itemID.get_global_range(0);
 
@@ -646,7 +742,7 @@ struct ExecExprFunctorKernel {
   }
   template <bool is_vec = Evaluator::PacketAccess>
   EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE typename std::enable_if<is_vec>::type
-  compute(const cl::sycl::nd_item<1>& itemID) {
+  compute(const sycl::nd_item<1>& itemID) {
     const Index vectorizedRange =
         (range / Evaluator::PacketSize) * Evaluator::PacketSize;
     Index gId = static_cast<Index>(itemID.get_global_linear_id());
@@ -686,8 +782,8 @@ class TensorExecutor<Expression, Eigen::SyclDevice, Vectorizable, Tiling> {
           typename Evaluator::CoeffReturnType,
           ExecExprFunctorKernel<Evaluator> >(
           evaluator,
-          cl::sycl::nd_range<1>(cl::sycl::range<1>(GRange),
-                                cl::sycl::range<1>(tileSize)),
+          sycl::nd_range<1>(sycl::range<1>(GRange),
+                                sycl::range<1>(tileSize)),
           Index(1), range);
     }
     evaluator.cleanup();
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h b/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h
index c62bc5fa9..c7e333215 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h
@@ -207,7 +207,7 @@ struct TensorEvaluator<const TensorFFTOp<FFT, ArgType, FFTResultType, FFTDir>, D
   EIGEN_DEVICE_FUNC EvaluatorPointerType data() const { return m_data; }
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_data.bind(cgh);
   }
 #endif
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorForcedEval.h b/unsupported/Eigen/CXX11/src/Tensor/TensorForcedEval.h
index 14020aa68..558869176 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorForcedEval.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorForcedEval.h
@@ -222,7 +222,7 @@ struct TensorEvaluator<const TensorForcedEvalOp<ArgType_>, Device>
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_buffer.bind(cgh);
     m_impl.bind(cgh);
   }
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorForwardDeclarations.h b/unsupported/Eigen/CXX11/src/Tensor/TensorForwardDeclarations.h
index 246ebe44e..36f2bab2c 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorForwardDeclarations.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorForwardDeclarations.h
@@ -107,12 +107,12 @@ struct SyclDevice;
 #ifdef EIGEN_USE_SYCL
 
 template <typename T> struct MakeSYCLPointer {
-  typedef Eigen::TensorSycl::internal::RangeAccess<cl::sycl::access::mode::read_write, T> Type;
+  typedef Eigen::TensorSycl::internal::RangeAccess<sycl::access::mode::read_write, T> Type;
 };
 
 template <typename T>
-EIGEN_STRONG_INLINE const Eigen::TensorSycl::internal::RangeAccess<cl::sycl::access::mode::read_write, T>&
-constCast(const Eigen::TensorSycl::internal::RangeAccess<cl::sycl::access::mode::read_write, T>& data) {
+EIGEN_STRONG_INLINE const Eigen::TensorSycl::internal::RangeAccess<sycl::access::mode::read_write, T>&
+constCast(const Eigen::TensorSycl::internal::RangeAccess<sycl::access::mode::read_write, T>& data) {
   return data;
 }
 
@@ -128,6 +128,12 @@ template <typename Evaluator, typename Op> class GenericNondeterministicReducer;
 }
 #endif
 
+#if defined(EIGEN_USE_DPCPP)
+namespace internal{
+template <typename Evaluator, typename Op> class GenericRandomReducer;
+}
+#endif // EIGEN_USE_DPCPP
+
 
 enum FFTResultType {
   RealPart = 0,
@@ -154,6 +160,30 @@ struct IsVectorizable<GpuDevice, Expression> {
                             TensorEvaluator<Expression, GpuDevice>::IsAligned;
 };
 
+template <typename Expression>
+struct IsTensorBroadcastingOp {
+  static const bool value = false;
+};
+
+template <typename Broadcast, typename XprType>
+struct IsTensorBroadcastingOp<const TensorBroadcastingOp<Broadcast, XprType> > {
+  static const bool value = true;
+};
+
+template <typename Expression>
+struct IsOperator {
+  static const bool value = true;
+};
+
+template <typename Expression>
+struct IsOperator<const Expression>
+  : IsOperator<Expression> {};
+
+template <typename PlainObjectType, int Options_, template <class> class MakePointer_>
+struct IsOperator<TensorMap<PlainObjectType, Options_, MakePointer_>> {
+  static const bool value = false;
+};
+
 // Tiled evaluation strategy.
 enum TiledEvaluation {
   Off = 0,    // tiled evaluation is not supported
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorFunctors.h b/unsupported/Eigen/CXX11/src/Tensor/TensorFunctors.h
index 2edc45f1a..cd0603760 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorFunctors.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorFunctors.h
@@ -309,7 +309,7 @@ template <typename T, typename Device>
 struct reducer_traits<ProdReducer<T>, Device> {
   enum {
     Cost = NumTraits<T>::MulCost,
-    PacketAccess = PacketType<T, Device>::HasMul,
+    PacketAccess = false,
     IsStateful = false,
     IsExactlyAssociative = true
   };
@@ -367,7 +367,8 @@ struct reducer_traits<OrReducer, Device> {
 template <typename T> struct ArgMaxTupleReducer
 {
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void reduce(const T t, T* accum) const {
-    if (t.second > accum->second) { *accum = t; }
+    if (t.second > accum->second || (t.second == accum->second && t.first < accum->first))
+      { *accum = t; }
   }
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE T initialize() const {
     return T(0, NumTraits<typename T::second_type>::lowest());
@@ -391,7 +392,8 @@ struct reducer_traits<ArgMaxTupleReducer<T>, Device> {
 template <typename T> struct ArgMinTupleReducer
 {
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void reduce(const T& t, T* accum) const {
-    if (t.second < accum->second) { *accum = t; }
+    if (t.second < accum->second || (t.second == accum->second && t.first < accum->first))
+      { *accum = t; }
   }
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE T initialize() const {
     return T(0, NumTraits<typename T::second_type>::highest());
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorGenerator.h b/unsupported/Eigen/CXX11/src/Tensor/TensorGenerator.h
index b1ff1d8b1..54cfd1907 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorGenerator.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorGenerator.h
@@ -267,7 +267,7 @@ struct TensorEvaluator<const TensorGeneratorOp<Generator, ArgType>, Device>
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler&) const {}
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler&) const {}
 #endif
 
  protected:
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorGpuHipCudaDefines.h b/unsupported/Eigen/CXX11/src/Tensor/TensorGpuHipCudaDefines.h
index f32ce27e9..fb704cdf4 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorGpuHipCudaDefines.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorGpuHipCudaDefines.h
@@ -49,6 +49,132 @@
 #define gpuDeviceSynchronize hipDeviceSynchronize
 #define gpuMemcpy hipMemcpy
 
+#elif defined(EIGEN_USE_DPCPP)
+
+
+inline const char *cudaGetErrorString(int error){
+  static char error_msg[] = "not implemented";
+  return error_msg;
+}
+
+struct gpuDeviceProp_t{
+  int multiProcessorCount;
+  int maxThreadsPerBlock;
+  int maxThreadsPerMultiProcessor;
+  int sharedMemPerBlock;
+  int major;
+  int minor;
+  sycl::info::local_mem_type local_mem_type;
+};
+
+
+class DefaultQueue {
+public:
+  DefaultQueue () {}
+  static sycl::queue* getInstance() {
+    static sycl::queue default_queue = sycl::queue();
+    return &default_queue;
+  }
+};
+
+
+
+enum dpruntimeMemcpyKind { 
+  dpruntimeMemcpyHostToDevice,
+  dpruntimeMemcpyDeviceToHost,
+  dpruntimeMemcpyDeviceToDevice
+};
+
+#define cudaMemcpyDeviceToDevice  3
+#define cudaMemcpyDeviceToHost    2
+#define cudaMemcpyHostToDevice    1
+
+#define cudaStreamDefault  DefaultQueue::getInstance()
+
+using  gpuStream_t    =  dpruntimeStream_t;
+#define gpuError_t       ITEX_GPUError_t
+#define gpuSuccess       ITEX_GPU_SUCCESS
+#define gpuErrorNotReady cudaErrorNotReady
+#define gpuGetDeviceCount dpruntimeGetDeviceCount
+#define gpuGetErrorString cudaGetErrorString
+#define gpuStreamDefault cudaStreamDefault
+#define gpuGetDevice dpruntimeGetCurrentDevice
+#define gpuSetDevice dpruntimeSetDevice
+#define gpuMemsetAsync dpruntimeMemsetAsync
+#define gpuMemcpyAsync dpruntimeMemcpyAsync
+#define gpuMemcpyDeviceToDevice dpruntimeMemcpyDeviceToDevice
+#define gpuMemcpyDeviceToHost dpruntimeMemcpyDeviceToHost
+#define gpuMemcpyHostToDevice dpruntimeMemcpyHostToDevice
+#define gpuStreamQuery cudaStreamQuery
+#define gpuSharedMemConfig cudaSharedMemConfig
+#define gpuDeviceSetSharedMemConfig cudaDeviceSetSharedMemConfig
+#define gpuStreamSynchronize dpruntimeStreamSynchronize
+#define gpuDeviceSynchronize cudaDeviceSynchronize
+#define gpuMemcpy dpruntimeMemcpy
+
+inline ITEX_GPUError_t gpuGetDeviceProperties(gpuDeviceProp_t * devProp, int device){
+  ITEX_GPUdevice_st dpcppDevice;
+  dpruntimeGetRealDPCPPDevice(&dpcppDevice, device);
+  devProp->multiProcessorCount = dpcppDevice.template get_info<sycl::info::device::max_compute_units>();
+  devProp->maxThreadsPerBlock = dpcppDevice.template get_info<sycl::info::device::max_work_group_size>();
+  devProp->maxThreadsPerMultiProcessor = dpcppDevice.template get_info<sycl::info::device::max_work_group_size>();
+  devProp->sharedMemPerBlock = dpcppDevice.template get_info<sycl::info::device::local_mem_size>();
+  devProp->major = 0;
+  devProp->minor = 0;
+  devProp->local_mem_type = dpcppDevice.template get_info<sycl::info::device::local_mem_type>();
+  return ITEX_GPU_SUCCESS;
+}
+
+
+inline ITEX_GPUError_t gpuMalloc(void** devPtr, size_t size) {
+  *devPtr = dpruntimeMalloc(size);
+  return ITEX_GPU_SUCCESS;
+}
+
+inline ITEX_GPUError_t gpuFree(void* devPtr) {
+  dpruntimeFree(devPtr);
+  return ITEX_GPU_SUCCESS;
+}
+
+inline ITEX_GPUError_t dpruntimeMemsetAsync(void *devPtr, int value, size_t count,
+                     const gpuStream_t stream)
+{
+  return dpruntimeMemsetD8Async(devPtr, value, count, stream);
+}
+
+inline ITEX_GPUError_t dpruntimeMemcpyAsync(void *dst, const void *src, size_t count, int kind,
+                   const gpuStream_t stream)
+{
+  switch (kind) {
+    case dpruntimeMemcpyDeviceToDevice:
+      return dpruntimeMemcpyDtoDAsync(dst, src, count, stream);
+    case dpruntimeMemcpyDeviceToHost:
+      return dpruntimeMemcpyDtoHAsync(dst, src, count, stream);
+    case dpruntimeMemcpyHostToDevice:
+      return dpruntimeMemcpyHtoDAsync(dst, src, count, stream);
+    default:
+      eigen_assert(((kind == dpruntimeMemcpyDeviceToDevice) || (kind == dpruntimeMemcpyDeviceToHost) || (kind == dpruntimeMemcpyHostToDevice)));
+  }
+  return ITEX_GPU_SUCCESS;
+}
+
+
+inline ITEX_GPUError_t dpruntimeMemcpy(void *dst, const void *src, size_t count, int kind)
+{
+  switch (kind) {
+    case dpruntimeMemcpyDeviceToDevice:
+      return dpruntimeMemcpyDtoD(dst, src, count);
+    case dpruntimeMemcpyDeviceToHost:
+      return dpruntimeMemcpyDtoH(dst, src, count);
+    case gpuMemcpyHostToDevice:
+      return dpruntimeMemcpyHtoD(dst, src, count);
+    default:
+      eigen_assert(((kind == dpruntimeMemcpyDeviceToDevice) || (kind == dpruntimeMemcpyDeviceToHost) || (kind == dpruntimeMemcpyHostToDevice)));
+  }
+  return ITEX_GPU_SUCCESS;
+}
+
+
 #else
 
 #define gpuStream_t cudaStream_t
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h b/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h
index 49d1004f3..4a7ba3b83 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h
@@ -509,7 +509,7 @@ struct TensorEvaluator<const TensorImagePatchOp<Rows, Cols, ArgType>, Device>
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorIndexList.h b/unsupported/Eigen/CXX11/src/Tensor/TensorIndexList.h
index 0e9133c49..d7667c6fb 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorIndexList.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorIndexList.h
@@ -468,7 +468,7 @@ struct index_statically_eq_impl {
 template <typename FirstType, typename... OtherTypes>
 struct index_statically_eq_impl<IndexList<FirstType, OtherTypes...> > {
   EIGEN_DEVICE_FUNC static constexpr bool run(const Index i, const Index value) {
-    return IndexList<FirstType, OtherTypes...>().value_known_statically(i) &
+    return IndexList<FirstType, OtherTypes...>().value_known_statically(i) &&
         (IndexList<FirstType, OtherTypes...>().get(i) == value);
   }
 };
@@ -476,7 +476,7 @@ struct index_statically_eq_impl<IndexList<FirstType, OtherTypes...> > {
 template <typename FirstType, typename... OtherTypes>
 struct index_statically_eq_impl<const IndexList<FirstType, OtherTypes...> > {
   EIGEN_DEVICE_FUNC static constexpr bool run(const Index i, const Index value) {
-    return IndexList<FirstType, OtherTypes...>().value_known_statically(i) &
+    return IndexList<FirstType, OtherTypes...>().value_known_statically(i) &&
         (IndexList<FirstType, OtherTypes...>().get(i) == value);
   }
 };
@@ -492,7 +492,7 @@ struct index_statically_ne_impl {
 template <typename FirstType, typename... OtherTypes>
 struct index_statically_ne_impl<IndexList<FirstType, OtherTypes...> > {
   EIGEN_DEVICE_FUNC static constexpr bool run(const Index i, const Index value) {
-    return IndexList<FirstType, OtherTypes...>().value_known_statically(i) &
+    return IndexList<FirstType, OtherTypes...>().value_known_statically(i) &&
         (IndexList<FirstType, OtherTypes...>().get(i) != value);
   }
 };
@@ -541,7 +541,7 @@ struct index_statically_lt_impl {
 template <typename FirstType, typename... OtherTypes>
 struct index_statically_lt_impl<IndexList<FirstType, OtherTypes...> > {
   EIGEN_DEVICE_FUNC static constexpr bool run(const Index i, const Index value) {
-    return IndexList<FirstType, OtherTypes...>().value_known_statically(i) &
+    return IndexList<FirstType, OtherTypes...>().value_known_statically(i) &&
         (IndexList<FirstType, OtherTypes...>().get(i) < value);
   }
 };
@@ -549,7 +549,7 @@ struct index_statically_lt_impl<IndexList<FirstType, OtherTypes...> > {
 template <typename FirstType, typename... OtherTypes>
 struct index_statically_lt_impl<const IndexList<FirstType, OtherTypes...> > {
   EIGEN_DEVICE_FUNC static constexpr bool run(const Index i, const Index value) {
-    return IndexList<FirstType, OtherTypes...>().value_known_statically(i) &
+    return IndexList<FirstType, OtherTypes...>().value_known_statically(i) &&
         (IndexList<FirstType, OtherTypes...>().get(i) < value);
   }
 };
@@ -566,7 +566,7 @@ struct index_pair_first_statically_eq_impl {
 template <typename FirstType, typename... OtherTypes>
 struct index_pair_first_statically_eq_impl<IndexPairList<FirstType, OtherTypes...> > {
   EIGEN_DEVICE_FUNC static constexpr bool run(const Index i, const Index value) {
-    return IndexPairList<FirstType, OtherTypes...>().value_known_statically(i) &
+    return IndexPairList<FirstType, OtherTypes...>().value_known_statically(i) &&
         (IndexPairList<FirstType, OtherTypes...>().operator[](i).first == value);
   }
 };
@@ -574,7 +574,7 @@ struct index_pair_first_statically_eq_impl<IndexPairList<FirstType, OtherTypes..
 template <typename FirstType, typename... OtherTypes>
 struct index_pair_first_statically_eq_impl<const IndexPairList<FirstType, OtherTypes...> > {
   EIGEN_DEVICE_FUNC static constexpr bool run(const Index i, const Index value) {
-    return IndexPairList<FirstType, OtherTypes...>().value_known_statically(i) &
+    return IndexPairList<FirstType, OtherTypes...>().value_known_statically(i) &&
         (IndexPairList<FirstType, OtherTypes...>().operator[](i).first == value);
   }
 };
@@ -591,7 +591,7 @@ struct index_pair_second_statically_eq_impl {
 template <typename FirstType, typename... OtherTypes>
 struct index_pair_second_statically_eq_impl<IndexPairList<FirstType, OtherTypes...> > {
   EIGEN_DEVICE_FUNC static constexpr bool run(const Index i, const Index value) {
-    return IndexPairList<FirstType, OtherTypes...>().value_known_statically(i) &
+    return IndexPairList<FirstType, OtherTypes...>().value_known_statically(i) &&
         (IndexPairList<FirstType, OtherTypes...>().operator[](i).second == value);
   }
 };
@@ -599,7 +599,7 @@ struct index_pair_second_statically_eq_impl<IndexPairList<FirstType, OtherTypes.
 template <typename FirstType, typename... OtherTypes>
 struct index_pair_second_statically_eq_impl<const IndexPairList<FirstType, OtherTypes...> > {
   EIGEN_DEVICE_FUNC static constexpr bool run(const Index i, const Index value) {
-    return IndexPairList<FirstType, OtherTypes...>().value_known_statically(i) &
+    return IndexPairList<FirstType, OtherTypes...>().value_known_statically(i) &&
         (IndexPairList<FirstType, OtherTypes...>().operator[](i).second == value);
   }
 };
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorInflation.h b/unsupported/Eigen/CXX11/src/Tensor/TensorInflation.h
index 7dadec7fb..8c4cf4821 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorInflation.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorInflation.h
@@ -106,32 +106,10 @@ struct TensorEvaluator<const TensorInflationOp<Strides, ArgType>, Device>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TensorEvaluator(const XprType& op, const Device& device)
       : m_impl(op.expression(), device), m_strides(op.strides())
   {
-    m_dimensions = m_impl.dimensions();
+    in_dimensions = m_impl.dimensions();
     // Expand each dimension to the inflated dimension.
     for (int i = 0; i < NumDims; ++i) {
-      m_dimensions[i] = (m_dimensions[i] - 1) * op.strides()[i] + 1;
-    }
-
-    // Remember the strides for fast division.
-    for (int i = 0; i < NumDims; ++i) {
-      m_fastStrides[i] = internal::TensorIntDivisor<Index>(m_strides[i]);
-    }
-
-    const typename TensorEvaluator<ArgType, Device>::Dimensions& input_dims = m_impl.dimensions();
-    if (static_cast<int>(Layout) == static_cast<int>(ColMajor)) {
-      m_outputStrides[0] = 1;
-      m_inputStrides[0] = 1;
-      for (int i = 1; i < NumDims; ++i) {
-        m_outputStrides[i] = m_outputStrides[i-1] * m_dimensions[i-1];
-        m_inputStrides[i] = m_inputStrides[i-1] * input_dims[i-1];
-      }
-    } else {  // RowMajor
-      m_outputStrides[NumDims-1] = 1;
-      m_inputStrides[NumDims-1] = 1;
-      for (int i = NumDims - 2; i >= 0; --i) {
-        m_outputStrides[i] = m_outputStrides[i+1] * m_dimensions[i+1];
-        m_inputStrides[i] = m_inputStrides[i+1] * input_dims[i+1];
-      }
+      m_dimensions[i] = (in_dimensions[i] - 1) * op.strides()[i] + 1;
     }
   }
 
@@ -150,36 +128,27 @@ struct TensorEvaluator<const TensorInflationOp<Strides, ArgType>, Device>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool getInputIndex(Index index, Index* inputIndex) const
   {
     eigen_assert(index < dimensions().TotalSize());
-    *inputIndex = 0;
+    Index tmp = 1;
     if (static_cast<int>(Layout) == static_cast<int>(ColMajor)) {
       EIGEN_UNROLL_LOOP
-      for (int i = NumDims - 1; i > 0; --i) {
-        const Index idx = index / m_outputStrides[i];
-        if (idx != idx / m_fastStrides[i] * m_strides[i]) {
-          return false;
-        }
-        *inputIndex += idx / m_strides[i] * m_inputStrides[i];
-        index -= idx * m_outputStrides[i];
-      }
-      if (index != index / m_fastStrides[0] * m_strides[0]) {
-        return false;
+      for(int i = 0;i < NumDims; ++i){
+        const Index idx = index % m_dimensions[i];
+        if(idx % m_strides[i] != 0) return false;
+
+        *inputIndex += idx / m_strides[i] * tmp;
+        index /= m_dimensions[i];
+        tmp *= in_dimensions[i];
       }
-      *inputIndex += index / m_strides[0];
-      return true;
     } else {
       EIGEN_UNROLL_LOOP
-      for (int i = 0; i < NumDims - 1; ++i) {
-        const Index idx = index / m_outputStrides[i];
-        if (idx != idx / m_fastStrides[i] * m_strides[i]) {
-          return false;
-        }
-        *inputIndex += idx / m_strides[i] * m_inputStrides[i];
-        index -= idx * m_outputStrides[i];
-      }
-      if (index != index / m_fastStrides[NumDims-1] * m_strides[NumDims-1]) {
-        return false;
+      for(int i = NumDims - 1;i >= 0; --i){
+        const Index idx = index % m_dimensions[i];
+        if(idx % m_strides[i] != 0) return false;
+
+        *inputIndex += idx / m_strides[i] * tmp;
+        index /= m_dimensions[i];
+        tmp *= in_dimensions[i];
       }
-      *inputIndex += index / m_strides[NumDims - 1];
     }
     return true;
   }
@@ -187,11 +156,7 @@ struct TensorEvaluator<const TensorInflationOp<Strides, ArgType>, Device>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType coeff(Index index) const
   {
     Index inputIndex = 0;
-    if (getInputIndex(index, &inputIndex)) {
-     return m_impl.coeff(inputIndex);
-    } else {
-     return Scalar(0);
-    }
+    return getInputIndex(index, &inputIndex) ? m_impl.coeff(inputIndex) : Scalar(0);
   }
 
   // TODO(yangke): optimize this function so that we can detect and produce
@@ -228,18 +193,16 @@ struct TensorEvaluator<const TensorInflationOp<Strides, ArgType>, Device>
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
 
  protected:
+  Dimensions in_dimensions;
   Dimensions m_dimensions;
-  array<Index, NumDims> m_outputStrides;
-  array<Index, NumDims> m_inputStrides;
   TensorEvaluator<ArgType, Device> m_impl;
   const Strides m_strides;
-  array<internal::TensorIntDivisor<Index>, NumDims> m_fastStrides;
 };
 
 } // end namespace Eigen
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorIntDiv.h b/unsupported/Eigen/CXX11/src/Tensor/TensorIntDiv.h
index 6d5cce4aa..50f0f5ab7 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorIntDiv.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorIntDiv.h
@@ -38,7 +38,7 @@ namespace {
 #ifdef EIGEN_GPU_COMPILE_PHASE
     return __clz(val);
 #elif defined(SYCL_DEVICE_ONLY)
-    return cl::sycl::clz(val);
+    return sycl::clz(val);
 #elif EIGEN_COMP_MSVC
     unsigned long index;
     _BitScanReverse(&index, val);
@@ -56,7 +56,7 @@ namespace {
 #ifdef EIGEN_GPU_COMPILE_PHASE
     return __clzll(val);
 #elif defined(SYCL_DEVICE_ONLY)
-    return static_cast<int>(cl::sycl::clz(val));
+    return static_cast<int>(sycl::clz(val));
 #elif EIGEN_COMP_MSVC && EIGEN_ARCH_x86_64
     unsigned long index;
     _BitScanReverse64(&index, val);
@@ -93,7 +93,7 @@ namespace {
 #if defined(EIGEN_GPU_COMPILE_PHASE)
     return __umulhi(a, b);
 #elif defined(SYCL_DEVICE_ONLY)
-    return cl::sycl::mul_hi(a, static_cast<uint32_t>(b));
+    return sycl::mul_hi(a, static_cast<uint32_t>(b));
 #else
     return (static_cast<uint64_t>(a) * b) >> 32;
 #endif
@@ -104,7 +104,7 @@ namespace {
 #if defined(EIGEN_GPU_COMPILE_PHASE)
     return __umul64hi(a, b);
 #elif defined(SYCL_DEVICE_ONLY)
-    return cl::sycl::mul_hi(a, static_cast<uint64_t>(b));
+    return sycl::mul_hi(a, static_cast<uint64_t>(b));
 #elif EIGEN_HAS_BUILTIN_INT128
     __uint128_t v = static_cast<__uint128_t>(a) * static_cast<__uint128_t>(b);
     return static_cast<uint64_t>(v >> 64);
@@ -206,7 +206,7 @@ class TensorIntDivisor<int32_t, true> {
 #ifdef EIGEN_GPU_COMPILE_PHASE
     return (__umulhi(magic, n) >> shift);
 #elif defined(SYCL_DEVICE_ONLY)
-    return (cl::sycl::mul_hi(magic, static_cast<uint32_t>(n)) >> shift);
+    return (sycl::mul_hi(magic, static_cast<uint32_t>(n)) >> shift);
 #else
     uint64_t v = static_cast<uint64_t>(magic) * static_cast<uint64_t>(n);
     return (static_cast<uint32_t>(v >> 32) >> shift);
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorLayoutSwap.h b/unsupported/Eigen/CXX11/src/Tensor/TensorLayoutSwap.h
index 05fa80e59..c12d1ce41 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorLayoutSwap.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorLayoutSwap.h
@@ -140,7 +140,7 @@ struct TensorEvaluator<const TensorLayoutSwapOp<ArgType>, Device>
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorMacros.h b/unsupported/Eigen/CXX11/src/Tensor/TensorMacros.h
index af9e5db70..97e9d0dfc 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorMacros.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorMacros.h
@@ -60,7 +60,9 @@
 #endif
 
 // Define a macro to use a reference on the host but a value on the device
-#if defined(SYCL_DEVICE_ONLY)
+#if defined(EIGEN_USE_DPCPP)
+  #define EIGEN_DEVICE_REF
+#elif defined(SYCL_DEVICE_ONLY)
   #define EIGEN_DEVICE_REF
 #else
   #define EIGEN_DEVICE_REF &
@@ -70,7 +72,7 @@
 #define EIGEN_SYCL_TRY_CATCH(X) \
   do { \
     EIGEN_TRY {X;} \
-    EIGEN_CATCH(const cl::sycl::exception& e) { \
+    EIGEN_CATCH(const sycl::exception& e) { \
       EIGEN_THROW_X(std::runtime_error("SYCL exception at " + \
                                        std::string(__FILE__) + ":" + \
                                        std::to_string(__LINE__) + "\n" + \
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorMeta.h b/unsupported/Eigen/CXX11/src/Tensor/TensorMeta.h
index a3a750f21..1b14e3b01 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorMeta.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorMeta.h
@@ -51,6 +51,9 @@ struct PacketType : internal::packet_traits<Scalar> {
   typedef typename internal::packet_traits<Scalar>::type type;
 };
 
+template<typename OutScalar, typename InScalar, typename Device>
+struct PacketType<const std::pair<OutScalar, InScalar>, Device> : PacketType<OutScalar, Device> {};
+
 // For CUDA packet types when using a GpuDevice
 #if defined(EIGEN_USE_GPU) && defined(EIGEN_HAS_GPU_FP16)
 
@@ -86,9 +89,11 @@ struct PacketType<half, GpuDevice> {
 };
 #endif
 
-#if defined(EIGEN_USE_SYCL)
+#if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
 
+#ifndef EIGEN_USE_DPCPP
 namespace TensorSycl {
+#endif
 namespace internal {
 
 template <typename Index, Index A, Index B> struct PlusOp {
@@ -133,10 +138,17 @@ static EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Index roundUp(Index x, Index y) {
 }
 
 } // namespace internal
+#ifndef EIGEN_USE_DPCPP
 } // namespace TensorSycl
+#endif
 
+#ifdef EIGEN_DONT_VECTORIZE_SYCL
 template <>
+#ifndef EIGEN_USE_DPCPP
   struct PacketType<half, SyclDevice> {
+#else
+  struct PacketType<half, GpuDevice> {
+#endif
   typedef half type;
   static const int size = 1;
   enum {
@@ -155,7 +167,11 @@ template <>
   };
 };
 template <typename Scalar>
+#ifndef EIGEN_USE_DPCPP
 struct PacketType<Scalar, SyclDevice> : internal::default_packet_traits {
+#else
+struct PacketType<Scalar, GpuDevice> : internal::default_packet_traits {
+#endif
   typedef Scalar type;
   typedef Scalar half;
   enum {
@@ -178,19 +194,27 @@ struct PacketType<Scalar, SyclDevice> : internal::default_packet_traits {
   };
 
 };
+#endif
 
 template <typename Scalar>
+#ifndef EIGEN_USE_DPCPP
 struct PacketType<Scalar, const SyclDevice> : PacketType<Scalar, SyclDevice>{};
+#else
+struct PacketType<Scalar, const GpuDevice> : PacketType<Scalar, GpuDevice>{};
+
+template<typename OutScalar, typename InScalar>
+struct PacketType<const std::pair<OutScalar, InScalar>, GpuDevice> : PacketType<OutScalar, GpuDevice> {};
+#endif
 
 #ifndef EIGEN_DONT_VECTORIZE_SYCL
-#define PACKET_TYPE(CVQual, Type, val, lengths, DEV)\
-template<> struct PacketType<CVQual Type, DEV> : internal::sycl_packet_traits<val, lengths> \
-{\
-  typedef typename internal::packet_traits<Type>::type type;\
-  typedef typename internal::packet_traits<Type>::half half;\
+#ifdef EIGEN_USE_SYCL
+#define PACKET_TYPE(CVQual, Type, val, lengths, DEV)                                         \
+template<> struct PacketType<CVQual Type, DEV> : internal::dpcpp_packet_traits<val, lengths> \
+{                                                                                            \
+  typedef typename internal::packet_traits<Type>::type type;                                 \
+  typedef typename internal::packet_traits<Type>::half half;                                 \
 };
 
-
 PACKET_TYPE(const, float, 1, 4, SyclDevice)
 PACKET_TYPE(, float, 1, 4, SyclDevice)
 PACKET_TYPE(const, float, 1, 4, const SyclDevice)
@@ -201,11 +225,101 @@ PACKET_TYPE(, double, 0, 2, SyclDevice)
 PACKET_TYPE(const, double, 0, 2, const SyclDevice)
 PACKET_TYPE(, double, 0, 2, const SyclDevice)
 #undef PACKET_TYPE
+#endif // EIGEN_USE_SYCL
+
+#ifdef EIGEN_USE_DPCPP
+#define PACKET_TYPE(CVQual, Type, lengths, DEV, IsVectorized)                                         \
+template<> struct PacketType<CVQual Type, DEV> : internal::dpcpp_packet_traits<IsVectorized, lengths> \
+{                                                                                                     \
+  typedef typename internal::packet_traits<Type>::type type;                                          \
+  typedef typename internal::packet_traits<Type>::half half;                                          \
+  static const bool vectorized = IsVectorized;                                                        \
+};
 
-template<> struct PacketType<half, const SyclDevice>: PacketType<half, SyclDevice>{};
-template<> struct PacketType<const half, const SyclDevice>: PacketType<half, SyclDevice>{};
-#endif
-#endif
+#define PACKET_TYPE_FOR_BOOL(DEV, IsVectorized)                                     \
+template<typename InScalar>                                                         \
+struct PacketType<const std::pair<bool, InScalar>, DEV> : PacketType<InScalar, DEV> \
+{                                                                                   \
+  typedef typename sycl::vec<bool, PacketType<InScalar, DEV>::size> type;       \
+  typedef typename sycl::vec<bool, PacketType<InScalar, DEV>::size> half;       \
+  static const bool vectorized = IsVectorized;                                      \
+};
+
+#define PACKET_TYPE_FOR_HALF8(CVQual, TYPE, DEV, IsVectorized)                       \
+typedef sycl::ulonglong2 Packet4h2;                                                  \
+template <>                                                                          \
+struct PacketType<CVQual TYPE, DEV> : internal::dpcpp_packet_traits<IsVectorized, 8> \
+{                                                                                    \
+  typedef Packet4h2 type;                                                            \
+  static const int size = 8;                                                         \
+  static const bool vectorized = IsVectorized;                                       \
+};
+
+#define PACKET_TYPE_FOR_BF16_8(CVQual, TYPE, DEV, IsVectorized)                      \
+typedef sycl::vec<unsigned short, 8> Packet8bf16;                                    \
+template <>                                                                          \
+struct PacketType<CVQual TYPE, DEV> : internal::dpcpp_packet_traits<IsVectorized, 8> \
+{                                                                                    \
+  typedef Packet8bf16 type;                                                          \
+  static const int size = 8;                                                         \
+  static const bool vectorized = IsVectorized;                                       \
+};
+
+#ifdef DPCPP_DEVICE_ONLY
+PACKET_TYPE(const, float, 4, GpuDevice, true)
+PACKET_TYPE(, float, 4, GpuDevice, true)
+PACKET_TYPE(const, float, 4, const GpuDevice, true)
+PACKET_TYPE(, float, 4, const GpuDevice, true)
+
+PACKET_TYPE(const, bool, 16, GpuDevice, true)
+PACKET_TYPE(, bool, 16, GpuDevice, true)
+PACKET_TYPE(const, bool, 16, const GpuDevice, true)
+PACKET_TYPE(, bool, 16, const GpuDevice, true)
+
+PACKET_TYPE_FOR_BOOL(GpuDevice, true)
+PACKET_TYPE_FOR_BOOL(const GpuDevice, true)
+
+PACKET_TYPE_FOR_HALF8(const, Eigen::half, GpuDevice, true);
+PACKET_TYPE_FOR_HALF8(, Eigen::half, GpuDevice, true);
+PACKET_TYPE_FOR_HALF8(const, Eigen::half, const GpuDevice, true);
+PACKET_TYPE_FOR_HALF8(, Eigen::half, const GpuDevice, true);
+
+PACKET_TYPE_FOR_BF16_8(const, Eigen::bfloat16, GpuDevice, true);
+PACKET_TYPE_FOR_BF16_8(, Eigen::bfloat16, GpuDevice, true);
+PACKET_TYPE_FOR_BF16_8(const, Eigen::bfloat16, const GpuDevice, true);
+PACKET_TYPE_FOR_BF16_8(, Eigen::bfloat16, const GpuDevice, true);
+
+#else
+PACKET_TYPE(const, float, 4, GpuDevice, false)
+PACKET_TYPE(, float, 4, GpuDevice, false)
+PACKET_TYPE(const, float, 4, const GpuDevice, false)
+PACKET_TYPE(, float, 4, const GpuDevice, false)
+
+PACKET_TYPE(const, bool, 16, GpuDevice, false)
+PACKET_TYPE(, bool, 16, GpuDevice, false)
+PACKET_TYPE(const, bool, 16, const GpuDevice, false)
+PACKET_TYPE(, bool, 16, const GpuDevice, false)
+
+PACKET_TYPE_FOR_BOOL(GpuDevice, false)
+PACKET_TYPE_FOR_BOOL(const GpuDevice, false)
+
+PACKET_TYPE_FOR_BF16_8(const, Eigen::bfloat16, GpuDevice, false);
+PACKET_TYPE_FOR_BF16_8(, Eigen::bfloat16, GpuDevice, false);
+PACKET_TYPE_FOR_BF16_8(const, Eigen::bfloat16, const GpuDevice, false);
+PACKET_TYPE_FOR_BF16_8(, Eigen::bfloat16, const GpuDevice, false);
+
+PACKET_TYPE_FOR_HALF8(const, Eigen::half, GpuDevice, false);
+PACKET_TYPE_FOR_HALF8(, Eigen::half, GpuDevice, false);
+PACKET_TYPE_FOR_HALF8(const, Eigen::half, const GpuDevice, false);
+PACKET_TYPE_FOR_HALF8(, Eigen::half, const GpuDevice, false);
+#endif // DPCPP_DEVICE_ONLY
+#undef PACKET_TYPE
+#undef PACKET_TYPE_FOR_BOOL
+#undef PACKET_TYPE_FOR_HALF8
+#undef PACKET_TYPE_FOR_BF16_8
+#endif // EIGEN_USE_DPCPP
+#endif // EIGEN_DONT_VECTORIZE_SYCL
+#endif 
 
 // Tuple mimics std::pair but works on e.g. nvcc.
 template <typename U, typename V> struct Tuple {
@@ -224,7 +338,7 @@ template <typename U, typename V> struct Tuple {
 
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
   Tuple& operator= (const Tuple& rhs) {
-  #ifndef SYCL_DEVICE_ONLY
+  #if !defined(SYCL_DEVICE_ONLY) && !defined(DPCPP_DEVICE_ONLY)
     if (&rhs == this) return *this;
   #endif
     first = rhs.first;
@@ -314,8 +428,6 @@ namespace internal {
 }
 #endif
 
-
-
 }  // namespace Eigen
 
 #endif  // EIGEN_CXX11_TENSOR_TENSOR_META_H
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h b/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h
index b4bcb54be..878a4b486 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h
@@ -241,10 +241,11 @@ struct TensorEvaluator<const TensorReshapingOp<NewDimensions, ArgType>, Device>
 
   #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
   #endif
+
  protected:
   TensorEvaluator<ArgType, Device> m_impl;
   NewDimensions m_dimensions;
@@ -285,13 +286,13 @@ template<typename NewDimensions, typename ArgType, typename Device>
       TensorBlockDesc;
   //===--------------------------------------------------------------------===//
 
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType& coeffRef(Index index)
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType& coeffRef(Index index) const
   {
     return this->m_impl.coeffRef(index);
   }
 
   template <int StoreMode> EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
-  void writePacket(Index index, const PacketReturnType& x)
+  void writePacket(Index index, const PacketReturnType& x) const
   {
     this->m_impl.template writePacket<StoreMode>(index, x);
   }
@@ -432,7 +433,6 @@ template <typename Index, bool BlockAccess> struct MemcpyTriggerForSlicing<Index
   EIGEN_DEVICE_FUNC bool operator ()(Index, Index contiguous) const { return contiguous > 4*1024*1024; }
 };
 #endif
-
 }
 
 // Eval as rvalue
@@ -621,8 +621,14 @@ struct TensorEvaluator<const TensorSlicingOp<StartIndices, Sizes, ArgType>, Devi
       inputIndices[1] += (indices[1] + m_offsets[NumDims-1]);
     }
     if (inputIndices[1] - inputIndices[0] == packetSize - 1) {
-      PacketReturnType rslt = m_impl.template packet<Unaligned>(inputIndices[0]);
-      return rslt;
+      // TODO(CMPLRLLVM-37436) Due to compiler bug, need to load vector explicitly.
+      if (TensorEvaluator<ArgType, Device>::IsAligned && !internal::IsOperator<ArgType>::value &&
+          inputIndices[0] % packetSize == 0) {
+        return m_impl.template packet<Aligned>(inputIndices[0]);
+      }
+      else {
+        return m_impl.template packet<Unaligned>(inputIndices[0]);
+      }
     }
     else {
       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[packetSize];
@@ -695,7 +701,7 @@ struct TensorEvaluator<const TensorSlicingOp<StartIndices, Sizes, ArgType>, Devi
   }
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
@@ -771,7 +777,7 @@ struct TensorEvaluator<TensorSlicingOp<StartIndices, Sizes, ArgType>, Device>
     : Base(op, device)
     { }
 
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType& coeffRef(Index index)
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType& coeffRef(Index index) const
   {
     if (this->m_is_identity) {
       return this->m_impl.coeffRef(index);
@@ -781,7 +787,7 @@ struct TensorEvaluator<TensorSlicingOp<StartIndices, Sizes, ArgType>, Device>
   }
 
   template <int StoreMode> EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
-  void writePacket(Index index, const PacketReturnType& x)
+  void writePacket(Index index, const PacketReturnType& x) const
   {
     if (this->m_is_identity) {
       this->m_impl.template writePacket<StoreMode>(index, x);
@@ -817,7 +823,14 @@ struct TensorEvaluator<TensorSlicingOp<StartIndices, Sizes, ArgType>, Device>
       inputIndices[1] += (indices[1] + this->m_offsets[NumDims-1]);
     }
     if (inputIndices[1] - inputIndices[0] == packetSize - 1) {
-      this->m_impl.template writePacket<StoreMode>(inputIndices[0], x);
+      // TODO(CMPLRLLVM-37436) Due to compiler bug, need to store vector explicitly.
+      if (TensorEvaluator<ArgType, Device>::IsAligned && !internal::IsOperator<ArgType>::value &&
+          inputIndices[0] % packetSize == 0) {
+        this->m_impl.template writePacket<Aligned>(inputIndices[0], x);
+      }
+      else {
+        this->m_impl.template writePacket<StoreMode>(inputIndices[0], x);
+      }
     }
     else {
       EIGEN_ALIGN_MAX CoeffReturnType values[packetSize];
@@ -1066,7 +1079,7 @@ struct TensorEvaluator<const TensorStridingSlicingOp<StartIndices, StopIndices,
   }
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
@@ -1093,10 +1106,10 @@ struct TensorEvaluator<const TensorStridingSlicingOp<StartIndices, StopIndices,
   }
 
   static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Index clamp(Index value, Index min, Index max) {
-#ifndef SYCL_DEVICE_ONLY
+#if !defined(SYCL_DEVICE_ONLY) && !defined(DPCPP_DEVICE_ONLY)
     return numext::maxi(min, numext::mini(max,value));
 #else
-    return cl::sycl::clamp(value, min, max);
+    return sycl::clamp(value, min, max);
 #endif
   }
 
@@ -1145,7 +1158,7 @@ struct TensorEvaluator<TensorStridingSlicingOp<StartIndices, StopIndices, Stride
   typedef typename PacketType<CoeffReturnType, Device>::type PacketReturnType;
   typedef Strides Dimensions;
 
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType& coeffRef(Index index)
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType& coeffRef(Index index) const
   {
     if (this->m_is_identity) {
       return this->m_impl.coeffRef(index);
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorPadding.h b/unsupported/Eigen/CXX11/src/Tensor/TensorPadding.h
index 561666c6f..51c54ec02 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorPadding.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorPadding.h
@@ -485,7 +485,7 @@ struct TensorEvaluator<const TensorPaddingOp<PaddingDimensions, ArgType>, Device
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorPatch.h b/unsupported/Eigen/CXX11/src/Tensor/TensorPatch.h
index 64a436e50..8f1357225 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorPatch.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorPatch.h
@@ -271,7 +271,7 @@ struct TensorEvaluator<const TensorPatchOp<PatchDim, ArgType>, Device>
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh); 
   }
 #endif
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h b/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h
index ea286fee1..eeef4f73d 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h
@@ -94,9 +94,16 @@ Eigen::half RandomToTypeUniform<Eigen::half>(uint64_t* state, uint64_t stream) {
   Eigen::half result;
   // Generate 10 random bits for the mantissa
   unsigned rnd = PCG_XSH_RS_generator(state, stream);
-  result.x = static_cast<uint16_t>(rnd & 0x3ffu);
   // Set the exponent
+#if defined(DPCPP_DEVICE_ONLY)
+  u16_to_sycl_half uhx;
+  uhx.u = static_cast<uint16_t>(rnd & 0x3ffu);
+  uhx.u |= (static_cast<uint16_t>(15) << 10);
+  result.x = uhx.h;
+#else
+  result.x = static_cast<uint16_t>(rnd & 0x3ffu);
   result.x |= (static_cast<uint16_t>(15) << 10);
+#endif
   // Return the final result
   return result - Eigen::half(1.0f);
 }
@@ -168,7 +175,7 @@ template <typename T> class UniformRandomGenerator {
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE UniformRandomGenerator(
       uint64_t seed = 0) {
     m_state = PCG_XSH_RS_state(seed);
-    #ifdef EIGEN_USE_SYCL
+    #if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
     // In SYCL it is not possible to build PCG_XSH_RS_state in one step. 
     // Therefor, we need two step to initializate the m_state.
     // IN SYCL, the constructor of the functor is s called on the CPU
@@ -188,14 +195,14 @@ template <typename T> class UniformRandomGenerator {
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE UniformRandomGenerator(
       const UniformRandomGenerator& other) {
     m_state = other.m_state;
-    #ifdef EIGEN_USE_SYCL
+    #if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
      m_exec_once =other.m_exec_once;
     #endif
   }
 
   template<typename Index> EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
   T operator()(Index i) const {
-    #ifdef EIGEN_USE_SYCL
+    #if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
       if(!m_exec_once) {
       // This is the second stage of adding thread Id to the CPU clock seed and build unique seed per thread
       // The (i * 6364136223846793005ULL) is the remaining part of the PCG_XSH_RS_state on the GPU side
@@ -211,7 +218,7 @@ template <typename T> class UniformRandomGenerator {
   Packet packetOp(Index i) const {
     const int packetSize = internal::unpacket_traits<Packet>::size;
     EIGEN_ALIGN_MAX T values[packetSize];
-      #ifdef EIGEN_USE_SYCL
+     #if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
       if(!m_exec_once) {
       // This is the second stage of adding thread Id to the CPU clock seed and build unique seed per thread
        m_state += (i * 6364136223846793005ULL);
@@ -227,7 +234,7 @@ template <typename T> class UniformRandomGenerator {
 
  private:
   mutable uint64_t m_state;
-  #ifdef EIGEN_USE_SYCL
+  #if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
   mutable bool m_exec_once;
   #endif
 };
@@ -281,7 +288,7 @@ template <typename T> class NormalRandomGenerator {
   // Uses the given "seed" if non-zero, otherwise uses a random seed.
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE NormalRandomGenerator(uint64_t seed = 0) {
     m_state = PCG_XSH_RS_state(seed);
-    #ifdef EIGEN_USE_SYCL
+    #if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
     // In SYCL it is not possible to build PCG_XSH_RS_state in one step. 
     // Therefor, we need two steps to initializate the m_state.
     // IN SYCL, the constructor of the functor is s called on the CPU
@@ -298,14 +305,14 @@ template <typename T> class NormalRandomGenerator {
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE NormalRandomGenerator(
       const NormalRandomGenerator& other) {
     m_state = other.m_state;
-#ifdef EIGEN_USE_SYCL
+#if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
     m_exec_once=other.m_exec_once;
 #endif
   }
 
  template<typename Index> EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
   T operator()(Index i) const {
-    #ifdef EIGEN_USE_SYCL
+    #if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
     if(!m_exec_once) {
       // This is the second stage of adding thread Id to the CPU clock seed and build unique seed per thread
       m_state += (i * 6364136223846793005ULL);
@@ -320,7 +327,7 @@ template <typename T> class NormalRandomGenerator {
   Packet packetOp(Index i) const {
     const int packetSize = internal::unpacket_traits<Packet>::size;
     EIGEN_ALIGN_MAX T values[packetSize];
-    #ifdef EIGEN_USE_SYCL
+    #if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
     if(!m_exec_once) {
       // This is the second stage of adding thread Id to the CPU clock seed and build unique seed per thread
       m_state += (i * 6364136223846793005ULL);
@@ -336,7 +343,7 @@ template <typename T> class NormalRandomGenerator {
 
  private:
   mutable uint64_t m_state;
-   #ifdef EIGEN_USE_SYCL
+  #if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
   mutable bool m_exec_once;
   #endif
 };
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h b/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h
index 9b0eb3e2f..4a2167be0 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h
@@ -141,7 +141,7 @@ struct preserve_inner_most_dims<ReducedDims, NumTensorDims, RowMajor>{
 
 template <int DimIndex, typename Self, typename Op>
 struct GenericDimReducer {
-  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void reduce(const Self& self, typename Self::Index firstIndex, Op& reducer, typename Self::CoeffReturnType* accum) {
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void reduce(const Self& self, typename Self::Index firstIndex, const Op& reducer, typename Self::CoeffReturnType* accum) {
     EIGEN_STATIC_ASSERT((DimIndex > 0), YOU_MADE_A_PROGRAMMING_MISTAKE);
     for (int j = 0; j < self.m_reducedDims[DimIndex]; ++j) {
       const typename Self::Index input = firstIndex + j * self.m_reducedStrides[DimIndex];
@@ -151,7 +151,7 @@ struct GenericDimReducer {
 };
 template <typename Self, typename Op>
 struct GenericDimReducer<0, Self, Op> {
-  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void reduce(const Self& self, typename Self::Index firstIndex, Op& reducer, typename Self::CoeffReturnType* accum) {
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void reduce(const Self& self, typename Self::Index firstIndex, const Op& reducer, typename Self::CoeffReturnType* accum) {
     for (int j = 0; j < self.m_reducedDims[0]; ++j) {
       const typename Self::Index input = firstIndex + j * self.m_reducedStrides[0];
       reducer.reduce(self.m_impl.coeff(input), accum);
@@ -160,7 +160,7 @@ struct GenericDimReducer<0, Self, Op> {
 };
 template <typename Self, typename Op>
 struct GenericDimReducer<-1, Self, Op> {
-  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void reduce(const Self& self, typename Self::Index index, Op& reducer, typename Self::CoeffReturnType* accum) {
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void reduce(const Self& self, typename Self::Index index, const Op& reducer, typename Self::CoeffReturnType* accum) {
     reducer.reduce(self.m_impl.coeff(index), accum);
   }
 };
@@ -169,7 +169,7 @@ template <typename Self, typename Op, bool Vectorizable = (Self::InputPacketAcce
           bool UseTreeReduction = (!Self::ReducerTraits::IsStateful &&
                                    !Self::ReducerTraits::IsExactlyAssociative)>
 struct InnerMostDimReducer {
-  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename Self::CoeffReturnType reduce(const Self& self, typename Self::Index firstIndex, typename Self::Index numValuesToReduce, Op& reducer) {
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename Self::CoeffReturnType reduce(const Self& self, typename Self::Index firstIndex, typename Self::Index numValuesToReduce, const Op& reducer) {
     typename Self::CoeffReturnType accum = reducer.initialize();
     for (typename Self::Index j = 0; j < numValuesToReduce; ++j) {
       reducer.reduce(self.m_impl.coeff(firstIndex + j), &accum);
@@ -180,7 +180,7 @@ struct InnerMostDimReducer {
 
 template <typename Self, typename Op>
 struct InnerMostDimReducer<Self, Op, true, false> {
-  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename Self::CoeffReturnType reduce(const Self& self, typename Self::Index firstIndex, typename Self::Index numValuesToReduce, Op& reducer) {
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename Self::CoeffReturnType reduce(const Self& self, typename Self::Index firstIndex, typename Self::Index numValuesToReduce, const Op& reducer) {
     const typename Self::Index packetSize = internal::unpacket_traits<typename Self::PacketReturnType>::size;
     const typename Self::Index VectorizedSize = (numValuesToReduce / packetSize) * packetSize;
     typename Self::PacketReturnType paccum = reducer.template initializePacket<typename Self::PacketReturnType>();
@@ -412,7 +412,7 @@ struct OuterReducer {
   }
 };
 
-#ifdef EIGEN_USE_SYCL
+#if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
 // Default Generic reducer
 template <typename Self, typename Op, typename Device>
 struct GenericReducer {
@@ -457,7 +457,7 @@ __global__ __launch_bounds__(1024) void OuterReductionKernel(R, const S, I_, I_,
  */
 template <typename Op, typename CoeffReturnType>
 struct ReductionReturnType {
-#if defined(EIGEN_USE_SYCL)
+#if EIGEN_HAS_CXX11 && (defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP))
   typedef typename remove_const<decltype(std::declval<Op>().initialize())>::type type;
 #else
   typedef typename remove_const<CoeffReturnType>::type type;
@@ -516,7 +516,9 @@ struct TensorReductionEvaluatorBase<const TensorReductionOp<Op, Dims, ArgType, M
   typedef typename internal::conditional<NumOutputDims==0, Sizes<>, DSizes<Index, NumOutputDims> >::type Dimensions;
   typedef typename XprType::Scalar Scalar;
   typedef TensorReductionEvaluatorBase<const TensorReductionOp<Op, Dims, ArgType, MakePointer_>, Device> Self;
-  static const bool InputPacketAccess = TensorEvaluator<ArgType, Device>::PacketAccess;
+
+  // TODO(itex): Reopen this vectorization implementation when its performance is not bad.
+  static const bool InputPacketAccess = false;
   typedef typename internal::ReductionReturnType<Op, typename XprType::CoeffReturnType>::type CoeffReturnType;
   typedef typename PacketType<CoeffReturnType, Device>::type PacketReturnType;
   static const Index PacketSize = PacketType<CoeffReturnType, Device>::size;
@@ -525,7 +527,7 @@ struct TensorReductionEvaluatorBase<const TensorReductionOp<Op, Dims, ArgType, M
   typedef StorageMemory<CoeffReturnType, Device> Storage;
   typedef typename Storage::Type EvaluatorPointerType;
 
-    // Subset of strides of the input tensor for the non-reduced dimensions.
+  // Subset of strides of the input tensor for the non-reduced dimensions.
   // Indexed by output dimensions.
   static const int NumPreservedStrides = max_n_1<NumOutputDims>::size;
 
@@ -545,6 +547,10 @@ struct TensorReductionEvaluatorBase<const TensorReductionOp<Op, Dims, ArgType, M
   typedef internal::TensorBlockNotImplemented TensorBlock;
   //===--------------------------------------------------------------------===//
 
+  //===- Tensor block evaluation strategy (see TensorBlock.h) -------------===//
+  typedef internal::TensorBlockNotImplemented TensorBlockV2;
+  //===--------------------------------------------------------------------===//
+
   static const bool ReducingInnerMostDims = internal::are_inner_most_dims<Dims, NumInputDims, Layout>::value;
   static const bool PreservingInnerMostDims = internal::preserve_inner_most_dims<Dims, NumInputDims, Layout>::value;
   static const bool RunningFullReduction = (NumOutputDims==0);
@@ -724,7 +730,7 @@ struct TensorReductionEvaluatorBase<const TensorReductionOp<Op, Dims, ArgType, M
           return (m_result != NULL);
         }
       }
-      #if defined(EIGEN_USE_SYCL)
+      #if defined(EIGEN_USE_SYCL) || defined(EIGEN_USE_DPCPP)
       // If there is no Optimised version for SYCL, the reduction expression 
       // must break into two subexpression and use the SYCL generic Reducer on the device.
       if(RunningOnSycl) {
@@ -801,7 +807,6 @@ struct TensorReductionEvaluatorBase<const TensorReductionOp<Op, Dims, ArgType, M
   template<int LoadMode>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE PacketReturnType packet(Index index) const
   {
-    EIGEN_STATIC_ASSERT((PacketSize > 1), YOU_MADE_A_PROGRAMMING_MISTAKE)
     eigen_assert(index + PacketSize - 1 < Index(internal::array_prod(dimensions())));
 
     if (RunningOnGPU && m_result) {
@@ -856,13 +861,6 @@ struct TensorReductionEvaluatorBase<const TensorReductionOp<Op, Dims, ArgType, M
   EIGEN_DEVICE_FUNC EvaluatorPointerType data() const { return m_result; }
   EIGEN_DEVICE_FUNC const TensorEvaluator<ArgType, Device>& impl() const { return m_impl; }
   EIGEN_DEVICE_FUNC const Device& device() const { return m_device; }
-#ifdef EIGEN_USE_SYCL
-  // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
-    m_impl.bind(cgh);
-    m_result.bind(cgh);
-  }
-#endif
 
   private:
   template <int, typename, typename> friend struct internal::GenericDimReducer;
@@ -890,6 +888,12 @@ struct TensorReductionEvaluatorBase<const TensorReductionOp<Op, Dims, ArgType, M
  template <typename, typename, typename> friend struct internal::GenericReducer;
 #endif
 
+#if defined(EIGEN_USE_DPCPP)
+ template < typename Evaluator_, typename Op__> friend class internal::GenericRandomReducer;
+ // SYCL need the Generic reducer for the case the recution algorithm is neither inner, outer, and full reducer
+ template <typename, typename, typename> friend struct internal::GenericReducer;
+#endif
+
 
   template <typename S, typename O, typename D> friend struct internal::InnerReducer;
 
@@ -974,6 +978,10 @@ struct TensorReductionEvaluatorBase<const TensorReductionOp<Op, Dims, ArgType, M
 #elif defined(EIGEN_USE_SYCL)
 static const bool RunningOnSycl = internal::is_same<typename internal::remove_all<Device>::type, Eigen::SyclDevice>::value;
 static const bool RunningOnGPU = false;
+#elif defined(EIGEN_USE_DPCPP)
+static const bool RunningOnSycl = internal::is_same<typename internal::remove_all<Device>::type, Eigen::GpuDevice>::value;
+static const bool RunningOnGPU = false;
+
 #else
   static const bool RunningOnGPU = false;
   static const bool RunningOnSycl = false;
@@ -990,7 +998,28 @@ struct TensorEvaluator<const TensorReductionOp<Op, Dims, ArgType, MakePointer_>,
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TensorEvaluator(const typename Base::XprType& op, const Device& device) : Base(op, device){}
 };
 
+#ifdef EIGEN_USE_DPCPP
+template<typename Op, typename Dims, typename ArgType, template <class> class MakePointer_>
+struct TensorEvaluator<const TensorReductionOp<Op, Dims, ArgType, MakePointer_>, Eigen::GpuDevice>
+: public TensorReductionEvaluatorBase<const TensorReductionOp<Op, Dims, ArgType, MakePointer_>, Eigen::GpuDevice> {
 
+  typedef TensorReductionEvaluatorBase<const TensorReductionOp<Op, Dims, ArgType, MakePointer_>, Eigen::GpuDevice> Base;
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TensorEvaluator(const typename Base::XprType& op, const Eigen::GpuDevice& device) : Base(op, device){}
+  // The coeff function in the base the recursive method which is not an standard layout and cannot be used in the SYCL kernel
+  //Therefore the coeff function should be overridden by for SYCL kernel
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename Base::CoeffReturnType coeff(typename Base::Index index) const {
+    return *(this->data() + index);
+  }
+  // The packet function in the base the recursive method which is not an standard layout and cannot be used in the SYCL kernel
+  //Therefore the packet function should be overridden by for SYCL kernel
+  template<int LoadMode>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename Base::PacketReturnType packet(typename Base::Index index) const {
+    return internal::pload<typename Base::PacketReturnType>(this->data() + index);
+  }
+};
+
+
+#endif
 template<typename Op, typename Dims, typename ArgType, template <class> class MakePointer_>
 struct TensorEvaluator<const TensorReductionOp<Op, Dims, ArgType, MakePointer_>, Eigen::SyclDevice>
 : public TensorReductionEvaluatorBase<const TensorReductionOp<Op, Dims, ArgType, MakePointer_>, Eigen::SyclDevice> {
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorReductionDpcpp.h b/unsupported/Eigen/CXX11/src/Tensor/TensorReductionDpcpp.h
new file mode 100644
index 000000000..66dcd0d0e
--- /dev/null
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorReductionDpcpp.h
@@ -0,0 +1,579 @@
+// This file is part of Eigen, a lightweight C++ template library
+// for linear algebra.
+//
+// Mehdi Goli    Codeplay Software Ltd.
+// Ralph Potter  Codeplay Software Ltd.
+// Luke Iwanski  Codeplay Software Ltd.
+// Contact: <eigen@codeplay.com>
+//
+// This Source Code Form is subject to the terms of the Mozilla
+// Public License v. 2.0. If a copy of the MPL was not distributed
+// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+/*****************************************************************
+ * TensorReductionDpcpp.h
+ *
+ * \brief:
+ *  This is the specialisation of the reduction operation
+ *
+ *****************************************************************/
+
+#if defined(EIGEN_USE_GPU) && !defined(UNSUPPORTED_EIGEN_CXX11_SRC_TENSOR_TENSOR_REDUCTION_DPCPP_HPP)
+#define UNSUPPORTED_EIGEN_CXX11_SRC_TENSOR_TENSOR_REDUCTION_DPCPP_HPP
+namespace Eigen {
+namespace internal {
+
+template <typename Op, typename CoeffReturnType, typename Index, bool Vectorizable>
+struct OpDefiner {
+  typedef typename Vectorise<CoeffReturnType, Eigen::GpuDevice, Vectorizable>::PacketReturnType PacketReturnType;
+  typedef Op type;
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE type get_op(Op &op) { return op; }
+
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE PacketReturnType finalise_op(const PacketReturnType &accumulator,
+                                                                            const Index &) {
+    return accumulator;
+  }
+};
+
+template <typename CoeffReturnType, typename Index>
+struct OpDefiner<Eigen::internal::MeanReducer<CoeffReturnType>, CoeffReturnType, Index, false> {
+  typedef Eigen::internal::SumReducer<CoeffReturnType> type;
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE type get_op(Eigen::internal::MeanReducer<CoeffReturnType> &) {
+    return type();
+  }
+
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE CoeffReturnType finalise_op(const CoeffReturnType &accumulator,
+                                                                           const Index &scale) {
+    ::Eigen::internal::scalar_quotient_op<CoeffReturnType> quotient_op;
+    return quotient_op(accumulator, CoeffReturnType(scale));
+  }
+};
+
+template <typename CoeffReturnType, typename Index>
+struct OpDefiner<Eigen::internal::MeanReducer<CoeffReturnType>, CoeffReturnType, Index, true> {
+  typedef typename Vectorise<CoeffReturnType, Eigen::GpuDevice, true>::PacketReturnType PacketReturnType;
+  typedef Eigen::internal::SumReducer<CoeffReturnType> type;
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE type get_op(Eigen::internal::MeanReducer<CoeffReturnType> &) {
+    return type();
+  }
+
+  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE PacketReturnType finalise_op(const PacketReturnType &accumulator,
+                                                                            const Index &scale) {
+    return ::Eigen::internal::pdiv(accumulator, ::Eigen::internal::pset1<PacketReturnType>(CoeffReturnType(scale)));
+  }
+};
+
+template <typename CoeffReturnType, typename OpType, typename InputAccessor, typename OutputAccessor, typename Index>
+struct SecondStepFullReducer {
+  typedef sycl::accessor<CoeffReturnType, 1, sycl::access::mode::read_write, sycl::access::target::local>
+      LocalAccessor;
+  typedef OpDefiner<OpType, CoeffReturnType, Index, true> OpDef;
+  typedef typename OpDef::type Op;
+  LocalAccessor scratch;
+  InputAccessor aI;
+  OutputAccessor outAcc;
+  Op op;
+  SecondStepFullReducer(LocalAccessor scratch_, InputAccessor aI_, OutputAccessor outAcc_, OpType op_)
+      : scratch(scratch_), aI(aI_), outAcc(outAcc_), op(OpDef::get_op(op_)) {}
+
+  void operator()(sycl::nd_item<1> itemID) const {
+    // Our empirical research shows that the best performance will be achieved
+    // when there is only one element per thread to reduce in the second step.
+    // in this step the second step reduction time is almost negligible.
+    // Hence, in the second step of reduction the input size is fixed to the
+    // local size, thus, there is only one element read per thread. The
+    // algorithm must be changed if the number of reduce per thread in the
+    // second step is greater than 1. Otherwise, the result will be wrong.
+    const Index localid = itemID.get_local_id(0);
+    auto aInPtr = aI + localid;
+    auto aOutPtr = outAcc;
+    CoeffReturnType *scratchptr = scratch.get_pointer();
+    CoeffReturnType accumulator = *aInPtr;
+
+    scratchptr[localid] = op.finalize(accumulator);
+#pragma unroll 8
+    for (Index offset = itemID.get_local_range(0) / 2; offset > 0; offset /= 2) {
+      sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+      if (localid < offset) {
+        op.reduce(scratchptr[localid + offset], &accumulator);
+        scratchptr[localid] = op.finalize(accumulator);
+      }
+    }
+    if (localid == 0) *aOutPtr = op.finalize(accumulator);
+  }
+};
+
+// full reductoion first phase // Vectorization is true and accept general
+// reducer op
+// This function should be fixed it  should not get packet in the acc
+template <typename Evaluator, typename OpType>
+class FullReductionKernelFunctor {
+ public:
+  typedef typename Evaluator::CoeffReturnType CoeffReturnType;
+  typedef typename Evaluator::Index Index;
+  typedef OpDefiner<OpType, typename Evaluator::CoeffReturnType, Index,
+                    (Evaluator::ReducerTraits::PacketAccess & Evaluator::InputPacketAccess)>
+      OpDef;
+
+  typedef typename OpDef::type Op;
+  typedef typename Evaluator::EvaluatorPointerType EvaluatorPointerType;
+  typedef typename Evaluator::PacketReturnType PacketReturnType;
+  typedef
+      typename ::Eigen::internal::conditional<(Evaluator::ReducerTraits::PacketAccess & Evaluator::InputPacketAccess),
+                                              PacketReturnType, CoeffReturnType>::type OutType;
+  typedef sycl::accessor<OutType, 1, sycl::access::mode::read_write, sycl::access::target::local>
+      LocalAccessor;
+  LocalAccessor scratch;
+  Evaluator evaluator;
+  EvaluatorPointerType final_output;
+  Index rng;
+  Op op;
+
+  FullReductionKernelFunctor(LocalAccessor scratch_, Evaluator evaluator_, EvaluatorPointerType final_output_,
+                             Index rng_, OpType op_)
+      : scratch(scratch_), evaluator(evaluator_), final_output(final_output_), rng(rng_), op(OpDef::get_op(op_)) {}
+
+  void operator()(sycl::nd_item<1> itemID) const { compute_reduction(itemID); }
+
+  template <bool Vect = (Evaluator::ReducerTraits::PacketAccess & Evaluator::InputPacketAccess)>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename ::Eigen::internal::enable_if<Vect>::type compute_reduction(
+      const sycl::nd_item<1> &itemID) const {
+    auto output_ptr = final_output;
+    Index VectorizedRange = (rng / Evaluator::PacketSize) * Evaluator::PacketSize;
+    Index globalid = itemID.get_global_id(0);
+    Index localid = itemID.get_local_id(0);
+    Index step = Evaluator::PacketSize * itemID.get_global_range(0);
+    Index start = Evaluator::PacketSize * globalid;
+    // vectorizable parts
+    PacketReturnType packetAccumulator = op.template initializePacket<PacketReturnType>();
+#pragma unroll(8 / Evaluator::PacketSize)
+    for (Index i = start; i < VectorizedRange; i += step) {
+      op.template reducePacket<PacketReturnType>(evaluator.impl().template packet<Unaligned>(i), &packetAccumulator);
+    }
+    globalid += VectorizedRange;
+    // non vectorizable parts
+    for (Index i = globalid; i < rng; i += itemID.get_global_range(0)) {
+      op.template reducePacket<PacketReturnType>(
+          ::Eigen::internal::PacketWrapper<PacketReturnType, Evaluator::PacketSize>::convert_to_packet_type(
+              evaluator.impl().coeff(i), op.initialize()),
+          &packetAccumulator);
+    }
+    scratch[localid] = packetAccumulator =
+        OpDef::finalise_op(op.template finalizePacket<PacketReturnType>(packetAccumulator), rng);
+    // reduction parts // Local size is always power of 2
+    EIGEN_CONSTEXPR Index local_range = EIGEN_SYCL_LOCAL_THREAD_DIM0 * EIGEN_SYCL_LOCAL_THREAD_DIM1;
+    EIGEN_UNROLL_LOOP
+    for (Index offset = local_range / 2; offset > 0; offset /= 2) {
+      sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+      if (localid < offset) {
+        op.template reducePacket<PacketReturnType>(scratch[localid + offset], &packetAccumulator);
+        scratch[localid] = op.template finalizePacket<PacketReturnType>(packetAccumulator);
+      }
+    }
+    if (localid == 0) {
+      output_ptr[itemID.get_group(0)] =
+          op.finalizeBoth(op.initialize(), op.template finalizePacket<PacketReturnType>(packetAccumulator));
+    }
+  }
+
+  template <bool Vect = (Evaluator::ReducerTraits::PacketAccess & Evaluator::InputPacketAccess)>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename ::Eigen::internal::enable_if<!Vect>::type compute_reduction(
+      const sycl::nd_item<1> &itemID) const {
+    auto output_ptr = final_output;
+    Index globalid = itemID.get_global_id(0);
+    Index localid = itemID.get_local_id(0);
+    // vectorizable parts
+    CoeffReturnType accumulator = op.initialize();
+    // non vectorizable parts
+    for (Index i = globalid; i < rng; i += itemID.get_global_range(0)) {
+      op.reduce(evaluator.impl().coeff(i), &accumulator);
+    }
+    scratch[localid] = accumulator = OpDef::finalise_op(op.finalize(accumulator), rng);
+
+    // reduction parts // Local size is always power of 2
+    EIGEN_CONSTEXPR Index local_range = EIGEN_SYCL_LOCAL_THREAD_DIM0 * EIGEN_SYCL_LOCAL_THREAD_DIM1;
+    EIGEN_UNROLL_LOOP
+    for (Index offset = local_range / 2; offset > 0; offset /= 2) {
+      sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+      if (localid < offset) {
+        op.reduce(scratch[localid + offset], &accumulator);
+        scratch[localid] = op.finalize(accumulator);
+      }
+    }
+    if (localid == 0) {
+      output_ptr[itemID.get_group(0)] = op.finalize(accumulator);
+    }
+  }
+};
+
+template <typename Evaluator, typename OpType>
+class GenericRandomReducer {
+ public:
+  typedef typename Evaluator::CoeffReturnType CoeffReturnType;
+  typedef typename Evaluator::EvaluatorPointerType EvaluatorPointerType;
+  typedef typename Evaluator::Index Index;
+  typedef OpDefiner<OpType, CoeffReturnType, Index, false> OpDef;
+  typedef typename OpDef::type Op;
+  template <typename Scratch>
+  GenericRandomReducer(Scratch, Evaluator evaluator_, EvaluatorPointerType output_accessor_, OpType functor_,
+                       Index range_, Index num_values_to_reduce_)
+      : evaluator(evaluator_),
+        output_accessor(output_accessor_),
+        functor(OpDef::get_op(functor_)),
+        range(range_),
+        num_values_to_reduce(num_values_to_reduce_) {}
+
+  void operator()(sycl::nd_item<1> itemID) const {
+    auto output_accessor_ptr = output_accessor;
+    /// const cast added as a naive solution to solve the qualifier drop error
+    Index globalid = static_cast<Index>(itemID.get_global_linear_id());
+    if (globalid < range) {
+      CoeffReturnType accum = functor.initialize();
+      Eigen::internal::GenericDimReducer<Evaluator::NumReducedDims - 1, Evaluator, Op>::reduce(
+          evaluator, evaluator.firstInput(globalid), functor, &accum);
+      output_accessor_ptr[globalid] = OpDef::finalise_op(functor.finalize(accum), num_values_to_reduce);
+    }
+  }
+
+ private:
+  Evaluator evaluator;
+  EvaluatorPointerType output_accessor;
+  Op functor;
+  Index range;
+  Index num_values_to_reduce;
+};
+
+enum class reduction_dim { inner_most, outer_most };
+// default is preserver
+template <typename Evaluator, typename OpType, typename PannelParameters, reduction_dim rt>
+struct PartialReductionKernel {
+  typedef typename Evaluator::CoeffReturnType CoeffReturnType;
+  typedef typename Evaluator::EvaluatorPointerType EvaluatorPointerType;
+  typedef typename Evaluator::Index Index;
+  typedef OpDefiner<OpType, CoeffReturnType, Index, false> OpDef;
+  typedef typename OpDef::type Op;
+  typedef sycl::accessor<CoeffReturnType, 1, sycl::access::mode::read_write, sycl::access::target::local>
+      ScratchAcc;
+  ScratchAcc scratch;
+  Evaluator evaluator;
+  EvaluatorPointerType output_accessor;
+  Op op;
+  const Index preserve_elements_num_groups;
+  const Index reduce_elements_num_groups;
+  const Index num_coeffs_to_preserve;
+  const Index num_coeffs_to_reduce;
+
+  PartialReductionKernel(ScratchAcc scratch_, Evaluator evaluator_, EvaluatorPointerType output_accessor_, OpType op_,
+                         const Index preserve_elements_num_groups_, const Index reduce_elements_num_groups_,
+                         const Index num_coeffs_to_preserve_, const Index num_coeffs_to_reduce_)
+      : scratch(scratch_),
+        evaluator(evaluator_),
+        output_accessor(output_accessor_),
+        op(OpDef::get_op(op_)),
+        preserve_elements_num_groups(preserve_elements_num_groups_),
+        reduce_elements_num_groups(reduce_elements_num_groups_),
+        num_coeffs_to_preserve(num_coeffs_to_preserve_),
+        num_coeffs_to_reduce(num_coeffs_to_reduce_) {}
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void element_wise_reduce(Index globalRId, Index globalPId,
+                                                                 CoeffReturnType &accumulator) const {
+    if (globalPId >= num_coeffs_to_preserve) {
+      return;
+    }
+    Index global_offset = rt == reduction_dim::outer_most ? globalPId + (globalRId * num_coeffs_to_preserve)
+                                                          : globalRId + (globalPId * num_coeffs_to_reduce);
+    Index localOffset = globalRId;
+
+    const Index per_thread_local_stride = PannelParameters::LocalThreadSizeR * reduce_elements_num_groups;
+    const Index per_thread_global_stride =
+        rt == reduction_dim::outer_most ? num_coeffs_to_preserve * per_thread_local_stride : per_thread_local_stride;
+#pragma unroll 8
+    for (Index i = globalRId; i < num_coeffs_to_reduce; i += per_thread_local_stride) {
+      op.reduce(evaluator.impl().coeff(global_offset), &accumulator);
+      localOffset += per_thread_local_stride;
+      global_offset += per_thread_global_stride;
+    }
+  }
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(sycl::nd_item<1> itemID) const {
+    const Index linearLocalThreadId = itemID.get_local_id(0);
+    Index pLocalThreadId = rt == reduction_dim::outer_most ? linearLocalThreadId % PannelParameters::LocalThreadSizeP
+                                                           : linearLocalThreadId / PannelParameters::LocalThreadSizeR;
+    Index rLocalThreadId = rt == reduction_dim::outer_most ? linearLocalThreadId / PannelParameters::LocalThreadSizeP
+                                                           : linearLocalThreadId % PannelParameters::LocalThreadSizeR;
+    const Index pGroupId = rt == reduction_dim::outer_most ? itemID.get_group(0) % preserve_elements_num_groups
+                                                           : itemID.get_group(0) / reduce_elements_num_groups;
+    const Index rGroupId = rt == reduction_dim::outer_most ? itemID.get_group(0) / preserve_elements_num_groups
+                                                           : itemID.get_group(0) % reduce_elements_num_groups;
+
+    Index globalPId = pGroupId * PannelParameters::LocalThreadSizeP + pLocalThreadId;
+    const Index globalRId = rGroupId * PannelParameters::LocalThreadSizeR + rLocalThreadId;
+    auto scratchPtr = scratch.get_pointer().get();
+    auto outPtr =
+        output_accessor + (reduce_elements_num_groups > 1 ? rGroupId * num_coeffs_to_preserve : 0);
+    CoeffReturnType accumulator = op.initialize();
+
+    element_wise_reduce(globalRId, globalPId, accumulator);
+
+    accumulator = OpDef::finalise_op(op.finalize(accumulator), num_coeffs_to_reduce);
+    scratchPtr[pLocalThreadId + rLocalThreadId * (PannelParameters::LocalThreadSizeP + PannelParameters::BC)] =
+        accumulator;
+    if (rt == reduction_dim::inner_most) {
+      pLocalThreadId = linearLocalThreadId % PannelParameters::LocalThreadSizeP;
+      rLocalThreadId = linearLocalThreadId / PannelParameters::LocalThreadSizeP;
+      globalPId = pGroupId * PannelParameters::LocalThreadSizeP + pLocalThreadId;
+    }
+
+    /* Apply the reduction operation between the current local
+     * id and the one on the other half of the vector. */
+    auto out_scratch_ptr =
+        scratchPtr + (pLocalThreadId + (rLocalThreadId * (PannelParameters::LocalThreadSizeP + PannelParameters::BC)));
+    sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+    if (rt == reduction_dim::inner_most) {
+      accumulator = *out_scratch_ptr;
+    }
+    // The Local LocalThreadSizeR is always power of 2
+    EIGEN_UNROLL_LOOP
+    for (Index offset = PannelParameters::LocalThreadSizeR >> 1; offset > 0; offset >>= 1) {
+      if (rLocalThreadId < offset) {
+        op.reduce(out_scratch_ptr[(PannelParameters::LocalThreadSizeP + PannelParameters::BC) * offset], &accumulator);
+        // The result has already been divided for mean reducer in the
+        // previous reduction so no need to divide furthermore
+        *out_scratch_ptr = op.finalize(accumulator);
+      }
+      /* All threads collectively read from global memory into local.
+       * The barrier ensures all threads' IO is resolved before
+       * execution continues (strictly speaking, all threads within
+       * a single work-group - there is no co-ordination between
+       * work-groups, only work-items). */
+      sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+    }
+
+    if (rLocalThreadId == 0 && (globalPId < num_coeffs_to_preserve)) {
+      outPtr[globalPId] = op.finalize(accumulator);
+    }
+  }
+};
+
+template <typename OutScalar, typename Index, typename InputAccessor, typename OutputAccessor, typename OpType>
+struct SecondStepPartialReduction {
+  typedef OpDefiner<OpType, OutScalar, Index, false> OpDef;
+  typedef typename OpDef::type Op;
+  typedef sycl::accessor<OutScalar, 1, sycl::access::mode::read_write, sycl::access::target::local>
+      ScratchAccessor;
+  InputAccessor input_accessor;
+  OutputAccessor output_accessor;
+  Op op;
+  const Index num_coeffs_to_preserve;
+  const Index num_coeffs_to_reduce;
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE SecondStepPartialReduction(ScratchAccessor, InputAccessor input_accessor_,
+                                                                   OutputAccessor output_accessor_, OpType op_,
+                                                                   const Index num_coeffs_to_preserve_,
+                                                                   const Index num_coeffs_to_reduce_)
+      : input_accessor(input_accessor_),
+        output_accessor(output_accessor_),
+        op(OpDef::get_op(op_)),
+        num_coeffs_to_preserve(num_coeffs_to_preserve_),
+        num_coeffs_to_reduce(num_coeffs_to_reduce_) {}
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(sycl::nd_item<1> itemID) const {
+    const Index globalId = itemID.get_global_id(0);
+
+    if (globalId >= num_coeffs_to_preserve) return;
+
+    auto in_ptr = input_accessor + globalId;
+
+    OutScalar accumulator = op.initialize();
+// num_coeffs_to_reduce is not bigger that 256
+#pragma unroll 8
+    for (Index i = 0; i < num_coeffs_to_reduce; i++) {
+      op.reduce(*in_ptr, &accumulator);
+      in_ptr += num_coeffs_to_preserve;
+    }
+    output_accessor[globalId] = op.finalize(accumulator);
+  }
+};  // namespace internal
+
+template <typename Index, Index LTP, Index LTR, bool BC_>
+struct ReductionPannel {
+  static EIGEN_CONSTEXPR Index LocalThreadSizeP = LTP;
+  static EIGEN_CONSTEXPR Index LocalThreadSizeR = LTR;
+  static EIGEN_CONSTEXPR bool BC = BC_;
+};
+
+template <typename Self, typename Op, internal::reduction_dim rt>
+struct PartialReducerLauncher {
+  typedef typename Self::EvaluatorPointerType EvaluatorPointerType;
+  typedef typename Self::CoeffReturnType CoeffReturnType;
+  typedef typename Self::Storage Storage;
+  typedef typename Self::Index Index;
+  typedef ReductionPannel<typename Self::Index, EIGEN_SYCL_LOCAL_THREAD_DIM0, EIGEN_SYCL_LOCAL_THREAD_DIM1, true>
+      PannelParameters;
+
+  typedef PartialReductionKernel<Self, Op, PannelParameters, rt> SyclReducerKerneType;
+
+  static bool run(const Self &self, const Op &reducer, const Eigen::GpuDevice &dev, EvaluatorPointerType output,
+                  Index num_coeffs_to_reduce, Index num_coeffs_to_preserve) {
+    Index roundUpP = roundUp(num_coeffs_to_preserve, PannelParameters::LocalThreadSizeP);
+
+    // getPowerOfTwo makes sure local range is power of 2 and <=
+    // maxSyclThreadPerBlock this will help us to avoid extra check on the
+    // kernel
+    static_assert(!((PannelParameters::LocalThreadSizeP * PannelParameters::LocalThreadSizeR) &
+                    (PannelParameters::LocalThreadSizeP * PannelParameters::LocalThreadSizeR - 1)),
+                  "The Local thread size must be a power of 2 for the reduction "
+                  "operation");
+
+    EIGEN_CONSTEXPR Index localRange = PannelParameters::LocalThreadSizeP * PannelParameters::LocalThreadSizeR;
+    // In this step, we force the code not to be more than 2-step reduction:
+    // Our empirical research shows that if each thread reduces at least 64
+    // elemnts individually, we get better performance. However, this can change
+    // on different platforms. In this step we force the code not to be
+    // morthan step reduction: Our empirical research shows that for inner_most
+    // dim reducer, it is better to have 8 group in a reduce dimension for sizes
+    // > 1024 to achieve the best performance.
+    const Index reductionPerThread = 64;
+    Index cu = dev.getPowerOfTwo(dev.getNumSyclMultiProcessors(), true);
+    const Index pNumGroups = roundUpP / PannelParameters::LocalThreadSizeP;
+    Index rGroups = (cu + pNumGroups - 1) / pNumGroups;
+    const Index rNumGroups = num_coeffs_to_reduce > reductionPerThread * localRange ? std::min(rGroups, localRange) : 1;
+    const Index globalRange = pNumGroups * rNumGroups * localRange;
+
+    EIGEN_CONSTEXPR Index scratchSize =
+        PannelParameters::LocalThreadSizeR * (PannelParameters::LocalThreadSizeP + PannelParameters::BC);
+    auto thread_range = sycl::nd_range<1>(sycl::range<1>(globalRange), sycl::range<1>(localRange));
+    if (rNumGroups > 1) {
+      CoeffReturnType *temp_pointer = static_cast<CoeffReturnType *>(
+          dev.allocate_temp(num_coeffs_to_preserve * rNumGroups * sizeof(CoeffReturnType)));
+      EvaluatorPointerType temp_accessor = dev.get(temp_pointer);
+      dev.template unary_kernel_launcher<CoeffReturnType, SyclReducerKerneType>(
+          self, temp_accessor, thread_range, scratchSize, reducer, pNumGroups, rNumGroups, num_coeffs_to_preserve,
+          num_coeffs_to_reduce);
+
+      typedef SecondStepPartialReduction<CoeffReturnType, Index, EvaluatorPointerType, EvaluatorPointerType, Op>
+          SecondStepPartialReductionKernel;
+
+      dev.template unary_kernel_launcher<CoeffReturnType, SecondStepPartialReductionKernel>(
+          temp_accessor, output,
+          sycl::nd_range<1>(sycl::range<1>(pNumGroups * localRange), sycl::range<1>(localRange)), Index(1),
+          reducer, num_coeffs_to_preserve, rNumGroups);
+
+      self.device().deallocate_temp(temp_pointer);
+    } else {
+      dev.template unary_kernel_launcher<CoeffReturnType, SyclReducerKerneType>(
+          self, output, thread_range, scratchSize, reducer, pNumGroups, rNumGroups, num_coeffs_to_preserve,
+          num_coeffs_to_reduce);
+    }
+    return false;
+  }
+};
+}  // namespace internal
+
+namespace internal {
+
+template <typename Self, typename Op, bool Vectorizable>
+struct FullReducer<Self, Op, Eigen::GpuDevice, Vectorizable> {
+  typedef typename Self::CoeffReturnType CoeffReturnType;
+  typedef typename Self::EvaluatorPointerType EvaluatorPointerType;
+  static EIGEN_CONSTEXPR bool HasOptimizedImplementation = true;
+  static EIGEN_CONSTEXPR int PacketSize = Self::PacketAccess ? Self::PacketSize : 1;
+  static void run(const Self &self, Op &reducer, const Eigen::GpuDevice &dev, EvaluatorPointerType data) {
+    typedef typename conditional<Self::PacketAccess, typename Self::PacketReturnType, CoeffReturnType>::type OutType;
+    static_assert(!((EIGEN_SYCL_LOCAL_THREAD_DIM0 * EIGEN_SYCL_LOCAL_THREAD_DIM1) &
+                    (EIGEN_SYCL_LOCAL_THREAD_DIM0 * EIGEN_SYCL_LOCAL_THREAD_DIM1 - 1)),
+                  "The Local thread size must be a power of 2 for the reduction "
+                  "operation");
+    EIGEN_CONSTEXPR Index local_range = EIGEN_SYCL_LOCAL_THREAD_DIM0 * EIGEN_SYCL_LOCAL_THREAD_DIM1;
+
+    typename Self::Index inputSize = self.impl().dimensions().TotalSize();
+    // In this step we force the code not to be more than 2-step reduction:
+    // Our empirical research shows that if each thread reduces at least 512
+    // elemnts individually, we get better performance.
+    const Index reductionPerThread = 2048;
+    // const Index num_work_group =
+    Index reductionGroup = dev.getPowerOfTwo(
+        (inputSize + (reductionPerThread * local_range - 1)) / (reductionPerThread * local_range), true);
+    const Index num_work_group = std::min(reductionGroup, local_range);
+    // 1
+    // ? local_range
+    // : 1);
+    const Index global_range = num_work_group * local_range;
+
+    auto thread_range = sycl::nd_range<1>(sycl::range<1>(global_range), sycl::range<1>(local_range));
+    typedef internal::FullReductionKernelFunctor<Self, Op> reduction_kernel_t;
+    if (num_work_group > 1) {
+      CoeffReturnType *temp_pointer =
+          static_cast<CoeffReturnType *>(dev.allocate_temp(num_work_group * sizeof(CoeffReturnType)));
+      typename Self::EvaluatorPointerType tmp_global_accessor = dev.get(temp_pointer);
+      dev.template unary_kernel_launcher<OutType, reduction_kernel_t>(self, tmp_global_accessor, thread_range,
+                                                                      local_range, inputSize, reducer);
+
+      typedef internal::SecondStepFullReducer<CoeffReturnType, Op, EvaluatorPointerType,
+                                                          EvaluatorPointerType, Index>
+          GenericRKernel;
+      dev.template unary_kernel_launcher<CoeffReturnType, GenericRKernel>(
+          tmp_global_accessor, data,
+          sycl::nd_range<1>(sycl::range<1>(num_work_group), sycl::range<1>(num_work_group)), num_work_group,
+          reducer);
+
+      dev.deallocate_temp(temp_pointer);
+    } else {
+      dev.template unary_kernel_launcher<OutType, reduction_kernel_t>(self, data, thread_range, local_range, inputSize,
+                                                                      reducer);
+    }
+  }
+};
+// vectorizable inner_most most dim preserver
+// col reduction
+template <typename Self, typename Op>
+struct OuterReducer<Self, Op, Eigen::GpuDevice> {
+  static EIGEN_CONSTEXPR bool HasOptimizedImplementation = true;
+
+  static bool run(const Self &self, const Op &reducer, const Eigen::GpuDevice &dev,
+                  typename Self::EvaluatorPointerType output, typename Self::Index num_coeffs_to_reduce,
+                  typename Self::Index num_coeffs_to_preserve) {
+    return ::Eigen::internal::PartialReducerLauncher<
+        Self, Op, ::Eigen::internal::reduction_dim::outer_most>::run(self, reducer, dev, output,
+                                                                                 num_coeffs_to_reduce,
+                                                                                 num_coeffs_to_preserve);
+  }
+};
+// row reduction
+template <typename Self, typename Op>
+struct InnerReducer<Self, Op, Eigen::GpuDevice> {
+  static EIGEN_CONSTEXPR bool HasOptimizedImplementation = true;
+
+  static bool run(const Self &self, const Op &reducer, const Eigen::GpuDevice &dev,
+                  typename Self::EvaluatorPointerType output, typename Self::Index num_coeffs_to_reduce,
+                  typename Self::Index num_coeffs_to_preserve) {
+    return ::Eigen::internal::PartialReducerLauncher<
+        Self, Op, ::Eigen::internal::reduction_dim::inner_most>::run(self, reducer, dev, output,
+                                                                                 num_coeffs_to_reduce,
+                                                                                 num_coeffs_to_preserve);
+  }
+};
+
+// ArmgMax uses this kernel for partial reduction//
+// TODO(@mehdi.goli) come up with a better kernel
+// generic partial reduction
+template <typename Self, typename Op>
+struct GenericReducer<Self, Op, Eigen::GpuDevice> {
+  static EIGEN_CONSTEXPR bool HasOptimizedImplementation = false;
+  static bool run(const Self &self, const Op &reducer, const Eigen::GpuDevice &dev,
+                  typename Self::EvaluatorPointerType output, typename Self::Index num_values_to_reduce,
+                  typename Self::Index num_coeffs_to_preserve) {
+    typename Self::Index range, GRange, tileSize;
+    dev.parallel_for_setup(num_coeffs_to_preserve, tileSize, range, GRange);
+
+    dev.template unary_kernel_launcher<typename Self::CoeffReturnType,
+                                       internal::GenericRandomReducer<Self, Op>>(
+        self, output, sycl::nd_range<1>(sycl::range<1>(GRange), sycl::range<1>(tileSize)), Index(1),
+        reducer, range, (num_values_to_reduce != 0) ? num_values_to_reduce : static_cast<Index>(1));
+    return false;
+  }
+};
+
+}  // namespace internal
+}  // namespace Eigen
+
+#endif  // UNSUPPORTED_EIGEN_CXX11_SRC_TENSOR_TENSOR_REDUCTION_DPCPP_HPP
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorReductionSycl.h b/unsupported/Eigen/CXX11/src/Tensor/TensorReductionSycl.h
index 387c3edf4..ec68d65cf 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorReductionSycl.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorReductionSycl.h
@@ -74,7 +74,7 @@ struct OpDefiner<Eigen::internal::MeanReducer<CoeffReturnType>, CoeffReturnType,
 template <typename CoeffReturnType, typename OpType, typename InputAccessor, typename OutputAccessor, typename Index,
           Index local_range>
 struct SecondStepFullReducer {
-  typedef cl::sycl::accessor<CoeffReturnType, 1, cl::sycl::access::mode::read_write, cl::sycl::access::target::local>
+  typedef sycl::accessor<CoeffReturnType, 1, sycl::access::mode::read_write, sycl::access::target::local>
       LocalAccessor;
   typedef OpDefiner<OpType, CoeffReturnType, Index, true> OpDef;
   typedef typename OpDef::type Op;
@@ -85,7 +85,7 @@ struct SecondStepFullReducer {
   SecondStepFullReducer(LocalAccessor scratch_, InputAccessor aI_, OutputAccessor outAcc_, OpType op_)
       : scratch(scratch_), aI(aI_), outAcc(outAcc_), op(OpDef::get_op(op_)) {}
 
-  void operator()(cl::sycl::nd_item<1> itemID) {
+  void operator()(sycl::nd_item<1> itemID) {
     // Our empirical research shows that the best performance will be achieved
     // when there is only one element per thread to reduce in the second step.
     // in this step the second step reduction time is almost negligible.
@@ -102,7 +102,7 @@ struct SecondStepFullReducer {
     scratchptr[localid] = op.finalize(accumulator);
 #pragma unroll 8
     for (Index offset = itemID.get_local_range(0) / 2; offset > 0; offset /= 2) {
-      itemID.barrier(cl::sycl::access::fence_space::local_space);
+      itemID.barrier(sycl::access::fence_space::local_space);
       if (localid < offset) {
         op.reduce(scratchptr[localid + offset], &accumulator);
         scratchptr[localid] = op.finalize(accumulator);
@@ -129,7 +129,7 @@ class FullReductionKernelFunctor {
   typedef
       typename ::Eigen::internal::conditional<(Evaluator::ReducerTraits::PacketAccess & Evaluator::InputPacketAccess),
                                               PacketReturnType, CoeffReturnType>::type OutType;
-  typedef cl::sycl::accessor<OutType, 1, cl::sycl::access::mode::read_write, cl::sycl::access::target::local>
+  typedef sycl::accessor<OutType, 1, sycl::access::mode::read_write, sycl::access::target::local>
       LocalAccessor;
   LocalAccessor scratch;
   Evaluator evaluator;
@@ -141,11 +141,11 @@ class FullReductionKernelFunctor {
                              Index rng_, OpType op_)
       : scratch(scratch_), evaluator(evaluator_), final_output(final_output_), rng(rng_), op(OpDef::get_op(op_)) {}
 
-  void operator()(cl::sycl::nd_item<1> itemID) { compute_reduction(itemID); }
+  void operator()(sycl::nd_item<1> itemID) { compute_reduction(itemID); }
 
   template <bool Vect = (Evaluator::ReducerTraits::PacketAccess & Evaluator::InputPacketAccess)>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename ::Eigen::internal::enable_if<Vect>::type compute_reduction(
-      const cl::sycl::nd_item<1> &itemID) {
+      const sycl::nd_item<1> &itemID) {
     auto output_ptr = final_output.get_pointer();
     Index VectorizedRange = (rng / Evaluator::PacketSize) * Evaluator::PacketSize;
     Index globalid = itemID.get_global_id(0);
@@ -171,7 +171,7 @@ class FullReductionKernelFunctor {
     // reduction parts // Local size is always power of 2
     EIGEN_UNROLL_LOOP
     for (Index offset = local_range / 2; offset > 0; offset /= 2) {
-      itemID.barrier(cl::sycl::access::fence_space::local_space);
+      itemID.barrier(sycl::access::fence_space::local_space);
       if (localid < offset) {
         op.template reducePacket<PacketReturnType>(scratch[localid + offset], &packetAccumulator);
         scratch[localid] = op.template finalizePacket<PacketReturnType>(packetAccumulator);
@@ -185,7 +185,7 @@ class FullReductionKernelFunctor {
 
   template <bool Vect = (Evaluator::ReducerTraits::PacketAccess & Evaluator::InputPacketAccess)>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename ::Eigen::internal::enable_if<!Vect>::type compute_reduction(
-      const cl::sycl::nd_item<1> &itemID) {
+      const sycl::nd_item<1> &itemID) {
     auto output_ptr = final_output.get_pointer();
     Index globalid = itemID.get_global_id(0);
     Index localid = itemID.get_local_id(0);
@@ -200,7 +200,7 @@ class FullReductionKernelFunctor {
     // reduction parts. the local size is always power of 2
     EIGEN_UNROLL_LOOP
     for (Index offset = local_range / 2; offset > 0; offset /= 2) {
-      itemID.barrier(cl::sycl::access::fence_space::local_space);
+      itemID.barrier(sycl::access::fence_space::local_space);
       if (localid < offset) {
         op.reduce(scratch[localid + offset], &accumulator);
         scratch[localid] = op.finalize(accumulator);
@@ -229,7 +229,7 @@ class GenericNondeterministicReducer {
         range(range_),
         num_values_to_reduce(num_values_to_reduce_) {}
 
-  void operator()(cl::sycl::nd_item<1> itemID) {
+  void operator()(sycl::nd_item<1> itemID) {
     auto output_accessor_ptr = output_accessor.get_pointer();
     /// const cast added as a naive solution to solve the qualifier drop error
     Index globalid = static_cast<Index>(itemID.get_global_linear_id());
@@ -258,7 +258,7 @@ struct PartialReductionKernel {
   typedef typename Evaluator::Index Index;
   typedef OpDefiner<OpType, CoeffReturnType, Index, false> OpDef;
   typedef typename OpDef::type Op;
-  typedef cl::sycl::accessor<CoeffReturnType, 1, cl::sycl::access::mode::read_write, cl::sycl::access::target::local>
+  typedef sycl::accessor<CoeffReturnType, 1, sycl::access::mode::read_write, sycl::access::target::local>
       ScratchAcc;
   ScratchAcc scratch;
   Evaluator evaluator;
@@ -300,7 +300,7 @@ struct PartialReductionKernel {
       global_offset += per_thread_global_stride;
     }
   }
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(cl::sycl::nd_item<1> itemID) {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(sycl::nd_item<1> itemID) {
     const Index linearLocalThreadId = itemID.get_local_id(0);
     Index pLocalThreadId = rt == reduction_dim::outer_most ? linearLocalThreadId % PannelParameters::LocalThreadSizeP
                                                            : linearLocalThreadId / PannelParameters::LocalThreadSizeR;
@@ -333,7 +333,7 @@ struct PartialReductionKernel {
      * id and the one on the other half of the vector. */
     auto out_scratch_ptr =
         scratchPtr + (pLocalThreadId + (rLocalThreadId * (PannelParameters::LocalThreadSizeP + PannelParameters::BC)));
-    itemID.barrier(cl::sycl::access::fence_space::local_space);
+    itemID.barrier(sycl::access::fence_space::local_space);
     if (rt == reduction_dim::inner_most) {
       accumulator = *out_scratch_ptr;
     }
@@ -351,7 +351,7 @@ struct PartialReductionKernel {
        * execution continues (strictly speaking, all threads within
        * a single work-group - there is no co-ordination between
        * work-groups, only work-items). */
-      itemID.barrier(cl::sycl::access::fence_space::local_space);
+      itemID.barrier(sycl::access::fence_space::local_space);
     }
 
     if (rLocalThreadId == 0 && (globalPId < num_coeffs_to_preserve)) {
@@ -364,7 +364,7 @@ template <typename OutScalar, typename Index, typename InputAccessor, typename O
 struct SecondStepPartialReduction {
   typedef OpDefiner<OpType, OutScalar, Index, false> OpDef;
   typedef typename OpDef::type Op;
-  typedef cl::sycl::accessor<OutScalar, 1, cl::sycl::access::mode::read_write, cl::sycl::access::target::local>
+  typedef sycl::accessor<OutScalar, 1, sycl::access::mode::read_write, sycl::access::target::local>
       ScratchAccessor;
   InputAccessor input_accessor;
   OutputAccessor output_accessor;
@@ -382,7 +382,7 @@ struct SecondStepPartialReduction {
         num_coeffs_to_preserve(num_coeffs_to_preserve_),
         num_coeffs_to_reduce(num_coeffs_to_reduce_) {}
 
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(cl::sycl::nd_item<1> itemID) {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(sycl::nd_item<1> itemID) {
     const Index globalId = itemID.get_global_id(0);
 
     if (globalId >= num_coeffs_to_preserve) return;
@@ -447,7 +447,7 @@ struct PartialReducerLauncher {
 
     EIGEN_CONSTEXPR Index scratchSize =
         PannelParameters::LocalThreadSizeR * (PannelParameters::LocalThreadSizeP + PannelParameters::BC);
-    auto thread_range = cl::sycl::nd_range<1>(cl::sycl::range<1>(globalRange), cl::sycl::range<1>(localRange));
+    auto thread_range = sycl::nd_range<1>(sycl::range<1>(globalRange), sycl::range<1>(localRange));
     if (rNumGroups > 1) {
       CoeffReturnType *temp_pointer = static_cast<CoeffReturnType *>(
           dev.allocate_temp(num_coeffs_to_preserve * rNumGroups * sizeof(CoeffReturnType)));
@@ -461,7 +461,7 @@ struct PartialReducerLauncher {
 
       dev.template unary_kernel_launcher<CoeffReturnType, SecondStepPartialReductionKernel>(
           temp_accessor, output,
-          cl::sycl::nd_range<1>(cl::sycl::range<1>(pNumGroups * localRange), cl::sycl::range<1>(localRange)), Index(1),
+          sycl::nd_range<1>(sycl::range<1>(pNumGroups * localRange), sycl::range<1>(localRange)), Index(1),
           reducer, num_coeffs_to_preserve, rNumGroups);
 
       self.device().deallocate_temp(temp_pointer);
@@ -506,7 +506,7 @@ struct FullReducer<Self, Op, Eigen::SyclDevice, Vectorizable> {
     // : 1);
     const Index global_range = num_work_group * local_range;
 
-    auto thread_range = cl::sycl::nd_range<1>(cl::sycl::range<1>(global_range), cl::sycl::range<1>(local_range));
+    auto thread_range = sycl::nd_range<1>(sycl::range<1>(global_range), sycl::range<1>(local_range));
     typedef TensorSycl::internal::FullReductionKernelFunctor<Self, Op, local_range> reduction_kernel_t;
     if (num_work_group > 1) {
       CoeffReturnType *temp_pointer =
@@ -520,7 +520,7 @@ struct FullReducer<Self, Op, Eigen::SyclDevice, Vectorizable> {
           GenericRKernel;
       dev.template unary_kernel_launcher<CoeffReturnType, GenericRKernel>(
           tmp_global_accessor, data,
-          cl::sycl::nd_range<1>(cl::sycl::range<1>(num_work_group), cl::sycl::range<1>(num_work_group)), num_work_group,
+          sycl::nd_range<1>(sycl::range<1>(num_work_group), sycl::range<1>(num_work_group)), num_work_group,
           reducer);
 
       dev.deallocate_temp(temp_pointer);
@@ -574,7 +574,7 @@ struct GenericReducer<Self, Op, Eigen::SyclDevice> {
 
     dev.template unary_kernel_launcher<typename Self::CoeffReturnType,
                                        TensorSycl::internal::GenericNondeterministicReducer<Self, Op>>(
-        self, output, cl::sycl::nd_range<1>(cl::sycl::range<1>(GRange), cl::sycl::range<1>(tileSize)), Index(1),
+        self, output, sycl::nd_range<1>(sycl::range<1>(GRange), sycl::range<1>(tileSize)), Index(1),
         reducer, range, (num_values_to_reduce != 0) ? num_values_to_reduce : static_cast<Index>(1));
     return false;
   }
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorReverse.h b/unsupported/Eigen/CXX11/src/Tensor/TensorReverse.h
index 2fc85c13c..83fbd5ef6 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorReverse.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorReverse.h
@@ -385,7 +385,7 @@ struct TensorEvaluator<const TensorReverseOp<ReverseDimensions, ArgType>, Device
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorScan.h b/unsupported/Eigen/CXX11/src/Tensor/TensorScan.h
index 9e3b1a0b9..5782cc585 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorScan.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorScan.h
@@ -508,7 +508,7 @@ struct TensorEvaluator<const TensorScanOp<Op, ArgType>, Device> {
 
 #ifdef EIGEN_USE_SYCL
  // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
     m_output.bind(cgh);
   }
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorScanDpcpp.h b/unsupported/Eigen/CXX11/src/Tensor/TensorScanDpcpp.h
new file mode 100644
index 000000000..940967140
--- /dev/null
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorScanDpcpp.h
@@ -0,0 +1,515 @@
+// This file is part of Eigen, a lightweight C++ template library
+// for linear algebra.
+//
+// Mehdi Goli    Codeplay Software Ltd.
+// Ralph Potter  Codeplay Software Ltd.
+// Luke Iwanski  Codeplay Software Ltd.
+// Contact: <eigen@codeplay.com>
+//
+// This Source Code Form is subject to the terms of the Mozilla
+// Public License v. 2.0. If a copy of the MPL was not distributed
+// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+/*****************************************************************
+ * TensorScanDpcpp.h
+ *
+ * \brief:
+ *  Tensor Scan Dpcpp implement the extend  version of
+ * "Efficient parallel scan algorithms for GPUs." .for Tensor operations.
+ * The algorithm requires up to 3 stage (consequently 3 kernels) depending on
+ * the size of the tensor. In the first kernel (ScanKernelFunctor), each
+ * threads within the work-group individually reduces the allocated elements per
+ * thread in order to reduces the total number of blocks. In the next step all
+ * thread within the work-group will reduce the associated blocks into the
+ * temporary buffers. In the next kernel(ScanBlockKernelFunctor), the temporary
+ * buffer is given as an input and all the threads within a work-group scan and
+ * reduces the boundaries between the blocks (generated from the previous
+ * kernel). and write the data on the temporary buffer. If the second kernel is
+ * required, the third and final kerenl (ScanAdjustmentKernelFunctor) will
+ * adjust the final result into the output buffer.
+ * The original algorithm for the parallel prefix sum can be found here:
+ *
+ * Sengupta, Shubhabrata, Mark Harris, and Michael Garland. "Efficient parallel
+ * scan algorithms for GPUs." NVIDIA, Santa Clara, CA, Tech. Rep. NVR-2008-003
+ *1, no. 1 (2008): 1-17.
+ *****************************************************************/
+
+#if defined(EIGEN_USE_DPCPP) && defined(EIGEN_USE_GPU) && !defined(UNSUPPORTED_EIGEN_CXX11_SRC_TENSOR_TENSOR_SCAN_DPCPP_HPP)
+#define UNSUPPORTED_EIGEN_CXX11_SRC_TENSOR_TENSOR_SCAN_DPCPP_HPP
+
+namespace Eigen {
+namespace internal {
+
+
+#define EIGEN_DPCPP_LOCAL_THREAD_DIM0 16
+#define EIGEN_DPCPP_LOCAL_THREAD_DIM1 16
+
+#ifndef EIGEN_DPCPP_MAX_GLOBAL_RANGE
+#define EIGEN_DPCPP_MAX_GLOBAL_RANGE (EIGEN_DPCPP_LOCAL_THREAD_DIM0 * EIGEN_DPCPP_LOCAL_THREAD_DIM1 * 4)
+#endif
+
+template <typename index_t>
+struct ScanParameters {
+  // must be power of 2
+  static EIGEN_CONSTEXPR index_t ScanPerThread = 8;
+  const index_t total_size;
+  const index_t non_scan_size;
+  const index_t scan_size;
+  const index_t non_scan_stride;
+  const index_t scan_stride;
+  const index_t panel_threads;
+  const index_t group_threads;
+  const index_t block_threads;
+  const index_t elements_per_group;
+  const index_t elements_per_block;
+  const index_t loop_range;
+
+  ScanParameters(index_t total_size_, index_t non_scan_size_, index_t scan_size_, index_t non_scan_stride_,
+                 index_t scan_stride_, index_t panel_threads_, index_t group_threads_, index_t block_threads_,
+                 index_t elements_per_group_, index_t elements_per_block_, index_t loop_range_)
+      : total_size(total_size_),
+        non_scan_size(non_scan_size_),
+        scan_size(scan_size_),
+        non_scan_stride(non_scan_stride_),
+        scan_stride(scan_stride_),
+        panel_threads(panel_threads_),
+        group_threads(group_threads_),
+        block_threads(block_threads_),
+        elements_per_group(elements_per_group_),
+        elements_per_block(elements_per_block_),
+        loop_range(loop_range_) {}
+};
+
+enum class scan_step { first, second };
+template <typename Evaluator, typename CoeffReturnType, typename OutAccessor, typename Op, typename Index,
+          scan_step stp>
+struct ScanKernelFunctor {
+  typedef sycl::accessor<CoeffReturnType, 1, sycl::access::mode::read_write, sycl::access::target::local>
+      LocalAccessor;
+  static EIGEN_CONSTEXPR int PacketSize = ScanParameters<Index>::ScanPerThread / 2;
+
+  LocalAccessor scratch;
+  Evaluator dev_eval;
+  OutAccessor out_accessor;
+  OutAccessor temp_accessor;
+  const ScanParameters<Index> scanParameters;
+  Op accumulator;
+  const bool inclusive;
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE ScanKernelFunctor(LocalAccessor scratch_, const Evaluator dev_eval_,
+                                                          OutAccessor out_accessor_, OutAccessor temp_accessor_,
+                                                          const ScanParameters<Index> scanParameters_, Op accumulator_,
+                                                          const bool inclusive_)
+      : scratch(scratch_),
+        dev_eval(dev_eval_),
+        out_accessor(out_accessor_),
+        temp_accessor(temp_accessor_),
+        scanParameters(scanParameters_),
+        accumulator(accumulator_),
+        inclusive(inclusive_) {}
+
+  template <scan_step sst = stp, typename Input>
+  typename ::Eigen::internal::enable_if<sst == scan_step::first, CoeffReturnType>::type EIGEN_DEVICE_FUNC
+      EIGEN_STRONG_INLINE
+      read(const Input &inpt, Index global_id) const {
+    return inpt.coeff(global_id);
+  }
+
+  template <scan_step sst = stp, typename Input>
+  typename ::Eigen::internal::enable_if<sst != scan_step::first, CoeffReturnType>::type EIGEN_DEVICE_FUNC
+      EIGEN_STRONG_INLINE
+      read(const Input &inpt, Index global_id) const {
+    return inpt[global_id];
+  }
+
+  template <scan_step sst = stp, typename InclusiveOp>
+  typename ::Eigen::internal::enable_if<sst == scan_step::first>::type EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+  first_step_inclusive_Operation(InclusiveOp inclusive_op) const {
+    inclusive_op();
+  }
+
+  template <scan_step sst = stp, typename InclusiveOp>
+  typename ::Eigen::internal::enable_if<sst != scan_step::first>::type EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+  first_step_inclusive_Operation(InclusiveOp) const {}
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(sycl::nd_item<1> itemID) const {
+    auto out_ptr = out_accessor;
+    auto tmp_ptr = temp_accessor;
+    auto scratch_ptr = scratch.get_pointer().get();
+
+    for (Index loop_offset = 0; loop_offset < scanParameters.loop_range; loop_offset++) {
+      Index data_offset = (itemID.get_global_id(0) + (itemID.get_global_range(0) * loop_offset));
+      Index tmp = data_offset % scanParameters.panel_threads;
+      const Index panel_id = data_offset / scanParameters.panel_threads;
+      const Index group_id = tmp / scanParameters.group_threads;
+      tmp = tmp % scanParameters.group_threads;
+      const Index block_id = tmp / scanParameters.block_threads;
+      const Index local_id = tmp % scanParameters.block_threads;
+      // we put one element per packet in scratch_mem
+      const Index scratch_stride = scanParameters.elements_per_block / PacketSize;
+      const Index scratch_offset = (itemID.get_local_id(0) / scanParameters.block_threads) * scratch_stride;
+      CoeffReturnType private_scan[ScanParameters<Index>::ScanPerThread];
+      CoeffReturnType inclusive_scan;
+      // the actual panel size is scan_size * non_scan_size.
+      // elements_per_panel is roundup to power of 2 for binary tree
+      const Index panel_offset = panel_id * scanParameters.scan_size * scanParameters.non_scan_size;
+      const Index group_offset = group_id * scanParameters.non_scan_stride;
+      // This will be effective when the size is bigger than elements_per_block
+      const Index block_offset = block_id * scanParameters.elements_per_block * scanParameters.scan_stride;
+      const Index thread_offset = (ScanParameters<Index>::ScanPerThread * local_id * scanParameters.scan_stride);
+      const Index global_offset = panel_offset + group_offset + block_offset + thread_offset;
+      Index next_elements = 0;
+      EIGEN_UNROLL_LOOP
+      for (int i = 0; i < ScanParameters<Index>::ScanPerThread; i++) {
+        Index global_id = global_offset + next_elements;
+        private_scan[i] = ((((block_id * scanParameters.elements_per_block) +
+                             (ScanParameters<Index>::ScanPerThread * local_id) + i) < scanParameters.scan_size) &&
+                           (global_id < scanParameters.total_size))
+                              ? read(dev_eval, global_id)
+                              : accumulator.initialize();
+        next_elements += scanParameters.scan_stride;
+      }
+      first_step_inclusive_Operation([&]() EIGEN_DEVICE_FUNC {
+        if (inclusive) {
+          inclusive_scan = private_scan[ScanParameters<Index>::ScanPerThread - 1];
+        }
+      });
+      // This for loop must be 2
+      EIGEN_UNROLL_LOOP
+      for (int packetIndex = 0; packetIndex < ScanParameters<Index>::ScanPerThread; packetIndex += PacketSize) {
+        Index private_offset = 1;
+        // build sum in place up the tree
+        EIGEN_UNROLL_LOOP
+        for (Index d = PacketSize >> 1; d > 0; d >>= 1) {
+          EIGEN_UNROLL_LOOP
+          for (Index l = 0; l < d; l++) {
+            Index ai = private_offset * (2 * l + 1) - 1 + packetIndex;
+            Index bi = private_offset * (2 * l + 2) - 1 + packetIndex;
+            CoeffReturnType accum = accumulator.initialize();
+            accumulator.reduce(private_scan[ai], &accum);
+            accumulator.reduce(private_scan[bi], &accum);
+            private_scan[bi] = accumulator.finalize(accum);
+          }
+          private_offset *= 2;
+        }
+        scratch_ptr[2 * local_id + (packetIndex / PacketSize) + scratch_offset] =
+            private_scan[PacketSize - 1 + packetIndex];
+        private_scan[PacketSize - 1 + packetIndex] = accumulator.initialize();
+        // traverse down tree & build scan
+        EIGEN_UNROLL_LOOP
+        for (Index d = 1; d < PacketSize; d *= 2) {
+          private_offset >>= 1;
+          EIGEN_UNROLL_LOOP
+          for (Index l = 0; l < d; l++) {
+            Index ai = private_offset * (2 * l + 1) - 1 + packetIndex;
+            Index bi = private_offset * (2 * l + 2) - 1 + packetIndex;
+            CoeffReturnType accum = accumulator.initialize();
+            accumulator.reduce(private_scan[ai], &accum);
+            accumulator.reduce(private_scan[bi], &accum);
+            private_scan[ai] = private_scan[bi];
+            private_scan[bi] = accumulator.finalize(accum);
+          }
+        }
+      }
+
+      Index offset = 1;
+      // build sum in place up the tree
+      for (Index d = scratch_stride >> 1; d > 0; d >>= 1) {
+        // Synchronise
+        sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+        if (local_id < d) {
+          Index ai = offset * (2 * local_id + 1) - 1 + scratch_offset;
+          Index bi = offset * (2 * local_id + 2) - 1 + scratch_offset;
+          CoeffReturnType accum = accumulator.initialize();
+          accumulator.reduce(scratch_ptr[ai], &accum);
+          accumulator.reduce(scratch_ptr[bi], &accum);
+          scratch_ptr[bi] = accumulator.finalize(accum);
+        }
+        offset *= 2;
+      }
+      // Synchronise
+      sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+      // next step optimisation
+      if (local_id == 0) {
+        if (((scanParameters.elements_per_group / scanParameters.elements_per_block) > 1)) {
+          const Index temp_id = panel_id * (scanParameters.elements_per_group / scanParameters.elements_per_block) *
+                                    scanParameters.non_scan_size +
+                                group_id * (scanParameters.elements_per_group / scanParameters.elements_per_block) +
+                                block_id;
+          tmp_ptr[temp_id] = scratch_ptr[scratch_stride - 1 + scratch_offset];
+        }
+        // clear the last element
+        scratch_ptr[scratch_stride - 1 + scratch_offset] = accumulator.initialize();
+      }
+      // traverse down tree & build scan
+      for (Index d = 1; d < scratch_stride; d *= 2) {
+        offset >>= 1;
+        // Synchronise
+        sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+        if (local_id < d) {
+          Index ai = offset * (2 * local_id + 1) - 1 + scratch_offset;
+          Index bi = offset * (2 * local_id + 2) - 1 + scratch_offset;
+          CoeffReturnType accum = accumulator.initialize();
+          accumulator.reduce(scratch_ptr[ai], &accum);
+          accumulator.reduce(scratch_ptr[bi], &accum);
+          scratch_ptr[ai] = scratch_ptr[bi];
+          scratch_ptr[bi] = accumulator.finalize(accum);
+        }
+      }
+      // Synchronise
+      sycl::group_barrier(itemID.get_group(), sycl::memory_scope_work_group);
+
+      // This for loop must be 2
+      EIGEN_UNROLL_LOOP
+      for (int packetIndex = 0; packetIndex < ScanParameters<Index>::ScanPerThread; packetIndex += PacketSize) {
+        EIGEN_UNROLL_LOOP
+        for (Index i = 0; i < PacketSize; i++) {
+          CoeffReturnType accum = private_scan[packetIndex + i];
+          accumulator.reduce(scratch_ptr[2 * local_id + (packetIndex / PacketSize) + scratch_offset], &accum);
+          private_scan[packetIndex + i] = accumulator.finalize(accum);
+        }
+      }
+      first_step_inclusive_Operation([&]() EIGEN_DEVICE_FUNC {
+        if (inclusive) {
+          accumulator.reduce(private_scan[ScanParameters<Index>::ScanPerThread - 1], &inclusive_scan);
+          private_scan[0] = accumulator.finalize(inclusive_scan);
+        }
+      });
+      next_elements = 0;
+      // right the first set of private param
+      EIGEN_UNROLL_LOOP
+      for (Index i = 0; i < ScanParameters<Index>::ScanPerThread; i++) {
+        Index global_id = global_offset + next_elements;
+        if ((((block_id * scanParameters.elements_per_block) + (ScanParameters<Index>::ScanPerThread * local_id) + i) <
+             scanParameters.scan_size) &&
+            (global_id < scanParameters.total_size)) {
+          Index private_id = (i * !inclusive) + (((i + 1) % ScanParameters<Index>::ScanPerThread) * (inclusive));
+          out_ptr[global_id] = private_scan[private_id];
+        }
+        next_elements += scanParameters.scan_stride;
+      }
+    }  // end for loop
+  }
+};
+
+template <typename CoeffReturnType, typename InAccessor, typename OutAccessor, typename Op, typename Index>
+struct ScanAdjustmentKernelFunctor {
+  typedef sycl::accessor<CoeffReturnType, 1, sycl::access::mode::read_write, sycl::access::target::local>
+      LocalAccessor;
+  static EIGEN_CONSTEXPR int PacketSize = ScanParameters<Index>::ScanPerThread / 2;
+  InAccessor in_accessor;
+  OutAccessor out_accessor;
+  const ScanParameters<Index> scanParameters;
+  Op accumulator;
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE ScanAdjustmentKernelFunctor(LocalAccessor, InAccessor in_accessor_,
+                                                                    OutAccessor out_accessor_,
+                                                                    const ScanParameters<Index> scanParameters_,
+                                                                    Op accumulator_)
+      : in_accessor(in_accessor_),
+        out_accessor(out_accessor_),
+        scanParameters(scanParameters_),
+        accumulator(accumulator_) {}
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(sycl::nd_item<1> itemID) const {
+    auto in_ptr = in_accessor;
+    auto out_ptr = out_accessor;
+
+    for (Index loop_offset = 0; loop_offset < scanParameters.loop_range; loop_offset++) {
+      Index data_offset = (itemID.get_global_id(0) + (itemID.get_global_range(0) * loop_offset));
+      Index tmp = data_offset % scanParameters.panel_threads;
+      const Index panel_id = data_offset / scanParameters.panel_threads;
+      const Index group_id = tmp / scanParameters.group_threads;
+      tmp = tmp % scanParameters.group_threads;
+      const Index block_id = tmp / scanParameters.block_threads;
+      const Index local_id = tmp % scanParameters.block_threads;
+
+      // the actual panel size is scan_size * non_scan_size.
+      // elements_per_panel is roundup to power of 2 for binary tree
+      const Index panel_offset = panel_id * scanParameters.scan_size * scanParameters.non_scan_size;
+      const Index group_offset = group_id * scanParameters.non_scan_stride;
+      // This will be effective when the size is bigger than elements_per_block
+      const Index block_offset = block_id * scanParameters.elements_per_block * scanParameters.scan_stride;
+      const Index thread_offset = ScanParameters<Index>::ScanPerThread * local_id * scanParameters.scan_stride;
+
+      const Index global_offset = panel_offset + group_offset + block_offset + thread_offset;
+      const Index block_size = scanParameters.elements_per_group / scanParameters.elements_per_block;
+      const Index in_id = (panel_id * block_size * scanParameters.non_scan_size) + (group_id * block_size) + block_id;
+      CoeffReturnType adjust_val = in_ptr[in_id];
+
+      Index next_elements = 0;
+      EIGEN_UNROLL_LOOP
+      for (Index i = 0; i < ScanParameters<Index>::ScanPerThread; i++) {
+        Index global_id = global_offset + next_elements;
+        if ((((block_id * scanParameters.elements_per_block) + (ScanParameters<Index>::ScanPerThread * local_id) + i) <
+             scanParameters.scan_size) &&
+            (global_id < scanParameters.total_size)) {
+          CoeffReturnType accum = adjust_val;
+          accumulator.reduce(out_ptr[global_id], &accum);
+          out_ptr[global_id] = accumulator.finalize(accum);
+        }
+        next_elements += scanParameters.scan_stride;
+      }
+    }
+  }
+};
+
+template <typename Index>
+struct ScanInfo {
+  const Index &total_size;
+  const Index &scan_size;
+  const Index &panel_size;
+  const Index &non_scan_size;
+  const Index &scan_stride;
+  const Index &non_scan_stride;
+
+  Index max_elements_per_block;
+  Index block_size;
+  Index panel_threads;
+  Index group_threads;
+  Index block_threads;
+  Index elements_per_group;
+  Index elements_per_block;
+  Index loop_range;
+  Index global_range;
+  Index local_range;
+  const GpuDevice &dev;
+  EIGEN_STRONG_INLINE ScanInfo(const Index &total_size_, const Index &scan_size_, const Index &panel_size_,
+                               const Index &non_scan_size_, const Index &scan_stride_, const Index &non_scan_stride_,
+                               const GpuDevice &dev_)
+      : total_size(total_size_),
+        scan_size(scan_size_),
+        panel_size(panel_size_),
+        non_scan_size(non_scan_size_),
+        scan_stride(scan_stride_),
+        non_scan_stride(non_scan_stride_),
+        dev(dev_) {
+    // must be power of 2
+    local_range = std::min(Index(dev.getNearestPowerOfTwoWorkGroupSize()),
+                           Index(EIGEN_DPCPP_LOCAL_THREAD_DIM0 * EIGEN_DPCPP_LOCAL_THREAD_DIM1));
+
+    max_elements_per_block = local_range * ScanParameters<Index>::ScanPerThread;
+
+    elements_per_group =
+        dev.getPowerOfTwo(Index(roundUp(Index(scan_size), ScanParameters<Index>::ScanPerThread)), true);
+    const Index elements_per_panel = elements_per_group * non_scan_size;
+    elements_per_block = std::min(Index(elements_per_group), Index(max_elements_per_block));
+    panel_threads = elements_per_panel / ScanParameters<Index>::ScanPerThread;
+    group_threads = elements_per_group / ScanParameters<Index>::ScanPerThread;
+    block_threads = elements_per_block / ScanParameters<Index>::ScanPerThread;
+    block_size = elements_per_group / elements_per_block;
+#ifdef EIGEN_DPCPP_MAX_GLOBAL_RANGE
+    const Index max_threads = std::min(Index(panel_threads * panel_size), Index(EIGEN_DPCPP_MAX_GLOBAL_RANGE));
+#else
+    const Index max_threads = panel_threads * panel_size;
+#endif
+    global_range = roundUp(max_threads, local_range);
+    loop_range = Index(
+        std::ceil(double(elements_per_panel * panel_size) / (global_range * ScanParameters<Index>::ScanPerThread)));
+  }
+  inline ScanParameters<Index> get_scan_parameter() {
+    return ScanParameters<Index>(total_size, non_scan_size, scan_size, non_scan_stride, scan_stride, panel_threads,
+                                 group_threads, block_threads, elements_per_group, elements_per_block, loop_range);
+  }
+  inline sycl::nd_range<1> get_thread_range() {
+    return sycl::nd_range<1>(sycl::range<1>(global_range), sycl::range<1>(local_range));
+  }
+};
+
+template <typename EvaluatorPointerType, typename CoeffReturnType, typename Reducer, typename Index>
+struct DPCPPAdjustBlockOffset {
+  EIGEN_STRONG_INLINE static void adjust_scan_block_offset(EvaluatorPointerType in_ptr, EvaluatorPointerType out_ptr,
+                                                           Reducer &accumulator, const Index total_size,
+                                                           const Index scan_size, const Index panel_size,
+                                                           const Index non_scan_size, const Index scan_stride,
+                                                           const Index non_scan_stride, const GpuDevice &dev) {
+    auto scan_info =
+        ScanInfo<Index>(total_size, scan_size, panel_size, non_scan_size, scan_stride, non_scan_stride, dev);
+
+    typedef ScanAdjustmentKernelFunctor<CoeffReturnType, EvaluatorPointerType, EvaluatorPointerType, Reducer, Index>
+        AdjustFuctor;
+    dev.template unary_kernel_launcher<CoeffReturnType, AdjustFuctor>(in_ptr, out_ptr, scan_info.get_thread_range(),
+                                                                      scan_info.max_elements_per_block,
+                                                                      scan_info.get_scan_parameter(), accumulator);
+  }
+};
+
+template <typename CoeffReturnType, scan_step stp>
+struct ScanLauncher_impl {
+  template <typename Input, typename EvaluatorPointerType, typename Reducer, typename Index>
+  EIGEN_STRONG_INLINE static void scan_block(Input in_ptr, EvaluatorPointerType out_ptr, Reducer &accumulator,
+                                             const Index total_size, const Index scan_size, const Index panel_size,
+                                             const Index non_scan_size, const Index scan_stride,
+                                             const Index non_scan_stride, const bool inclusive,
+                                             const GpuDevice &dev) {
+    auto scan_info =
+        ScanInfo<Index>(total_size, scan_size, panel_size, non_scan_size, scan_stride, non_scan_stride, dev);
+    const Index temp_pointer_size = scan_info.block_size * non_scan_size * panel_size;
+    const Index scratch_size = scan_info.max_elements_per_block / (ScanParameters<Index>::ScanPerThread / 2);
+    CoeffReturnType *temp_pointer =
+        static_cast<CoeffReturnType *>(dev.allocate_temp(temp_pointer_size * sizeof(CoeffReturnType)));
+    EvaluatorPointerType tmp_global_accessor = dev.get(temp_pointer);
+
+    typedef ScanKernelFunctor<Input, CoeffReturnType, EvaluatorPointerType, Reducer, Index, stp> ScanFunctor;
+    dev.template binary_kernel_launcher<CoeffReturnType, ScanFunctor>(
+        in_ptr, out_ptr, tmp_global_accessor, scan_info.get_thread_range(), scratch_size,
+        scan_info.get_scan_parameter(), accumulator, inclusive);
+
+    if (scan_info.block_size > 1) {
+      ScanLauncher_impl<CoeffReturnType, scan_step::second>::scan_block(
+          tmp_global_accessor, tmp_global_accessor, accumulator, temp_pointer_size, scan_info.block_size, panel_size,
+          non_scan_size, Index(1), scan_info.block_size, false, dev);
+
+      DPCPPAdjustBlockOffset<EvaluatorPointerType, CoeffReturnType, Reducer, Index>::adjust_scan_block_offset(
+          tmp_global_accessor, out_ptr, accumulator, total_size, scan_size, panel_size, non_scan_size, scan_stride,
+          non_scan_stride, dev);
+    }
+    dev.deallocate_temp(temp_pointer);
+  }
+};
+
+}  // namespace internal
+
+template <typename Self, typename Reducer, bool Vectorize>
+struct internal::ScanLauncher<Self, Reducer, GpuDevice, Vectorize> {
+  typedef typename Self::Index Index;
+  typedef typename Self::CoeffReturnType CoeffReturnType;
+  typedef typename Self::Storage Storage;
+  typedef typename Self::EvaluatorPointerType EvaluatorPointerType;
+  void operator()(Self &self, EvaluatorPointerType data) {
+    const Index total_size = internal::array_prod(self.dimensions());
+    const Index scan_size = self.size();
+    const Index scan_stride = self.stride();
+    // this is the scan op (can be sum or ...)
+    auto accumulator = self.accumulator();
+    auto inclusive = !self.exclusive();
+    auto consume_dim = self.consume_dim();
+    auto dev = self.device();
+
+    auto dims = self.inner().dimensions();
+
+    Index non_scan_size = 1;
+    Index panel_size = 1;
+    if (static_cast<int>(Self::Layout) == static_cast<int>(ColMajor)) {
+      for (int i = 0; i < consume_dim; i++) {
+        non_scan_size *= dims[i];
+      }
+      for (int i = consume_dim + 1; i < Self::NumDims; i++) {
+        panel_size *= dims[i];
+      }
+    } else {
+      for (int i = Self::NumDims - 1; i > consume_dim; i--) {
+        non_scan_size *= dims[i];
+      }
+      for (int i = consume_dim - 1; i >= 0; i--) {
+        panel_size *= dims[i];
+      }
+    }
+    const Index non_scan_stride = (scan_stride > 1) ? 1 : scan_size;
+    auto eval_impl = self.inner();
+    internal::ScanLauncher_impl<CoeffReturnType, internal::scan_step::first>::scan_block(
+        eval_impl, data, accumulator, total_size, scan_size, panel_size, non_scan_size, scan_stride, non_scan_stride,
+        inclusive, dev);
+  }
+};
+}  // namespace Eigen
+
+#endif  // UNSUPPORTED_EIGEN_CXX11_SRC_TENSOR_TENSOR_SCAN_DPCPP_HPP
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorScanSycl.h b/unsupported/Eigen/CXX11/src/Tensor/TensorScanSycl.h
index 7f68ecb6a..01084801d 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorScanSycl.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorScanSycl.h
@@ -81,7 +81,7 @@ enum class scan_step { first, second };
 template <typename Evaluator, typename CoeffReturnType, typename OutAccessor, typename Op, typename Index,
           scan_step stp>
 struct ScanKernelFunctor {
-  typedef cl::sycl::accessor<CoeffReturnType, 1, cl::sycl::access::mode::read_write, cl::sycl::access::target::local>
+  typedef sycl::accessor<CoeffReturnType, 1, sycl::access::mode::read_write, sycl::access::target::local>
       LocalAccessor;
   static EIGEN_CONSTEXPR int PacketSize = ScanParameters<Index>::ScanPerThread / 2;
 
@@ -128,7 +128,7 @@ struct ScanKernelFunctor {
   typename ::Eigen::internal::enable_if<sst != scan_step::first>::type EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
   first_step_inclusive_Operation(InclusiveOp) {}
 
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(cl::sycl::nd_item<1> itemID) {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(sycl::nd_item<1> itemID) {
     auto out_ptr = out_accessor.get_pointer();
     auto tmp_ptr = temp_accessor.get_pointer();
     auto scratch_ptr = scratch.get_pointer().get();
@@ -212,7 +212,7 @@ struct ScanKernelFunctor {
       // build sum in place up the tree
       for (Index d = scratch_stride >> 1; d > 0; d >>= 1) {
         // Synchronise
-        itemID.barrier(cl::sycl::access::fence_space::local_space);
+        itemID.barrier(sycl::access::fence_space::local_space);
         if (local_id < d) {
           Index ai = offset * (2 * local_id + 1) - 1 + scratch_offset;
           Index bi = offset * (2 * local_id + 2) - 1 + scratch_offset;
@@ -224,7 +224,7 @@ struct ScanKernelFunctor {
         offset *= 2;
       }
       // Synchronise
-      itemID.barrier(cl::sycl::access::fence_space::local_space);
+      itemID.barrier(sycl::access::fence_space::local_space);
       // next step optimisation
       if (local_id == 0) {
         if (((scanParameters.elements_per_group / scanParameters.elements_per_block) > 1)) {
@@ -241,7 +241,7 @@ struct ScanKernelFunctor {
       for (Index d = 1; d < scratch_stride; d *= 2) {
         offset >>= 1;
         // Synchronise
-        itemID.barrier(cl::sycl::access::fence_space::local_space);
+        itemID.barrier(sycl::access::fence_space::local_space);
         if (local_id < d) {
           Index ai = offset * (2 * local_id + 1) - 1 + scratch_offset;
           Index bi = offset * (2 * local_id + 2) - 1 + scratch_offset;
@@ -253,7 +253,7 @@ struct ScanKernelFunctor {
         }
       }
       // Synchronise
-      itemID.barrier(cl::sycl::access::fence_space::local_space);
+      itemID.barrier(sycl::access::fence_space::local_space);
       // This for loop must be 2
       EIGEN_UNROLL_LOOP
       for (int packetIndex = 0; packetIndex < ScanParameters<Index>::ScanPerThread; packetIndex += PacketSize) {
@@ -289,7 +289,7 @@ struct ScanKernelFunctor {
 
 template <typename CoeffReturnType, typename InAccessor, typename OutAccessor, typename Op, typename Index>
 struct ScanAdjustmentKernelFunctor {
-  typedef cl::sycl::accessor<CoeffReturnType, 1, cl::sycl::access::mode::read_write, cl::sycl::access::target::local>
+  typedef sycl::accessor<CoeffReturnType, 1, sycl::access::mode::read_write, sycl::access::target::local>
       LocalAccessor;
   static EIGEN_CONSTEXPR int PacketSize = ScanParameters<Index>::ScanPerThread / 2;
   InAccessor in_accessor;
@@ -305,7 +305,7 @@ struct ScanAdjustmentKernelFunctor {
         scanParameters(scanParameters_),
         accumulator(accumulator_) {}
 
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(cl::sycl::nd_item<1> itemID) {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(sycl::nd_item<1> itemID) {
     auto in_ptr = in_accessor.get_pointer();
     auto out_ptr = out_accessor.get_pointer();
 
@@ -405,8 +405,8 @@ struct ScanInfo {
     return ScanParameters<Index>(total_size, non_scan_size, scan_size, non_scan_stride, scan_stride, panel_threads,
                                  group_threads, block_threads, elements_per_group, elements_per_block, loop_range);
   }
-  inline cl::sycl::nd_range<1> get_thread_range() {
-    return cl::sycl::nd_range<1>(cl::sycl::range<1>(global_range), cl::sycl::range<1>(local_range));
+  inline sycl::nd_range<1> get_thread_range() {
+    return sycl::nd_range<1>(sycl::range<1>(global_range), sycl::range<1>(local_range));
   }
 };
 
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h b/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h
index 597ca64cd..60ec3e5bb 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h
@@ -46,6 +46,25 @@ struct nested<TensorShufflingOp<Shuffle, XprType>, 1, typename eval<TensorShuffl
   typedef TensorShufflingOp<Shuffle, XprType> type;
 };
 
+template <typename Self, typename Device, int SUBGROUPSIZE, int VECSIZE>
+struct Transpose2D {
+
+  static void run(const Self &self, Device &dev, typename Self::EvaluatorPointerType data,
+                  int outer_dim_size, int inner_dim_size) {
+    eigen_assert(false && "should never be called");
+                  }
+};
+
+template <typename Self, typename Device, int SUBGROUPSIZE, int VECSIZE>
+struct Transpose3D {
+
+  static void run(const Self &self, Device &dev, typename Self::EvaluatorPointerType data,
+                  int outer_dim_size, int mid_dim_size, int inner_dim_size) {
+    eigen_assert(false && "should never be called");
+                  }
+};
+
+
 }  // end namespace internal
 
 
@@ -178,11 +197,140 @@ struct TensorEvaluator<const TensorShufflingOp<Shuffle, ArgType>, Device>
 
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Dimensions& dimensions() const { return m_dimensions; }
 
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool evalSubExprsIfNeeded(EvaluatorPointerType /*data*/) {
-    m_impl.evalSubExprsIfNeeded(NULL);
+  // TODO(itex): maybe move this part to ITEX?
+  template <typename Type, size_t N>
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void reduceTransposeDimsIfPossible(const Type dims,
+      const array<int, N> shuffle, std::vector<int> *new_dims,  std::vector<int> *new_shuffle) {
+    std::vector<int> new_dim_position(N, -1);
+    std::vector<int> combined_dims(N, 0);
+    int cur_head = shuffle[0];
+    new_dim_position[cur_head] = 0;
+    combined_dims[0] = dims[cur_head];
+    int dim_idx = 0;
+    for (int idx = 1; idx < N; ++idx) {
+      // If two indices in permutation are consecutive numbers, combine their
+      // dimensions.
+      if (cur_head + 1 == shuffle[idx]) {
+        cur_head = shuffle[idx];
+        combined_dims[dim_idx] *= dims[cur_head];
+      } else {
+        // Else start a new dimension.
+        cur_head = shuffle[idx];
+        dim_idx++;
+        new_dim_position[cur_head] = dim_idx;
+        combined_dims[dim_idx] = dims[cur_head];
+      }
+    }
+    // Compact the new permutations and dimension sizes.
+    new_shuffle->resize(dim_idx + 1);
+    new_dims->resize(dim_idx + 1);
+
+    dim_idx = 0;
+    for (int i = 0; i < new_dim_position.size(); ++i) {
+      if (new_dim_position[i] >= 0) {
+        int new_perm_idx = new_dim_position[i];
+        (*new_shuffle)[dim_idx] = new_perm_idx;
+        (*new_dims)[dim_idx] = combined_dims[new_perm_idx];
+        dim_idx++;
+      }
+    }
+  }
+
+#if defined(EIGEN_USE_DPCPP)
+  template <bool IsBuiltInDataType>
+  EIGEN_DEVICE_FUNC
+  bool evalSubExprsIfNeededCommon(EvaluatorPointerType data);
+
+  template <>
+  EIGEN_DEVICE_FUNC
+  bool evalSubExprsIfNeededCommon<true>(EvaluatorPointerType data){
+    static const bool RunningOnGPU = internal::is_same<Device, Eigen::GpuDevice>::value;
+    if (!RunningOnGPU || m_is_identity) return true;
+
+    EIGEN_CONSTEXPR int SUBGROUPSIZE = 16;
+    EIGEN_CONSTEXPR int VECSIZE = 16;
+    const bool is_col_major = static_cast<int>(Layout) == static_cast<int>(ColMajor);
+
+    const typename TensorEvaluator<ArgType, Device>::Dimensions& input_dims = m_impl.dimensions();
+    if (NumDims == 2 && input_dims[0] % SUBGROUPSIZE == 0 && input_dims[1] % VECSIZE == 0){
+      if (is_col_major)
+        internal::Transpose2D<Self, Device, SUBGROUPSIZE, VECSIZE>::run(*this, m_device, data, input_dims[1], input_dims[0]);
+      else
+        internal::Transpose2D<Self, Device, SUBGROUPSIZE, VECSIZE>::run(*this, m_device, data, input_dims[0], input_dims[1]);
+      return false;
+    }
+    if (NumDims == 3){
+      if (is_col_major && m_shuffle[0] == 1 && m_shuffle[1] == 0 && m_shuffle[2] == 2
+                      && input_dims[0] % SUBGROUPSIZE == 0 && input_dims[1] % VECSIZE == 0){
+        internal::Transpose3D<Self, Device, SUBGROUPSIZE, VECSIZE>::run(*this, m_device, data, input_dims[2], input_dims[1], input_dims[0]);
+        return false;
+      }else if (!is_col_major && m_shuffle[0] == 0 && m_shuffle[1] == 2 && m_shuffle[2] == 1
+                      && input_dims[1] % SUBGROUPSIZE == 0 && input_dims[2] % VECSIZE == 0){
+        internal::Transpose3D<Self, Device, SUBGROUPSIZE, VECSIZE>::run(*this, m_device, data, input_dims[0], input_dims[1], input_dims[2]);
+        return false;
+      }
+    }
+
+    std::vector<int> new_dims;
+    std::vector<int> new_shuffle;
+    reduceTransposeDimsIfPossible(input_dims, m_shuffle, &new_dims, &new_shuffle);
+
+    int new_n_dim = new_dims.size();
+    if (new_n_dim < NumDims){
+      if (new_n_dim == 2 && new_dims[0] % SUBGROUPSIZE == 0 && new_dims[1] % VECSIZE == 0){
+        if (is_col_major)
+          internal::Transpose2D<Self, Device, SUBGROUPSIZE, VECSIZE>::run(*this, m_device, data, new_dims[1], new_dims[0]);
+        else
+          internal::Transpose2D<Self, Device, SUBGROUPSIZE, VECSIZE>::run(*this, m_device, data, new_dims[0], new_dims[1]);
+        return false;
+      }
+      else if (new_n_dim == 3){
+        if (is_col_major && new_shuffle[0] == 1 && new_shuffle[1] == 0 && new_shuffle[2] == 2
+                        && new_dims[0] % SUBGROUPSIZE == 0 && new_dims[1] % VECSIZE == 0){
+          internal::Transpose3D<Self, Device, SUBGROUPSIZE, VECSIZE>::run(*this, m_device, data, new_dims[2], new_dims[1], new_dims[0]);
+          return false;
+        }else if (!is_col_major && new_shuffle[0] == 0 && new_shuffle[1] == 2 && new_shuffle[2] == 1
+                        && new_dims[1] % SUBGROUPSIZE == 0 && new_dims[2] % VECSIZE == 0){
+          internal::Transpose3D<Self, Device, SUBGROUPSIZE, VECSIZE>::run(*this, m_device, data, new_dims[0], new_dims[1], new_dims[2]);
+          return false;
+        }
+      }
+      else
+        return true;
+    }
     return true;
   }
 
+  template <>
+  EIGEN_DEVICE_FUNC
+  bool evalSubExprsIfNeededCommon<false>(EvaluatorPointerType data){
+      return true;
+  }
+
+#endif
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool evalSubExprsIfNeeded(EvaluatorPointerType data) {
+    m_impl.evalSubExprsIfNeeded(NULL);
+#if defined(EIGEN_USE_DPCPP)
+    // Only use SubGroup shuffle when shuffle is the last funcor and datatype must be trivially copyable
+    if (data == NULL)
+      return true;
+    else
+      return evalSubExprsIfNeededCommon<
+                  std::is_same<CoeffReturnType, float>::value ||
+                  std::is_same<CoeffReturnType, std::uint32_t>::value ||
+                  std::is_same<CoeffReturnType, std::int32_t>::value ||
+                  std::is_same<CoeffReturnType, std::uint64_t>::value ||
+                  std::is_same<CoeffReturnType, std::int64_t>::value ||
+                  std::is_same<CoeffReturnType, std::int16_t>::value ||
+                  std::is_same<CoeffReturnType, std::uint16_t>::value ||
+                  std::is_same<CoeffReturnType, std::int8_t>::value ||
+                  std::is_same<CoeffReturnType, std::uint8_t>::value>(data);
+#else
+  return true;
+#endif
+  }
+
 #ifdef EIGEN_USE_THREADS
   template <typename EvalSubExprsCallback>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void evalSubExprsIfNeededAsync(
@@ -302,10 +450,11 @@ struct TensorEvaluator<const TensorShufflingOp<Shuffle, ArgType>, Device>
   }
 
   EIGEN_DEVICE_FUNC typename Storage::Type data() const { return NULL; }
+  const TensorEvaluator<ArgType, Device>& impl() const { return m_impl; }
 
 #ifdef EIGEN_USE_SYCL
    // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorShufflingDpcpp.h b/unsupported/Eigen/CXX11/src/Tensor/TensorShufflingDpcpp.h
new file mode 100644
index 000000000..48cadf103
--- /dev/null
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorShufflingDpcpp.h
@@ -0,0 +1,252 @@
+#if defined(EIGEN_USE_DPCPP) &&                                                  \
+    !defined(UNSUPPORTED_EIGEN_CXX11_SRC_TENSOR_TENSOR_SHUFFLING_DPCPP_HPP)
+#define UNSUPPORTED_EIGEN_CXX11_SRC_TENSOR_TENSOR_SHUFFLING_DPCPP_HPP
+namespace Eigen {
+namespace internal {
+
+template <typename Evaluator, int SUBGROUPSIZE, int VECSIZE>
+struct Transpose2DFunctor {
+  typedef typename Evaluator::CoeffReturnType CoeffReturnType;
+  typedef typename Evaluator::Index Index;
+
+  typedef typename Evaluator::EvaluatorPointerType EvaluatorPointerType;
+  typedef sycl::multi_ptr<CoeffReturnType,
+                          sycl::access::address_space::global_space>
+      global_ptr;
+
+  Evaluator evaluator;
+  EvaluatorPointerType final_output;
+  Index outer_dim_size;
+  Index inner_dim_size;
+  Index outer_sub_group;
+  Index inner_sub_group;
+
+  template <typename Scratch>
+  Transpose2DFunctor(Scratch scratch_, Evaluator evaluator_,
+                     EvaluatorPointerType final_output_, const Index outer_dim_size_,
+                     const Index inner_dim_size_, const Index outer_sub_group_,
+                     const Index inner_sub_group_)
+      : evaluator(evaluator_), final_output(final_output_),
+        outer_dim_size(outer_dim_size_), inner_dim_size(inner_dim_size_),
+        outer_sub_group(outer_sub_group_), inner_sub_group(inner_sub_group_) {}
+
+#if defined(DPCPP_DEVICE_ONLY)
+  [[intel::reqd_sub_group_size(SUBGROUPSIZE)]]
+#endif
+  void operator()(sycl::nd_item<1> itemID) const {
+    transpose_impl(itemID);
+  }
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+  void transpose_impl(const sycl::nd_item<1> &itemID) const {
+    auto group_index = itemID.get_group_linear_id();
+    auto group_num_inner = inner_dim_size / VECSIZE / inner_sub_group;
+    auto group_index_outer = group_index / group_num_inner;
+    auto group_index_inner =
+        group_index - (group_index_outer * group_num_inner);
+    auto global_outer = group_index_outer * SUBGROUPSIZE * outer_sub_group;
+    auto global_inner = group_index_inner * VECSIZE * inner_sub_group;
+
+    auto sg = itemID.get_sub_group();
+    auto sg_id = sg.get_group_linear_id();
+    auto sg_id_outer = sg_id / inner_sub_group;
+    auto sg_id_inner = sg_id - sg_id_outer * inner_sub_group;
+    auto local_outer = sg_id_outer * SUBGROUPSIZE;
+    auto local_inner = sg_id_inner * VECSIZE;
+
+    uint32_t lid_in_sg = sg.get_local_linear_id();
+    sycl::vec<CoeffReturnType, VECSIZE> data;
+
+    auto in_offset = (global_outer + local_outer) * inner_dim_size +
+                     global_inner + local_inner;
+    auto out_offset = (global_inner + local_inner) * outer_dim_size +
+                      global_outer + local_outer;
+
+    EIGEN_UNROLL_LOOP
+    for (int i = 0; i < VECSIZE; ++i) {
+      int offset = in_offset + lid_in_sg * inner_dim_size + i;
+      data[i] = evaluator.impl().coeff(offset);
+    }
+
+    EIGEN_UNROLL_LOOP
+    for (int i = 0; i < VECSIZE; ++i) {
+      sg.store(global_ptr(&final_output[out_offset + i * outer_dim_size]),
+               data[i]);
+    }
+  }
+};
+
+template <typename Self, int SUBGROUPSIZE, int VECSIZE>
+struct Transpose2D<Self, Eigen::GpuDevice, SUBGROUPSIZE, VECSIZE> {
+
+  typedef typename Self::CoeffReturnType CoeffReturnType;
+  typedef typename Self::EvaluatorPointerType EvaluatorPointerType;
+
+  static void run(const Self &self, const Eigen::GpuDevice &dev,
+                  EvaluatorPointerType data, const int outer_dim_size,
+                  const int inner_dim_size) {
+
+    int MAXGROUPSIZE =
+        static_cast<int>(dev.getNearestPowerOfTwoWorkGroupSize());
+    int outer_sub_group = 1;
+    int inner_sub_group = 1;
+    if (outer_dim_size >= inner_dim_size) {
+      while (outer_dim_size % ((outer_sub_group << 1) * SUBGROUPSIZE) == 0) {
+        outer_sub_group <<= 1;
+      }
+      outer_sub_group = std::min(outer_sub_group, MAXGROUPSIZE / SUBGROUPSIZE);
+    } else {
+      while (inner_dim_size % ((inner_sub_group << 1) * VECSIZE) == 0) {
+        inner_sub_group <<= 1;
+      }
+      inner_sub_group = std::min(inner_sub_group, MAXGROUPSIZE / SUBGROUPSIZE);
+    }
+
+    sycl::range<1> global{size_t(outer_dim_size * inner_dim_size / VECSIZE)};
+    sycl::range<1> local{
+        size_t(outer_sub_group * SUBGROUPSIZE * inner_sub_group)};
+    auto thread_range = sycl::nd_range<1>(global, local);
+
+    typedef internal::Transpose2DFunctor<Self, SUBGROUPSIZE, VECSIZE>
+        transpose_2d_kernel_t;
+    dev.template unary_kernel_launcher<CoeffReturnType, transpose_2d_kernel_t>(
+        self, data, thread_range, 0, outer_dim_size, inner_dim_size,
+        outer_sub_group, inner_sub_group);
+  }
+};
+
+template <typename Evaluator, int SUBGROUPSIZE, int VECSIZE>
+struct Transpose3DFunctor {
+  typedef typename Evaluator::CoeffReturnType CoeffReturnType;
+  typedef typename Evaluator::Index Index;
+
+  typedef typename Evaluator::EvaluatorPointerType EvaluatorPointerType;
+  typedef sycl::multi_ptr<CoeffReturnType,
+                          sycl::access::address_space::global_space>
+      global_ptr;
+
+  Evaluator evaluator;
+  EvaluatorPointerType final_output;
+  Index outer_dim_size;
+  Index mid_dim_size;
+  Index inner_dim_size;
+  Index outer_sub_group;
+  Index mid_sub_group;
+  Index inner_sub_group;
+
+  template <typename Scratch>
+  Transpose3DFunctor(Scratch scratch_, Evaluator evaluator_,
+                     EvaluatorPointerType final_output_, const Index outer_dim_size_,
+                     const Index mid_dim_size_, const Index inner_dim_size_,
+                     const Index outer_sub_group_, const Index mid_sub_group_,
+                     const Index inner_sub_group_)
+      : evaluator(evaluator_), final_output(final_output_),
+        outer_dim_size(outer_dim_size_), mid_dim_size(mid_dim_size_),
+        inner_dim_size(inner_dim_size_), outer_sub_group(outer_sub_group_),
+        mid_sub_group(mid_sub_group_), inner_sub_group(inner_sub_group_) {}
+
+#if defined(DPCPP_DEVICE_ONLY)
+  [[intel::reqd_sub_group_size(SUBGROUPSIZE)]]
+#endif
+  void operator()(sycl::nd_item<1> itemID) const {
+    transpose_impl(itemID);
+  }
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
+  void transpose_impl(const sycl::nd_item<1> &itemID) const {
+    auto gnum_mid = mid_dim_size / (SUBGROUPSIZE * mid_sub_group);
+    auto gnum_inner = inner_dim_size / VECSIZE / inner_sub_group;
+    auto glid = itemID.get_group_linear_id();
+    auto tmp = glid;
+    auto gid_outer = glid / (gnum_mid * gnum_inner);
+    tmp -= gid_outer * gnum_mid * gnum_inner;
+    auto gid_mid = tmp / gnum_inner;
+    tmp -= gid_mid * gnum_inner;
+    auto gid_inner = tmp;
+    auto global_outer = gid_outer * outer_sub_group;
+    auto global_mid = gid_mid * SUBGROUPSIZE * mid_sub_group;
+    auto global_inner = gid_inner * VECSIZE * inner_sub_group;
+
+    auto sg = itemID.get_sub_group();
+    auto sg_id = sg.get_group_linear_id();
+    auto sg_id_outer = sg_id / (mid_sub_group * inner_sub_group);
+    sg_id -= sg_id_outer * mid_sub_group * inner_sub_group;
+    auto sg_id_mid = sg_id / inner_sub_group;
+    sg_id -= sg_id_mid * inner_sub_group;
+    auto sg_id_inner = sg_id;
+
+    auto local_outer = sg_id_outer;
+    auto local_mid = sg_id_mid * SUBGROUPSIZE;
+    auto local_inner = sg_id_inner * VECSIZE;
+
+    uint32_t lid_in_sg = sg.get_local_linear_id();
+    sycl::vec<CoeffReturnType, VECSIZE> data;
+
+    auto in_offset = (global_outer + local_outer) * mid_dim_size * inner_dim_size +
+                     (global_mid + local_mid) * inner_dim_size + global_inner + local_inner;
+    auto out_offset = (global_outer + local_outer) * mid_dim_size * inner_dim_size +
+                      (global_inner + local_inner) * mid_dim_size + global_mid + local_mid;
+
+    EIGEN_UNROLL_LOOP
+    for (int i = 0; i < VECSIZE; ++i) {
+      int offset = in_offset + lid_in_sg * inner_dim_size + i;
+      data[i] = evaluator.impl().coeff(offset);
+    }
+
+    EIGEN_UNROLL_LOOP
+    for (int i = 0; i < VECSIZE; ++i) {
+      sg.store(global_ptr(&final_output[out_offset + i * mid_dim_size]),
+               data[i]);
+    }
+  }
+};
+
+template <typename Self, int SUBGROUPSIZE, int VECSIZE>
+struct Transpose3D<Self, Eigen::GpuDevice, SUBGROUPSIZE, VECSIZE> {
+
+  typedef typename Self::CoeffReturnType CoeffReturnType;
+  typedef typename Self::EvaluatorPointerType EvaluatorPointerType;
+
+  static void run(const Self &self, const Eigen::GpuDevice &dev,
+                  EvaluatorPointerType data, const int outer_dim_size,
+                  const int mid_dim_size, const int inner_dim_size) {
+
+    int MAXGROUPSIZE =
+        static_cast<int>(dev.getNearestPowerOfTwoWorkGroupSize());
+    int outer_sub_group = 1;
+    int mid_sub_group = 1;
+    int inner_sub_group = 1;
+
+    if (outer_dim_size % 4 == 0)
+      outer_sub_group = 4;
+
+    if (mid_dim_size >= inner_dim_size) {
+      while (mid_dim_size % ((mid_sub_group << 1) * SUBGROUPSIZE) == 0) {
+        mid_sub_group <<= 1;
+      }
+      mid_sub_group = std::min(mid_sub_group, MAXGROUPSIZE / SUBGROUPSIZE / outer_sub_group);
+    } else {
+      while (inner_dim_size % ((inner_sub_group << 1) * VECSIZE) == 0) {
+        inner_sub_group <<= 1;
+      }
+      inner_sub_group = std::min(inner_sub_group, MAXGROUPSIZE / SUBGROUPSIZE / outer_sub_group);
+    }
+
+    sycl::range<1> global{
+        size_t(outer_dim_size * mid_dim_size * inner_dim_size / VECSIZE)};
+    sycl::range<1> local{size_t(outer_sub_group * mid_sub_group * SUBGROUPSIZE *
+                                inner_sub_group)};
+    auto thread_range = sycl::nd_range<1>(global, local);
+
+    typedef internal::Transpose3DFunctor<Self, SUBGROUPSIZE, VECSIZE>
+        transpose_3d_kernel_t;
+    dev.template unary_kernel_launcher<CoeffReturnType, transpose_3d_kernel_t>(
+        self, data, thread_range, 1, outer_dim_size, mid_dim_size,
+        inner_dim_size, outer_sub_group, mid_sub_group, inner_sub_group);
+  }
+};
+
+} // namespace internal
+} // namespace Eigen
+
+#endif // UNSUPPORTED_EIGEN_CXX11_SRC_TENSOR_TENSOR_SHUFFLING_DPCPP_HPP
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorStriding.h b/unsupported/Eigen/CXX11/src/Tensor/TensorStriding.h
index d05f37532..15529de0c 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorStriding.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorStriding.h
@@ -239,7 +239,7 @@ struct TensorEvaluator<const TensorStridingOp<Strides, ArgType>, Device>
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorTrace.h b/unsupported/Eigen/CXX11/src/Tensor/TensorTrace.h
index 24d22c189..887703123 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorTrace.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorTrace.h
@@ -133,7 +133,7 @@ struct TensorEvaluator<const TensorTraceOp<Dims, ArgType>, Device>
         ++num_distinct_reduce_dims;
       }
     }
-
+    (void) num_distinct_reduce_dims;
     eigen_assert(num_distinct_reduce_dims == NumReducedDims);
 
     // Compute the dimensions of the result.
@@ -257,7 +257,7 @@ struct TensorEvaluator<const TensorTraceOp<Dims, ArgType>, Device>
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorTraits.h b/unsupported/Eigen/CXX11/src/Tensor/TensorTraits.h
index 4f7fd340e..1b139c938 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorTraits.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorTraits.h
@@ -87,6 +87,7 @@ struct traits<TensorMap<PlainObjectType, Options_, MakePointer_> >
   : public traits<PlainObjectType>
 {
   typedef traits<PlainObjectType> BaseTraits;
+  typedef BaseTraits XprTraits;
   typedef typename BaseTraits::Scalar Scalar;
   typedef typename BaseTraits::StorageKind StorageKind;
   typedef typename BaseTraits::Index Index;
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorUInt128.h b/unsupported/Eigen/CXX11/src/Tensor/TensorUInt128.h
index d23f2e4c8..afbcba4a2 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorUInt128.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorUInt128.h
@@ -78,14 +78,14 @@ template <typename HL, typename LL, typename HR, typename LR>
 EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
 bool operator == (const TensorUInt128<HL, LL>& lhs, const TensorUInt128<HR, LR>& rhs)
 {
-  return (lhs.high == rhs.high) & (lhs.low == rhs.low);
+  return (lhs.high == rhs.high) && (lhs.low == rhs.low);
 }
 
 template <typename HL, typename LL, typename HR, typename LR>
 EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
 bool operator != (const TensorUInt128<HL, LL>& lhs, const TensorUInt128<HR, LR>& rhs)
 {
-  return (lhs.high != rhs.high) | (lhs.low != rhs.low);
+  return (lhs.high != rhs.high) || (lhs.low != rhs.low);
 }
 
 template <typename HL, typename LL, typename HR, typename LR>
diff --git a/unsupported/Eigen/CXX11/src/Tensor/TensorVolumePatch.h b/unsupported/Eigen/CXX11/src/Tensor/TensorVolumePatch.h
index 000ed5b41..d3a7d1231 100644
--- a/unsupported/Eigen/CXX11/src/Tensor/TensorVolumePatch.h
+++ b/unsupported/Eigen/CXX11/src/Tensor/TensorVolumePatch.h
@@ -258,12 +258,12 @@ struct TensorEvaluator<const TensorVolumePatchOp<Planes, Rows, Cols, ArgType>, D
           m_outputPlanes = numext::ceil(m_input_planes_eff / static_cast<float>(m_plane_strides));
           m_outputRows = numext::ceil(m_input_rows_eff / static_cast<float>(m_row_strides));
           m_outputCols = numext::ceil(m_input_cols_eff / static_cast<float>(m_col_strides));
-          const Index dz = m_outputPlanes * m_plane_strides + m_patch_planes_eff - 1 - m_input_planes_eff;
-          const Index dy = m_outputRows * m_row_strides + m_patch_rows_eff - 1 - m_input_rows_eff;
-          const Index dx = m_outputCols * m_col_strides + m_patch_cols_eff - 1 - m_input_cols_eff;
-          m_planePaddingTop = dz - dz / 2;
-          m_rowPaddingTop = dy - dy / 2;
-          m_colPaddingLeft = dx - dx / 2;
+          const Index dz = (m_outputPlanes - 1) * m_plane_strides + m_patch_planes_eff - m_input_planes_eff;
+          const Index dy = (m_outputRows - 1) * m_row_strides + m_patch_rows_eff - m_input_rows_eff;
+          const Index dx = (m_outputCols - 1) * m_col_strides + m_patch_cols_eff - m_input_cols_eff;
+          m_planePaddingTop = dz / 2;
+          m_rowPaddingTop = dy / 2;
+          m_colPaddingLeft = dx / 2;
           break;
         }
         default:
@@ -536,7 +536,7 @@ struct TensorEvaluator<const TensorVolumePatchOp<Planes, Rows, Cols, ArgType>, D
 
 #ifdef EIGEN_USE_SYCL
   // binding placeholder accessors to a command group handler for SYCL
-  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(cl::sycl::handler &cgh) const {
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void bind(sycl::handler &cgh) const {
     m_impl.bind(cgh);
   }
 #endif
diff --git a/unsupported/Eigen/src/SpecialFunctions/SpecialFunctionsFunctors.h b/unsupported/Eigen/src/SpecialFunctions/SpecialFunctionsFunctors.h
index abefe99b7..e037e32ab 100644
--- a/unsupported/Eigen/src/SpecialFunctions/SpecialFunctionsFunctors.h
+++ b/unsupported/Eigen/src/SpecialFunctions/SpecialFunctionsFunctors.h
@@ -158,7 +158,7 @@ template<typename Scalar> struct scalar_lgamma_op {
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator() (const Scalar& a) const {
     using numext::lgamma; return lgamma(a);
   }
-  typedef typename packet_traits<Scalar>::type Packet;
+  template<typename Packet>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a) const { return internal::plgamma(a); }
 };
 template<typename Scalar>
@@ -224,7 +224,7 @@ template<typename Scalar> struct scalar_polygamma_op {
     EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator() (const Scalar& n, const Scalar& x) const {
         using numext::polygamma; return polygamma(n, x);
     }
-    typedef typename packet_traits<Scalar>::type Packet;
+    template <typename Packet> 
     EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& n, const Packet& x) const { return internal::ppolygamma(n, x); }
 };
 template<typename Scalar>
@@ -285,7 +285,7 @@ template<typename Scalar> struct scalar_erfc_op {
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator() (const Scalar& a) const {
     using numext::erfc; return erfc(a);
   }
-  typedef typename packet_traits<Scalar>::type Packet;
+  template <typename Packet>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a) const { return internal::perfc(a); }
 };
 template<typename Scalar>
@@ -308,7 +308,7 @@ template<typename Scalar> struct scalar_ndtri_op {
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator() (const Scalar& a) const {
     using numext::ndtri; return ndtri(a);
   }
-  typedef typename packet_traits<Scalar>::type Packet;
+  template <typename Packet>
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a) const { return internal::pndtri(a); }
 };
 template<typename Scalar>
diff --git a/unsupported/Eigen/src/SpecialFunctions/SpecialFunctionsImpl.h b/unsupported/Eigen/src/SpecialFunctions/SpecialFunctionsImpl.h
index 0044b8a27..d5e781832 100644
--- a/unsupported/Eigen/src/SpecialFunctions/SpecialFunctionsImpl.h
+++ b/unsupported/Eigen/src/SpecialFunctions/SpecialFunctionsImpl.h
@@ -61,11 +61,11 @@ template <>
 struct lgamma_impl<float> {
   EIGEN_DEVICE_FUNC
   static EIGEN_STRONG_INLINE float run(float x) {
-#if !defined(EIGEN_GPU_COMPILE_PHASE) && (defined(_BSD_SOURCE) || defined(_SVID_SOURCE)) && !defined(__APPLE__)
+#if defined(DPCPP_DEVICE_ONLY) || defined(EIGEN_USE_DPCPP)
+    return sycl::lgamma(x);
+#elif !defined(EIGEN_GPU_COMPILE_PHASE) && (defined(_BSD_SOURCE) || defined(_SVID_SOURCE)) && !defined(__APPLE__)
     int dummy;
     return ::lgammaf_r(x, &dummy);
-#elif defined(SYCL_DEVICE_ONLY)
-    return cl::sycl::lgamma(x);
 #else
     return ::lgammaf(x);
 #endif
@@ -76,11 +76,11 @@ template <>
 struct lgamma_impl<double> {
   EIGEN_DEVICE_FUNC
   static EIGEN_STRONG_INLINE double run(double x) {
-#if !defined(EIGEN_GPU_COMPILE_PHASE) && (defined(_BSD_SOURCE) || defined(_SVID_SOURCE)) && !defined(__APPLE__)
+#if defined(DPCPP_DEVICE_ONLY) || defined(EIGEN_USE_DPCPP)
+    return sycl::lgamma(x);
+#elif !defined(EIGEN_GPU_COMPILE_PHASE) && (defined(_BSD_SOURCE) || defined(_SVID_SOURCE)) && !defined(__APPLE__)
     int dummy;
     return ::lgamma_r(x, &dummy);
-#elif defined(SYCL_DEVICE_ONLY)
-    return cl::sycl::lgamma(x);
 #else
     return ::lgamma(x);
 #endif
@@ -349,8 +349,8 @@ template <>
 struct erf_impl<float> {
   EIGEN_DEVICE_FUNC
   static EIGEN_STRONG_INLINE float run(float x) {
-#if defined(SYCL_DEVICE_ONLY)
-    return cl::sycl::erf(x);
+#if defined(DPCPP_DEVICE_ONLY)
+    return sycl::erf(x);
 #else
     return generic_fast_erf_float(x);
 #endif
@@ -361,8 +361,8 @@ template <>
 struct erf_impl<double> {
   EIGEN_DEVICE_FUNC
   static EIGEN_STRONG_INLINE double run(double x) {
-#if defined(SYCL_DEVICE_ONLY)
-    return cl::sycl::erf(x);
+#if defined(DPCPP_DEVICE_ONLY)
+    return sycl::erf(x);
 #else
     return ::erf(x);
 #endif
@@ -394,8 +394,8 @@ template <>
 struct erfc_impl<float> {
   EIGEN_DEVICE_FUNC
   static EIGEN_STRONG_INLINE float run(const float x) {
-#if defined(SYCL_DEVICE_ONLY)
-    return cl::sycl::erfc(x);
+#if defined(DPCPP_DEVICE_ONLY)
+    return sycl::erfc(x);
 #else
     return ::erfcf(x);
 #endif
@@ -406,8 +406,8 @@ template <>
 struct erfc_impl<double> {
   EIGEN_DEVICE_FUNC
   static EIGEN_STRONG_INLINE double run(const double x) {
-#if defined(SYCL_DEVICE_ONLY)
-    return cl::sycl::erfc(x);
+#if defined(DPCPP_DEVICE_ONLY)
+    return sycl::erfc(x);
 #else
     return ::erfc(x);
 #endif
diff --git a/unsupported/test/cxx11_tensor_broadcast_sycl.cpp b/unsupported/test/cxx11_tensor_broadcast_sycl.cpp
index 20f84b8e0..9d315350a 100644
--- a/unsupported/test/cxx11_tensor_broadcast_sycl.cpp
+++ b/unsupported/test/cxx11_tensor_broadcast_sycl.cpp
@@ -127,8 +127,8 @@ static void test_broadcast_sycl(const Eigen::SyclDevice &sycl_device){
   sycl_device.deallocate(gpu_out_data);
 }
 
-template<typename DataType> void sycl_broadcast_test_per_device(const cl::sycl::device& d){
-  std::cout << "Running on " << d.template get_info<cl::sycl::info::device::name>() << std::endl;
+template<typename DataType> void sycl_broadcast_test_per_device(const sycl::device& d){
+  std::cout << "Running on " << d.template get_info<sycl::info::device::name>() << std::endl;
   QueueInterface queueInterface(d);
   auto sycl_device = Eigen::SyclDevice(&queueInterface);
   test_broadcast_sycl<DataType, RowMajor, int64_t>(sycl_device);
diff --git a/unsupported/test/cxx11_tensor_builtins_sycl.cpp b/unsupported/test/cxx11_tensor_builtins_sycl.cpp
index 72cb62fd5..81f2546f6 100644
--- a/unsupported/test/cxx11_tensor_builtins_sycl.cpp
+++ b/unsupported/test/cxx11_tensor_builtins_sycl.cpp
@@ -29,12 +29,12 @@ using Eigen::TensorMap;
 // the equivalent on the host
 namespace cl {
 namespace sycl {
-template <typename T> T abs(T x) { return cl::sycl::fabs(x); }
+template <typename T> T abs(T x) { return sycl::fabs(x); }
 template <typename T> T square(T x) { return x * x; }
 template <typename T> T cube(T x) { return x * x * x; }
 template <typename T> T inverse(T x) { return T(1) / x; }
-template <typename T> T cwiseMax(T x, T y) { return cl::sycl::max(x, y); }
-template <typename T> T cwiseMin(T x, T y) { return cl::sycl::min(x, y); }
+template <typename T> T cwiseMax(T x, T y) { return sycl::max(x, y); }
+template <typename T> T cwiseMin(T x, T y) { return sycl::min(x, y); }
 }
 }
 
@@ -109,8 +109,8 @@ void test_unary_builtins_for_scalar(const Eigen::SyclDevice& sycl_device,
 #define DECLARE_UNARY_STRUCT(FUNC)                                 \
   struct op_##FUNC {                                               \
     template <typename T>                                          \
-    auto operator()(const T& x) -> decltype(cl::sycl::FUNC(x)) {   \
-      return cl::sycl::FUNC(x);                                    \
+    auto operator()(const T& x) -> decltype(sycl::FUNC(x)) {   \
+      return sycl::FUNC(x);                                    \
     }                                                              \
     template <typename T>                                          \
     auto operator()(const TensorMap<T>& x) -> decltype(x.FUNC()) { \
@@ -278,8 +278,8 @@ void test_binary_builtins_fixed_arg2(const Eigen::SyclDevice& sycl_device,
 #define DECLARE_BINARY_STRUCT(FUNC)                                                          \
   struct op_##FUNC {                                                                         \
     template <typename T1, typename T2>                                                      \
-    auto operator()(const T1& x, const T2& y) -> decltype(cl::sycl::FUNC(x, y)) {            \
-      return cl::sycl::FUNC(x, y);                                                           \
+    auto operator()(const T1& x, const T2& y) -> decltype(sycl::FUNC(x, y)) {            \
+      return sycl::FUNC(x, y);                                                           \
     }                                                                                        \
     template <typename T1, typename T2>                                                      \
     auto operator()(const TensorMap<T1>& x, const TensorMap<T2>& y) -> decltype(x.FUNC(y)) { \
diff --git a/unsupported/test/cxx11_tensor_contract_sycl.cpp b/unsupported/test/cxx11_tensor_contract_sycl.cpp
index fbcc29358..89fef0efb 100644
--- a/unsupported/test/cxx11_tensor_contract_sycl.cpp
+++ b/unsupported/test/cxx11_tensor_contract_sycl.cpp
@@ -1002,7 +1002,7 @@ void inline tensor_contraction_both_transposed_per_device(
 EIGEN_DECLARE_TEST(cxx11_tensor_contract_sycl) {
   for (const auto &device : Eigen::get_sycl_supported_devices()) {
     std::cout << "Running on "
-              << device.template get_info<cl::sycl::info::device::name>()
+              << device.template get_info<sycl::info::device::name>()
               << std::endl;
     QueueInterface queueInterface(device);
     auto sycl_device = Eigen::SyclDevice(&queueInterface);
diff --git a/unsupported/test/cxx11_tensor_device_sycl.cpp b/unsupported/test/cxx11_tensor_device_sycl.cpp
index 5095cb078..b16c2cd1d 100644
--- a/unsupported/test/cxx11_tensor_device_sycl.cpp
+++ b/unsupported/test/cxx11_tensor_device_sycl.cpp
@@ -25,7 +25,7 @@
 template <typename DataType, int DataLayout, typename IndexType>
 void test_device_memory(const Eigen::SyclDevice &sycl_device) {
   std::cout << "Running on : "
-            << sycl_device.sycl_queue().get_device(). template get_info<cl::sycl::info::device::name>()
+            << sycl_device.sycl_queue().get_device(). template get_info<sycl::info::device::name>()
             <<std::endl;
   IndexType sizeDim1 = 100;
   array<IndexType, 1> tensorRange = {{sizeDim1}};
@@ -58,8 +58,8 @@ void test_device_exceptions(const Eigen::SyclDevice &sycl_device) {
   sycl_device.deallocate(gpu_data);
 }
 
-template<typename DataType> void sycl_device_test_per_device(const cl::sycl::device& d){
-  std::cout << "Running on " << d.template get_info<cl::sycl::info::device::name>() << std::endl;
+template<typename DataType> void sycl_device_test_per_device(const sycl::device& d){
+  std::cout << "Running on " << d.template get_info<sycl::info::device::name>() << std::endl;
   QueueInterface queueInterface(d);
   auto sycl_device = Eigen::SyclDevice(&queueInterface);
   test_device_memory<DataType, RowMajor, int64_t>(sycl_device);
diff --git a/unsupported/test/cxx11_tensor_reduction_sycl.cpp b/unsupported/test/cxx11_tensor_reduction_sycl.cpp
index a297716e4..39eca07c1 100644
--- a/unsupported/test/cxx11_tensor_reduction_sycl.cpp
+++ b/unsupported/test/cxx11_tensor_reduction_sycl.cpp
@@ -1001,7 +1001,7 @@ void sycl_reduction_test_last_dim_per_device(const Dev& sycl_device) {
 EIGEN_DECLARE_TEST(cxx11_tensor_reduction_sycl) {
   for (const auto& device : Eigen::get_sycl_supported_devices()) {
     std::cout << "Running on "
-              << device.template get_info<cl::sycl::info::device::name>()
+              << device.template get_info<sycl::info::device::name>()
               << std::endl;
     QueueInterface queueInterface(device);
     auto sycl_device = Eigen::SyclDevice(&queueInterface);
diff --git a/unsupported/test/cxx11_tensor_reverse_sycl.cpp b/unsupported/test/cxx11_tensor_reverse_sycl.cpp
index dd30c235d..e111a7fab 100644
--- a/unsupported/test/cxx11_tensor_reverse_sycl.cpp
+++ b/unsupported/test/cxx11_tensor_reverse_sycl.cpp
@@ -228,7 +228,7 @@ static void test_expr_reverse(const Eigen::SyclDevice& sycl_device,
 }
 
 template <typename DataType>
-void sycl_reverse_test_per_device(const cl::sycl::device& d) {
+void sycl_reverse_test_per_device(const sycl::device& d) {
   QueueInterface queueInterface(d);
   auto sycl_device = Eigen::SyclDevice(&queueInterface);
   test_simple_reverse<DataType, RowMajor, int64_t>(sycl_device);
@@ -241,7 +241,7 @@ void sycl_reverse_test_per_device(const cl::sycl::device& d) {
 EIGEN_DECLARE_TEST(cxx11_tensor_reverse_sycl) {
   for (const auto& device : Eigen::get_sycl_supported_devices()) {
     std::cout << "Running on "
-              << device.get_info<cl::sycl::info::device::name>() << std::endl;
+              << device.get_info<sycl::info::device::name>() << std::endl;
     CALL_SUBTEST_1(sycl_reverse_test_per_device<short>(device));
     CALL_SUBTEST_2(sycl_reverse_test_per_device<int>(device));
     CALL_SUBTEST_3(sycl_reverse_test_per_device<unsigned int>(device));
diff --git a/unsupported/test/cxx11_tensor_scan_sycl.cpp b/unsupported/test/cxx11_tensor_scan_sycl.cpp
index 09c45fce5..a50cba8a1 100644
--- a/unsupported/test/cxx11_tensor_scan_sycl.cpp
+++ b/unsupported/test/cxx11_tensor_scan_sycl.cpp
@@ -121,7 +121,7 @@ void sycl_scan_test_inclusive_dim2_per_device(const Dev& sycl_device) {
 EIGEN_DECLARE_TEST(cxx11_tensor_scan_sycl) {
   for (const auto& device : Eigen::get_sycl_supported_devices()) {
     std::cout << "Running on "
-              << device.template get_info<cl::sycl::info::device::name>()
+              << device.template get_info<sycl::info::device::name>()
               << std::endl;
     QueueInterface queueInterface(device);
     auto sycl_device = Eigen::SyclDevice(&queueInterface);
diff --git a/unsupported/test/cxx11_tensor_volume_patch_sycl.cpp b/unsupported/test/cxx11_tensor_volume_patch_sycl.cpp
index ca7994fd9..11dad95ec 100644
--- a/unsupported/test/cxx11_tensor_volume_patch_sycl.cpp
+++ b/unsupported/test/cxx11_tensor_volume_patch_sycl.cpp
@@ -210,7 +210,7 @@ static void test_entire_volume_patch_sycl(const Eigen::SyclDevice& sycl_device)
 template<typename DataType, typename dev_Selector> void sycl_tensor_volume_patch_test_per_device(dev_Selector s){
 QueueInterface queueInterface(s);
 auto sycl_device = Eigen::SyclDevice(&queueInterface);
-std::cout << "Running on " << s.template get_info<cl::sycl::info::device::name>() << std::endl;
+std::cout << "Running on " << s.template get_info<sycl::info::device::name>() << std::endl;
 test_single_voxel_patch_sycl<DataType, int64_t>(sycl_device);
 test_entire_volume_patch_sycl<DataType, int64_t>(sycl_device);
 }
