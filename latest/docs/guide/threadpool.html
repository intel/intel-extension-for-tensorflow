<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Selecting Thread Pool in Intel® Extension for TensorFlow* CPU [Experimental] &mdash; Intel® Extension for TensorFlow* 0.1.dev1+g9974032 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-tensorflow"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel® Extension for TensorFlow*
          </a>
            <div class="version">
              <a href="../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../get_started.html">Quick Get Started*</a></li>
<li class="toctree-l1"><a class="reference internal" href="infrastructure.html">Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/installation_guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/README.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="practice_guide.html">Practice Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/contributing.html">Contributing guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-tensorflow">GitHub Repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel® Extension for TensorFlow*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Selecting Thread Pool in Intel® Extension for TensorFlow* CPU [Experimental]</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/docs/guide/threadpool.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="selecting-thread-pool-in-intel-extension-for-tensorflow-cpu-experimental">
<h1>Selecting Thread Pool in Intel® Extension for TensorFlow* CPU [Experimental]<a class="headerlink" href="#selecting-thread-pool-in-intel-extension-for-tensorflow-cpu-experimental" title="Permalink to this heading"></a></h1>
<p>Intel® Extension for TensorFlow* CPU lets you choose between the OpenMP thread pool (default) or Eigen thread pool through environment variable <code class="docutils literal notranslate"><span class="pre">ITEX_OMP_THREADPOOL=1</span></code> or <code class="docutils literal notranslate"><span class="pre">0</span></code> respectively. This gives you flexibility to select a more efficient thread pool for your workload and hardware configuration. If setting <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/config/threading/set_inter_op_parallelism_threads"><code class="docutils literal notranslate"><span class="pre">inter_op_parallelism_threads=1</span></code></a> causes a large performance drop for your workload, we recommended you use Eigen thread pool. Running independent operations concurrently can be more efficient for cheap ops which cannot fully utilize the hardware compute units on their own.</p>
<section id="using-openmp-thread-pool">
<h2>Using OpenMP Thread Pool<a class="headerlink" href="#using-openmp-thread-pool" title="Permalink to this heading"></a></h2>
<p><a class="reference external" href="practice_guide.html#OpenMP">OpenMP</a> thread pool is default in Intel® Extension for TensorFlow* CPU. It provides lower scheduling overheads, better data locality, and better cache usage. Configure the number of OMP threads via the <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> environment variable. Due to the fork-join model of OpenMP and TensorFlow parallelism between independent operations, you must set the correct configuration to avoid thread conflicts. Make sure the total number of OMP threads forked from <code class="docutils literal notranslate"><span class="pre">inter_op_parallelism_threads</span></code> is less than the number of available CPU cores. For example, by default, Intel® Extension for TensorFlow* sets the number of threads used by independent non-blocking operations to be <code class="docutils literal notranslate"><span class="pre">1</span></code>. Set <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> to be the number of cores available, and <code class="docutils literal notranslate"><span class="pre">KMP_BLOCKTIME=1</span></code>, <code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY=granularity=fine,compact,1,0</span></code>.</p>
</section>
<section id="using-eigen-thread-pool">
<h2>Using Eigen Thread Pool<a class="headerlink" href="#using-eigen-thread-pool" title="Permalink to this heading"></a></h2>
<p>For workloads with large inter-op concurrency, an OpenMP thread pool may not supply sufficient parallelism between operations. In this case, you should switch to the non-blocking thread pool provided by Eigen, which is the default in TensorFlow. In this case, same as TensorFlow, <code class="docutils literal notranslate"><span class="pre">inter_op_parallelism_threads</span></code> is set to 0 by default, which means to parallelize independent operations as much as possible. The work-stealing queue in Eigen thread pool allows better dynamic load balancing, giving better performance and scaling with larger <code class="docutils literal notranslate"><span class="pre">inter_op_parallelism_threads</span></code>. No other configuration is needed when using Eigen thread pool.</p>
</section>
<section id="example">
<h2>Example<a class="headerlink" href="#example" title="Permalink to this heading"></a></h2>
<p>Here we show two examples using different thread pools on Intel® Xeon® Platinum 8480+ systems.</p>
<ol>
<li><p>This example is modified from <a class="reference external" href="https://github.com/keras-team/keras-io/blob/master/examples/keras_recipes/antirectifier.py">a keras example</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">## The Antirectifier layer</span>
<span class="sd">&quot;&quot;&quot;</span>


<span class="k">class</span> <span class="nc">Antirectifier</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initializer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="n">output_dim</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">output_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">initializer</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;kernel&quot;</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">-=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">neg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="o">-</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">concatenated</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">pos</span><span class="p">,</span> <span class="n">neg</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mixed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">concatenated</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mixed</span>

    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Implement get_config to enable serialization. This is optional.</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;initializer&quot;</span><span class="p">:</span> <span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">initializer</span><span class="p">)}</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">base_config</span><span class="o">.</span><span class="n">items</span><span class="p">())</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">()))</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">## Let&#39;s test-drive it on MNIST</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Training parameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span>

<span class="c1"># The data, split between train and test sets</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">/=</span> <span class="mi">255</span>
<span class="n">x_test</span> <span class="o">/=</span> <span class="mi">255</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;train samples&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;test samples&quot;</span><span class="p">)</span>

<span class="c1"># Build the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,)),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
        <span class="n">Antirectifier</span><span class="p">(),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
        <span class="n">Antirectifier</span><span class="p">(),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Compile the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(),</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">()],</span>
<span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="c1"># warmup</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)</span>
<span class="c1"># Train the model</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;train time(s): &quot;</span><span class="p">,</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>
<span class="c1"># Test the model</span>
<span class="c1"># warmup</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;evaluation time(s): &quot;</span><span class="p">,</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
<p>For Eigen thread pool, run with <code class="docutils literal notranslate"><span class="pre">ITEX_OMP_THREADPOOL=0</span> <span class="pre">numactl</span> <span class="pre">-C</span> <span class="pre">0-55</span> <span class="pre">-l</span> <span class="pre">python</span> <span class="pre">antirectifier.py</span></code>, This run takes about 60 seconds to train and 0.26 seconds to evaluate.
For OpenMP thread pool, run with <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS=56</span> <span class="pre">KMP_BLOCKTIME=1</span> <span class="pre">KMP_AFFINITY=granularity=fine,verbose,compact,1,0</span>&#160; <span class="pre">numactl</span> <span class="pre">-C</span> <span class="pre">0-55</span> <span class="pre">-l</span> <span class="pre">python</span> <span class="pre">antirectifier.py</span></code>, This run takes about 85 seconds to train and 0.36 seconds to evaluate. Eigen thread pool is about 30% faster on this small model.</p>
</li>
<li><p>Benchmark the inception_v4 example.
Get the trained model via
<code class="docutils literal notranslate"><span class="pre">wget</span> <span class="pre">https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/inceptionv4_fp32_pretrained_model.pb</span></code> before running the following code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework.graph_pb2</span> <span class="kn">import</span> <span class="n">GraphDef</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="k">def</span> <span class="nf">load_pb</span><span class="p">(</span><span class="n">pb_file</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">pb_file</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">gd</span> <span class="o">=</span> <span class="n">GraphDef</span><span class="p">()</span>
        <span class="n">gd</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">gd</span>

<span class="k">def</span> <span class="nf">get_concrete_function</span><span class="p">(</span><span class="n">graph_def</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">print_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">imports_graph_def</span><span class="p">():</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">import_graph_def</span><span class="p">(</span><span class="n">graph_def</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

    <span class="n">wrap_function</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">wrap_function</span><span class="p">(</span><span class="n">imports_graph_def</span><span class="p">,</span> <span class="p">[])</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">wrap_function</span><span class="o">.</span><span class="n">graph</span>

    <span class="k">return</span> <span class="n">wrap_function</span><span class="o">.</span><span class="n">prune</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">,</span> <span class="n">inputs</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">,</span> <span class="n">outputs</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">save_json_data</span><span class="p">(</span><span class="n">logs</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;train.json&quot;</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span><span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">logs</span><span class="p">,</span><span class="n">f</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">run_infer</span><span class="p">(</span><span class="n">concrete_function</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="n">total_times</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">warm</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.2</span><span class="o">*</span><span class="n">total_times</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_times</span><span class="p">):</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">minval</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">bt</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>        
        <span class="n">y</span><span class="o">=</span><span class="n">concrete_function</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">input_x</span><span class="p">)</span>
        <span class="n">delta_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">bt</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration </span><span class="si">%d</span><span class="s1">: </span><span class="si">%.3f</span><span class="s1"> sec&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">delta_time</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">warm</span><span class="p">:</span>
            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">delta_time</span><span class="p">)</span>
    <span class="n">latency</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">latency</span>

<span class="k">def</span> <span class="nf">do_benchmark</span><span class="p">(</span><span class="n">pb_file</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">base_shape</span><span class="p">):</span>

    <span class="n">concrete_function</span> <span class="o">=</span> <span class="n">get_concrete_function</span><span class="p">(</span>
        <span class="n">graph_def</span><span class="o">=</span><span class="n">load_pb</span><span class="p">(</span><span class="n">pb_file</span><span class="p">),</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span>
        <span class="n">print_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bs</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">base_shape</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">base_shape</span><span class="p">)</span>
    <span class="n">latency</span> <span class="o">=</span> <span class="n">run_infer</span><span class="p">(</span><span class="n">concrete_function</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;latency&#39;</span><span class="p">:</span><span class="n">latency</span><span class="p">}</span>


    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Benchmark is done!&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Benchmark res </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Finished&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="k">def</span> <span class="nf">benchmark</span><span class="p">():</span>
    <span class="n">pb_file</span> <span class="o">=</span> <span class="s2">&quot;inceptionv4_fp32_pretrained_model.pb&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;input:0&#39;</span><span class="p">]</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;InceptionV4/Logits/Predictions:0&#39;</span><span class="p">]</span>
    <span class="n">base_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">299</span><span class="p">,</span> <span class="mi">299</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">do_benchmark</span><span class="p">(</span><span class="n">pb_file</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">base_shape</span><span class="p">)</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">benchmark</span><span class="p">()</span>
</pre></div>
</div>
<p>For Eigen thread pool, run with <code class="docutils literal notranslate"><span class="pre">ITEX_OMP_THREADPOOL=0</span> <span class="pre">numactl</span> <span class="pre">-C</span> <span class="pre">0-3</span> <span class="pre">-l</span> <span class="pre">python</span> <span class="pre">benchmark.py</span></code>. The latency is about 0.08 second/image.
For OpenMP thread pool, run with <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS=4</span> <span class="pre">KMP_BLOCKTIME=1</span> <span class="pre">KMP_AFFINITY=granularity=fine,verbose,compact,1,0</span>&#160; <span class="pre">numactl</span> <span class="pre">-C</span> <span class="pre">0-3</span> <span class="pre">-l</span> <span class="pre">python</span> <span class="pre">benchmark.py</span></code>. The latency is 0.04 second/image. OMP thread pool is about 2x slower than Eigen thread pool on this model.</p>
</li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022-2024 Intel Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f4d6071d1b0> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>