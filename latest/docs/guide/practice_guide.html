<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Practice Guide &mdash; Intel® Extension for TensorFlow* v1.0.0 documentation</title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Frequently Asked Questions" href="FAQ.html" />
    <link rel="prev" title="Examples" href="../../examples/examples.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Intel® Extension for TensorFlow*
          </a>
              <div class="version">
                <a href="../../../versions.html">latest ▼</a>
                <p>Click link above to switch version</p>              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../get_started.html">Quick Get Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="infrastructure.html">Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/installation_guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/examples.html">Examples</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Practice Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#table-of-contents">Table of Contents</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cpu-practice-guide">CPU Practice Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hardware-configuration">Hardware Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#non-uniform-memory-access-numa">Non-Uniform Memory Access (NUMA)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#software-configuration">Software Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#memory-layout-format">Memory Layout format</a></li>
<li class="toctree-l4"><a class="reference internal" href="#numactl">Numactl</a></li>
<li class="toctree-l4"><a class="reference internal" href="#openmp">OpenMP</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#omp-num-threads">OMP_NUM_THREADS</a></li>
<li class="toctree-l5"><a class="reference internal" href="#gnu-openmp">GNU OpenMP</a></li>
<li class="toctree-l5"><a class="reference internal" href="#intel-openmp">Intel OpenMP</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#memory-allocator">Memory Allocator</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#tcmalloc">TCMalloc</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gpu-practice-guide">GPU Practice Guide</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/contributing.html">Contributing guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-tensorflow">GitHub Repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel® Extension for TensorFlow*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Practice Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/docs/guide/practice_guide.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="practice-guide">
<h1>Practice Guide<a class="headerlink" href="#practice-guide" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Intel® Extension for TensorFlow* is a Python package to extend official TensorFlow, achieve higher performance. Although official TensorFlow and the default configuration of Intel® Extension for TensorFlow* perform well, there are still something that users can do for performance optimization on specific platforms. Most optimized configurations can be automatically set by the launcher script. This article introduces common tips that Intel developers recommend to take.</p>
</div>
<div class="section" id="table-of-contents">
<h2>Table of Contents<a class="headerlink" href="#table-of-contents" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="#cpu-practice-guide">Practice Guide</a></p>
<ul>
<li><p><a class="reference external" href="#overview">Overview</a></p></li>
<li><p><a class="reference external" href="#table-of-contents">Table of Contents</a></p></li>
<li><p><a class="reference external" href="#cpu-practice-guide">CPU Practice Guide</a></p>
<ul>
<li><p><a class="reference external" href="#hardware-configuration">Hardware Configuration</a></p>
<ul>
<li><p><a class="reference external" href="#non-uniform-memory-access-numa">Non-Uniform Memory Access (NUMA)</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#software-configuration">Software Configuration</a></p>
<ul>
<li><p><a class="reference external" href="#memory-layout-format">Memory Layout format</a></p></li>
<li><p><a class="reference external" href="#numactl">Numactl</a></p></li>
<li><p><a class="reference external" href="#openmp">OpenMP</a></p>
<ul>
<li><p><a class="reference external" href="#omp_num_threads">OMP_NUM_THREADS</a></p></li>
<li><p><a class="reference external" href="#gnu-openmp">GNU OpenMP</a></p></li>
<li><p><a class="reference external" href="#intel-openmp">Intel OpenMP</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#memory-allocator">Memory Allocator</a></p>
<ul>
<li><p><a class="reference external" href="#tcmalloc">TCMalloc</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="#gpu-practice-guide">GPU Practice Guide</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="cpu-practice-guide">
<h2>CPU Practice Guide<a class="headerlink" href="#cpu-practice-guide" title="Permalink to this headline">¶</a></h2>
<div class="section" id="hardware-configuration">
<h3>Hardware Configuration<a class="headerlink" href="#hardware-configuration" title="Permalink to this headline">¶</a></h3>
<p>This section briefly instroduces structure of Intel CPUs, as well as concept of Non-Uniform Memory Access (NUMA), as background knowledge.</p>
<div class="section" id="non-uniform-memory-access-numa">
<h4>Non-Uniform Memory Access (NUMA)<a class="headerlink" href="#non-uniform-memory-access-numa" title="Permalink to this headline">¶</a></h4>
<p>It is a good thing that more and more CPU cores are provided to users in one socket, because this brings more computation resources. However, this also brings memory access competitions. Program may be stalled as memory is busy. To address this problem, <code class="docutils literal notranslate"><span class="pre">Non-Uniform</span> <span class="pre">Memory</span> <span class="pre">Access</span></code> (<code class="docutils literal notranslate"><span class="pre">NUMA</span></code>) was introduced. Comparing to <code class="docutils literal notranslate"><span class="pre">Uniform</span> <span class="pre">Memory</span> <span class="pre">Access</span></code> (<code class="docutils literal notranslate"><span class="pre">UMA</span></code>), where all memories are connected to all cores equally, NUMA divide memories into multiple groups. Certain number of memories are directly attached to one socket’s integrated memory controller to become local memory of this socket. While other memories are on other sockets as remote memory. Local memory access is much faster than remote memory access.</p>
<p>You can get CPU information with <code class="docutils literal notranslate"><span class="pre">lscpu</span></code> command on Linux to get how many cores, sockets on the machine. Also, NUMA information such as how CPU cores are distributed can also be retrieved.
The following is an example of <code class="docutils literal notranslate"><span class="pre">lscpu</span></code> execution on a machine with two Intel® Xeon® Platinum 8180M CPUs. Two sockets were detected. Each socket has 28 physical cores onboard. Since Hyper-Threading is enabled, each core can run 2 threads. That means each socket has another 28 logical cores. Thus, a tatal of 112 CPU cores available. When indexing CPU cores, usually physical cores are indexed prior to logical core. In this case, the first 28 cores (0-27) are physical cores on the first NUMA socket (node), the second 28 cores (28-55) are physical cores on the second <code class="docutils literal notranslate"><span class="pre">NUMA</span></code> socket (node). Logical cores are indexed afterward. 56-83 are 28 logical cores on the first <code class="docutils literal notranslate"><span class="pre">NUMA</span></code> socket (node), 84-111 are the second 28 logical cores on the second <code class="docutils literal notranslate"><span class="pre">NUMA</span></code> socket (node). Typically, avoid running <code class="docutils literal notranslate"><span class="pre">Intel®</span> <span class="pre">Extension</span> <span class="pre">for</span> <span class="pre">TensorFlow*</span></code> on logical cores if you want to get good performance.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ lscpu
...
CPU(s):              112
On-line CPU(s) list: 0-111
Thread(s) per core:  2
Core(s) per socket:  28
Socket(s):           2
NUMA node(s):        2
...
Model name:          Intel(R) Xeon(R) Platinum 8180M CPU @ 2.50GHz
...
NUMA node0 CPU(s):   0-27,56-83
NUMA node1 CPU(s):   28-55,84-111
...
</pre></div>
</div>
</div>
</div>
<div class="section" id="software-configuration">
<h3>Software Configuration<a class="headerlink" href="#software-configuration" title="Permalink to this headline">¶</a></h3>
<p>This section introduces software configurations that helps to boost performance.</p>
<div class="section" id="memory-layout-format">
<h4>Memory Layout format<a class="headerlink" href="#memory-layout-format" title="Permalink to this headline">¶</a></h4>
<p>The default memory layout format of Intel® Extension for TensorFlow* is NHWC format, which is the same with official TensorFlow default format. This format is generally friendly to most of models, but some models may get higher performance with NCHW format by the benefit from <code class="docutils literal notranslate"><span class="pre">oneDNN</span></code> block format.</p>
<p>Below environment settings are for two different memory formats mentioned above.</p>
<p><code class="docutils literal notranslate"><span class="pre">ITEX_LAYOUT_OPT=0</span></code>
<code class="docutils literal notranslate"><span class="pre">ITEX_LAYOUT_OPT=1</span></code></p>
</div>
<div class="section" id="numactl">
<h4>Numactl<a class="headerlink" href="#numactl" title="Permalink to this headline">¶</a></h4>
<p>Since NUMA largely influences memory access performance, the Linux tool <code class="docutils literal notranslate"><span class="pre">numactl</span></code>allows users to control NUMA policy for processes or shared memory. It runs processes with a specific NUMA scheduling or memory placement policy. As described in previous section, cores share high-speed cache in one socket, thus it is a good idea to avoid cross socket computations. From memory access perspective, bounding memory access to local ones is much faster than accessing remote memories.</p>
<p>The following is an example of numactl usage to run a workload on the Nth socket, and limit memory access to its local memories on the Nth socket. More detailed description of numactl command can be found <a class="reference external" href="https://linux.die.net/man/8/numactl">Numactl Linux Man Page</a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">numactl</span> <span class="pre">--cpunodebind</span> <span class="pre">N</span> <span class="pre">--membind</span> <span class="pre">N</span> <span class="pre">python</span> <span class="pre">&lt;script&gt;</span></code></p>
<p>Assume core 0-3 are on socket 0, the following command binds script execution on core 0-3, and binds memory access to socket 0 local memories.</p>
<p><code class="docutils literal notranslate"><span class="pre">numactl</span> <span class="pre">--membind</span> <span class="pre">0</span> <span class="pre">-C</span> <span class="pre">0-3</span> <span class="pre">python</span> <span class="pre">&lt;script&gt;</span></code></p>
<p>[1] <a class="reference external" href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">Wikipedia - Non-uniform memory access</a></p>
</div>
<div class="section" id="openmp">
<h4>OpenMP<a class="headerlink" href="#openmp" title="Permalink to this headline">¶</a></h4>
<p>OpenMP is an implementation of multithreading, a method of parallelizing whereby a master thread (a series of instructions executed consecutively) forks a specified number of sub-threads and the system divides tasks among them. The threads then run concurrently, with the runtime environment allocating threads to different processors[1]. Figure 1 illustrates fork-join model of OpenMP execution.</p>
<p><img alt="A number of parallel block execution threads are forked from primary thread" src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/Fork_join.svg/1920px-Fork_join.svg.png" /></p>
<p>Figure 1 A number of parallel block execution threads are forked from master thread</p>
<p>Users can control OpenMP behaviors through some environment variables to fit for their workloads. Also, beside GNU OpenMP library (<a class="reference external" href="https://gcc.gnu.org/onlinedocs/libgomp/">libgomp</a>), Intel provides another OpenMP implementation <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support.html">libiomp</a> for users to choose from. Environment variables which controls behaviour of OpenMP threads may differ from libgomp and libiomp. They will be introduced separately in sections below.</p>
<p>[1] <a class="reference external" href="https://en.wikipedia.org/wiki/OpenMP">Wikipedia - OpenMP</a></p>
<div class="section" id="omp-num-threads">
<h5>OMP_NUM_THREADS<a class="headerlink" href="#omp-num-threads" title="Permalink to this headline">¶</a></h5>
<p>Environment variable <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> sets the number of threads to use for parallel regions. By default, it is set to be number of available physical cores. It can be used along with <code class="docutils literal notranslate"><span class="pre">numactl</span></code> settings, as the following example. If cores 0-3 are on socket 0, this example command runs &lt;script&gt; on cores 0-3, with 4 <code class="docutils literal notranslate"><span class="pre">OpenMP</span></code> threads.</p>
<p>This environment variable works on both libgomp and libiomp.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">OMP_NUM_THREADS</span><span class="o">=</span><span class="mi">4</span>
<span class="n">numactl</span> <span class="o">-</span><span class="n">C</span> <span class="mi">0</span><span class="o">-</span><span class="mi">3</span> <span class="o">--</span><span class="n">membind</span> <span class="mi">0</span> <span class="n">python</span> <span class="o">&lt;</span><span class="n">script</span><span class="o">&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="gnu-openmp">
<h5>GNU OpenMP<a class="headerlink" href="#gnu-openmp" title="Permalink to this headline">¶</a></h5>
<p>Beside <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code>, a couple of GNU <code class="docutils literal notranslate"><span class="pre">OpenMP</span></code> specific environment variables are commonly used to improve performance.</p>
<ul class="simple">
<li><p>GOMP_CPU_AFFINITY: Binds threads to specific CPUs. The variable should contain a space-separated or comma-separated list of CPUs or Hyphen-separated CPU numbers specify a range of CPUs.</p></li>
<li><p>OMP_PROC_BIND: Specifies whether threads may be moved between processors. Setting it to <code class="docutils literal notranslate"><span class="pre">CLOSE</span></code> keeps OpenMP threads close to the primary thread in contiguous place partitions.</p></li>
<li><p>OMP_SCHEDULE: Determine how OpenMP threads are scheduled.</p></li>
</ul>
<p>Following is a recommended combination of these environment variables:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">GOMP_CPU_AFFINITY</span><span class="o">=</span><span class="s2">&quot;0-3&quot;</span>
<span class="n">export</span> <span class="n">OMP_PROC_BIND</span><span class="o">=</span><span class="n">CLOSE</span>
<span class="n">export</span> <span class="n">OMP_SCHEDULE</span><span class="o">=</span><span class="n">STATIC</span>
</pre></div>
</div>
</div>
<div class="section" id="intel-openmp">
<h5>Intel OpenMP<a class="headerlink" href="#intel-openmp" title="Permalink to this headline">¶</a></h5>
<p>On Intel platforms, Intel OpenMP Runtime Library (libiomp) provides OpenMP API specification support. It can provide better performance compared to libgomp. Utilizing environment variable LD_PRELOAD can switch OpenMP library to libiomp:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export LD_PRELOAD=&lt;path&gt;/libiomp5.so:$LD_PRELOAD
</pre></div>
</div>
<p>Similar to GNU OpenMP, beside <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code>, there are several Intel OpenMP specific environment variables that control behavior of OpenMP threads.</p>
<ul>
<li><p>KMP_AFFINITY</p>
<p>KMP_AFFINITY controls how to to bind OpenMP threads to physical processing units. Depending on the system (machine) topology, application, and operating system, thread affinity can have a dramatic effect on the application speed.</p>
<p>A common usage scenario is for consecutive threads to be bound close together, as is done with <code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code>=compact, so that communication overhead, cache line invalidation overhead, and page thrashing are minimized. Now, suppose the application also had a number of parallel regions which did not utilize all of the available OpenMP threads. We should avoid binding multiple threads to the same core and leaving other cores not utilized, since a thread normally executes faster on a core where it is not competing for resources with another active thread on the same core. This can be achieved by the following command. Figure 2.2 illustrates this strategy.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">KMP_AFFINITY</span><span class="o">=</span><span class="n">granularity</span><span class="o">=</span><span class="n">fine</span><span class="p">,</span><span class="n">compact</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span>
</pre></div>
</div>
<p><img alt="KMP_AFFINITY=granularity=fine,compact,1,0" src="../../_images/kmp_affinity.jpg" /></p>
<p>​				Figure 2.2 <em>KMP_AFFINITY=granularity=fine,compact,1,0</em></p>
<p>The OpenMP thread n+1 is bound to a thread context as close as possible to OpenMP thread n, but on a different core. Once each core has been assigned one OpenMP thread, the subsequent OpenMP threads are assigned to the available cores in the same order, but they are assigned on different thread contexts.</p>
<p>It is also possible to bind OpenMP threads to certain CPU cores with the following command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">KMP_AFFINITY</span><span class="o">=</span><span class="n">granularity</span><span class="o">=</span><span class="n">fine</span><span class="p">,</span><span class="n">proclist</span><span class="o">=</span><span class="p">[</span><span class="n">N</span><span class="o">-</span><span class="n">M</span><span class="p">],</span><span class="n">explicit</span>
</pre></div>
</div>
<p>More detailed information about KMP_AFFINITY can be found <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support/openmp-library-support/thread-affinity-interface-linux-and-windows.html">Intel® C++ Compiler Classic Developer Guide and Reference</a>.</p>
</li>
<li><p>KMP_BLOCKTIME</p>
<p>KMP_BLOCKTIME sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping. The default value is 200ms.</p>
<p>After completing the execution of a parallel region, threads wait for new parallel work to become available. After a certain period of time has elapsed, they stop waiting and sleep. Sleeping allows the threads to be used, until more parallel work becomes available, by non-OpenMP threaded code that may execute between parallel regions, or by other applications. A small <code class="docutils literal notranslate"><span class="pre">KMP_BLOCKTIME</span></code> value may offer better overall performance if the application contains non-OpenMP threaded code that executes between parallel regions. A larger KMP_BLOCKTIME value may be more appropriate if threads are to be reserved solely for use for OpenMP execution, but may penalize other concurrently-running OpenMP or threaded applications. It is suggested to be set to 0 or 1 for convolutional neural network (CNN) based models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">KMP_BLOCKTIME</span><span class="o">=</span><span class="mi">0</span> <span class="p">(</span><span class="ow">or</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="section" id="memory-allocator">
<h4>Memory Allocator<a class="headerlink" href="#memory-allocator" title="Permalink to this headline">¶</a></h4>
<p>Memory allocator plays an important performance role as well. A more efficient memory usage reduces overhead on unnecessary memory allocations or destructions, and thus results in a faster execution. From practical experiences, for deep learning workloads, TCMalloc can get better performance by reusing memory as much as possible than default malloc funtion.</p>
<p>To enable TCMalloc, adding path of TCMalloc dynamic library to environment variable LD_PRELOAD to switch the memory allocator.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export LD_PRELOAD=&lt;jemalloc.so/tcmalloc.so&gt;:$LD_PRELOAD
</pre></div>
</div>
<div class="section" id="tcmalloc">
<h5>TCMalloc<a class="headerlink" href="#tcmalloc" title="Permalink to this headline">¶</a></h5>
<p><a class="reference external" href="https://github.com/google/tcmalloc">TCMalloc</a> features optimizations to speed up program executions. One of them is holding memory in caches to speed up access of commonly-used objects. Holding such caches even after deallocation helps avoid costly system calls if such memory is later re-allocated. It is part of <a class="reference external" href="https://github.com/gperftools/gperftools">gperftools</a>, a collection of a high-performance multi-threaded malloc() implementation, plus some nice performance analysis tools.</p>
<p>Install <code class="docutils literal notranslate"><span class="pre">gperftools</span></code> using these instructions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">gperftools</span><span class="o">/</span><span class="n">gperftools</span><span class="o">/</span><span class="n">releases</span><span class="o">/</span><span class="n">download</span><span class="o">/</span><span class="n">gperftools</span><span class="o">-&lt;</span><span class="n">version</span><span class="o">&gt;/</span><span class="n">gperftools</span><span class="o">-&lt;</span><span class="n">version</span><span class="o">&gt;.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span>
<span class="n">tar</span> <span class="n">xzvf</span> <span class="n">gperftools</span><span class="o">-&lt;</span><span class="n">version</span><span class="o">&gt;.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span>
<span class="n">cd</span> <span class="n">gperftools</span><span class="o">-&lt;</span><span class="n">version</span><span class="o">&gt;</span>
<span class="o">./</span><span class="n">configure</span> <span class="o">--</span><span class="n">disable</span><span class="o">-</span><span class="n">cpu</span><span class="o">-</span><span class="n">profiler</span> <span class="o">--</span><span class="n">disable</span><span class="o">-</span><span class="n">heap</span><span class="o">-</span><span class="n">profiler</span> <span class="o">--</span><span class="n">disable</span><span class="o">-</span><span class="n">heap</span><span class="o">-</span><span class="n">checker</span> <span class="o">--</span><span class="n">disable</span><span class="o">-</span><span class="n">debugalloc</span> <span class="o">--</span><span class="n">enable</span><span class="o">-</span><span class="n">minimal</span> 
<span class="n">make</span>
<span class="n">make</span> <span class="n">install</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="gpu-practice-guide">
<h2>GPU Practice Guide<a class="headerlink" href="#gpu-practice-guide" title="Permalink to this headline">¶</a></h2>
<p>The default configuration is recommended for GPU practice.</p>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../../examples/examples.html" class="btn btn-neutral float-left" title="Examples" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="FAQ.html" class="btn btn-neutral float-right" title="Frequently Asked Questions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Copyright (c) 2022 Intel Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>