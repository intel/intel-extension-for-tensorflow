<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NextPluggableDevice Overview &mdash; Intel® Extension for TensorFlow* 0.1.dev1+ga990da2 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=439db15d" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-tensorflow"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel® Extension for TensorFlow*
          </a>
            <div class="version">
              <a href="../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../get_started.html">Intel® Extension for TensorFlow*</a></li>
<li class="toctree-l1"><a class="reference internal" href="infrastructure.html">Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/installation_guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/README.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="practice_guide.html">Practice Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/contributing.html">Contributing guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-tensorflow">GitHub Repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel® Extension for TensorFlow*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">NextPluggableDevice Overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/docs/guide/next_pluggable_device.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="nextpluggabledevice-overview">
<h1>NextPluggableDevice Overview<a class="headerlink" href="#nextpluggabledevice-overview" title="Link to this heading"></a></h1>
<p>The NextPluggableDevice (NPD) represents an advanced generation of <a class="reference external" href="https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.html">PluggableDevice</a> mechanism. It not only facilitates a seamless integration of new accelerator plugins for registering devices with TensorFlow without requiring modifications to the TensorFlow codebase, but it also serves as a conduit to <a class="reference external" href="https://github.com/openxla/xla">OpenXLA (Accelerated Linear Algebra)</a> via its <a class="reference external" href="https://github.com/openxla/community/blob/main/rfcs/20230123-pjrt-plugin.html">PJRT plugin</a>.</p>
<ul class="simple">
<li><p><a class="reference external" href="#NextPluggableDevice-Overview">Overview</a></p></li>
<li><p><a class="reference external" href="#Why-NextPluggableDevice">Why NextPluggableDevice</a></p></li>
<li><p><a class="reference external" href="#How-to-start-with-XLA-using-NextPluggableDevice">Starting With NextPluggableDevice</a></p></li>
<li><p><a class="reference external" href="#NextPluggableDevice-Architecture">Architecture</a></p></li>
<li><p><a class="reference external" href="#Runtime-Switch-of-NextPluggableDevice-and-PluggableDevice">Runtime Switch</a></p></li>
</ul>
<section id="why-nextpluggabledevice">
<h2>Why NextPluggableDevice<a class="headerlink" href="#why-nextpluggabledevice" title="Link to this heading"></a></h2>
<p>Previously, Stock TensorFlow has designed &amp; developed the <a class="reference external" href="https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.html">PluggableDevice</a> to extend new device extensions without making device-specific changes to the TensorFlow code, and the PluggableDevice is tightly integrated with the <a class="reference external" href="https://github.com/tensorflow/community/pull/257">StreamExecutor C API</a> today.</p>
<p>However, excessive binding with StreamExecutor has made it difficult for the PluggableDevice to be compatible
with <a class="reference external" href="https://github.com/openxla/xla">OpenXLA</a>. Precisely for this reason, TensorFlow evolved to use <a class="reference external" href="https://github.com/openxla/community/blob/main/rfcs/20230123-pjrt-plugin.html">PJRT plugin</a> as the device API, resulting in the decoupling of the Pluggable Device from StreamExecutor, implemented as NextPluggableDevice.</p>
</section>
<section id="start-with-xla-using-nextpluggabledevice">
<h2>Start with XLA using NextPluggableDevice<a class="headerlink" href="#start-with-xla-using-nextpluggabledevice" title="Link to this heading"></a></h2>
<p>Enabling XLA in ITEX is exactly the same as it is in TensorFlow, except that you need to export environment variables first:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ export TF_XLA_FLAGS=&quot;--tf_xla_use_device_api=true  --tf_xla_auto_jit=2&quot;
$ python
&gt;&gt;&gt; import tensorflow as tf  # TensorFlow registers NextPluggableDevice here
&gt;&gt;&gt; @tf.function(experimental_compile=True)
... def add_with_xla(a, b):
...     return a + b
&gt;&gt;&gt; a = tf.constant([1.0, 2.0, 3.0])
&gt;&gt;&gt; b = tf.constant([4.0, 5.0, 6.0])
&gt;&gt;&gt; result = add_with_xla(a, b)
&gt;&gt;&gt; print(&quot;Result: &quot;, result)
Result:  tf.Tensor([5. 7. 9.], shape=(3,), dtype=float32)
</pre></div>
</div>
</section>
<section id="nextpluggabledevice-architecture">
<h2>NextPluggableDevice Architecture<a class="headerlink" href="#nextpluggabledevice-architecture" title="Link to this heading"></a></h2>
<p>The NextPluggableDevice represents an advanced generation of the <a class="reference external" href="https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.html">PluggableDevice</a> mechanism. Intel® Extension for TensorFlow* integrates the NextPluggableDevice as a new device type, along with the corresponding <a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/pjrt/c/pjrt_c_api.h">PJRT CAPI</a> for registering its Ops &amp; Kernels, XLA PJRT client, Runtime, as well as the legacy Graph Optimization API and Profiler interface. In this way, it not only facilitates a seamless integration of new accelerator plugins for registering devices with TensorFlow without requiring modifications to the TensorFlow codebase, but it also serves as a conduit to <a class="reference external" href="https://github.com/openxla/xla">OpenXLA</a> via its <a class="reference external" href="https://github.com/openxla/community/blob/main/rfcs/20230123-pjrt-plugin.html">PJRT plugin</a>.</p>
<p align="center">
  <img src="images/npd_architecture.png" alt="npd_architecture.png" />
</p><section id="openxla-pjrt-plugin">
<h3>OpenXLA PJRT Plugin<a class="headerlink" href="#openxla-pjrt-plugin" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/pjrt/c/pjrt_c_api.h">PJRT</a> is a uniform Device API in the OpenXLA ecosystem. The long term vision for PJRT is that: (1) frameworks (TensorFlow, PyTorch, JAX, etc.) will call PJRT, which has device-specific implementations that are opaque to the frameworks; (2) each device focuses on implementing PJRT APIs, and can remain opaque to the frameworks.</p>
<p align="center">
  <img src="images/openxla_pjrt.png" alt="openxla_pjrt.png" />
</p><p>the PJRT API will provide an easy interface with which frameworks can integrate a packaged compiler and runtime solution. It will be the supported interface used by TensorFlow and JAX for all compiler and runtime integration. And as such it will be easy for other compilers and runtimes that implement the PJRT interface to integrate with these systems.</p>
</section>
</section>
<section id="runtime-switch-of-nextpluggabledevice-and-pluggabledevice">
<h2>Runtime Switch of NextPluggableDevice and PluggableDevice<a class="headerlink" href="#runtime-switch-of-nextpluggabledevice-and-pluggabledevice" title="Link to this heading"></a></h2>
<p>Intel® Extension for TensorFlow* is fully integrated with both PluggableDevice and NextPluggableDevice, and provides a runtime switch mechanism for enhancing both efficiency and flexibility. Figure 3 presents the architectures of the PluggableDevice and NextPluggableDevice.</p>
<p align="center">
  <img src="images/npd_pd_architecture.png" alt="npd_pd_architecture.png" />
</p><p>Intel® Extension for TensorFlow* offers environmental variables to enable and disable different devices. Simply export the variables listed in the table below within your runtime environment to selectively enable or disable the corresponding device.
|Environment Variable|NextPluggableDevice|PluggableDevice|
|:-|:-:|:-:|
|export ITEX_ENABLE_NEXTPLUGGABLE_DEVICE=1| enabled | disabled |
|export ITEX_ENABLE_NEXTPLUGGABLE_DEVICE=0| disabled | enabled |
|export TF_XLA_FLAGS=”–tf_xla_use_device_api=true  –tf_xla_auto_jit=2”| enabled | disabled |
|default| enabled | disabled |</p>
<section id="check-currently-used-device-type">
<h3>Check Currently Used Device Type<a class="headerlink" href="#check-currently-used-device-type" title="Link to this heading"></a></h3>
<p>In order to easily distinguish the currently used device type, users can check the verbose output as below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Using NextPluggableDevice</span>
<span class="n">tensorflow</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">common_runtime</span><span class="o">/</span><span class="n">next_pluggable_device</span><span class="o">/</span><span class="n">next_pluggable_device_factory</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">118</span><span class="p">]</span> <span class="n">Created</span> <span class="mi">1</span> <span class="n">TensorFlow</span> <span class="n">NextPluggableDevices</span><span class="o">.</span> <span class="n">Physical</span> <span class="n">device</span> <span class="nb">type</span><span class="p">:</span> <span class="n">XPU</span>

<span class="c1"># Using PluggableDevice</span>
<span class="n">tensorflow</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">common_runtime</span><span class="o">/</span><span class="n">pluggable_device</span><span class="o">/</span><span class="n">pluggable_device_factory</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">272</span><span class="p">]</span> <span class="n">Created</span> <span class="n">TensorFlow</span> <span class="n">device</span> <span class="o">-&gt;</span> <span class="n">physical</span> <span class="n">PluggableDevice</span> <span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="n">XPU</span><span class="p">,</span> <span class="n">pci</span> <span class="n">bus</span> <span class="nb">id</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">undefined</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022-2024 Intel Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f8fece76530> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>