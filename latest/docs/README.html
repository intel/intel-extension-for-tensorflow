<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Welcome to Intel® Extension for TensorFlow* documentation &mdash; Intel® Extension for TensorFlow* 0.1.dev1+gc71ec32 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=439db15d" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-tensorflow"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Intel® Extension for TensorFlow*
          </a>
            <div class="version">
              <a href="../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started.html">Intel® Extension for TensorFlow*</a></li>
<li class="toctree-l1"><a class="reference internal" href="guide/infrastructure.html">Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="guide/features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="guide/performance.html">Performance Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="install/installation_guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/README.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="guide/practice_guide.html">Practice Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="guide/FAQ.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contributing.html">Contributing guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-tensorflow">GitHub Repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel® Extension for TensorFlow*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Welcome to Intel® Extension for TensorFlow* documentation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/docs/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="welcome-to-intel-extension-for-tensorflow-documentation">
<h1>Welcome to Intel® Extension for TensorFlow* documentation<a class="headerlink" href="#welcome-to-intel-extension-for-tensorflow-documentation" title="Link to this heading"></a></h1>
<section id="documentation">
<h2>Documentation<a class="headerlink" href="#documentation" title="Link to this heading"></a></h2>
<table class="docutils">
  <thead>
  <tr>
    <th colspan="12">Overview</th>
  </tr>
  </thead>
  <tbody>
    <tr>
      <td colspan="3" align="center"><a href="guide/infrastructure.html">Infrastructure</a></td>
      <td colspan="3" align="center"><a href="../examples/quick_example.html">Quick example</a></td>
      <td colspan="3" align="center"><a href="../examples">Examples</a></td>
      <td colspan="3" align="center"><a href="community/releases.html">Releases</a></td>
    </tr>
    <tr>
      <td colspan="3" align="center"><a href="guide/performance.html">Performance data</a></td>
      <td colspan="6" align="center"><a href="guide/FAQ.html">Frequently asked questions</a></td>
      <td colspan="3" align="center"><a href="community/contributing.html">Contributing guidelines</a></td>
    </tr>
  </tbody>
  <thead>
  <tr>
    <th colspan="12">Installation guide</th>
  </tr>
  </thead>
  <tbody>
    <tr>
      <td colspan="3" align="center"><a href="install/install_for_cpu.html">Install for CPU</a></td>
      <td colspan="3" align="center"><a href="install/install_for_xpu.html">Install for XPU</a></td>
      <td colspan="3" align="center"><a href="install/how_to_build.html">Install by source build</a></td>
	  <td colspan="3" align="center"><a href="install/experimental/install_for_gpu_conda.html">Install Conda for GPU distributed</a></td>
    </tr>
  </tbody>
  <thead>
    <tr>
      <th colspan="12">Features</th>
    </tr>
  </thead>
  <tbody>
    <tr>
        <td colspan="2" align="center"><a href="guide/environment_variables.html">Environment variables</a></td>
	    	<td colspan="2" align="center"><a href="guide/python_api.html">Python API</a></td>
        <td colspan="4" align="center"><a href="guide/next_pluggable_device.html">Next Pluggable Device</a></td>
        <td colspan="2" align="center"><a href="guide/threadpool.html">CPU Thread Pool</a></td>
    </tr>
    <tr>
        <td colspan="2" align="center"><a href="guide/itex_fusion.html">Graph optimization</a></td>
        <td colspan="2" align="center"><a href="guide/itex_ops.html">Custom operator</a></td>
        <td colspan="4" align="center"><a href="guide/advanced_auto_mixed_precision.html">Advanced auto mixed precision</a></td>
	      <td colspan="2" align="center"><a href="guide/itex_ops_override.html">Operator override</a></td>
    </tr>
    <tr>    
	      <td colspan="3" align="center"><a href="guide/INT8_quantization.html">INT8 quantization</a></td>
	      <td colspan="2" align="center"><a href="guide/XPUAutoShard.html">XPUAutoShard</a></td>
        <td colspan="2" align="center"><a href="guide/how_to_enable_profiler.html">GPU profiler</a></td>
	      <td colspan="2" align="center"><a href="guide/launch.html">CPU launcher</a></td>
      	<td colspan="2" align="center"><a href="guide/weight_prepack.html">Weight prepack</a></td>
    </tr>
  </tbody>
  <thead>
      <tr>
        <th colspan="12">Advanced topics</th>
      </tr>
  </thead>
  <tbody>
      <tr>
        <td colspan="3" align="center"><a href="guide/practice_guide.html#cpu-practice-guide">CPU practice guide</a></td>
        <td colspan="3" align="center"><a href="guide/practice_guide.html#gpu-practice-guide">GPU practice guide</a></td>
        <td colspan="2" align="center"><a href="install/install_for_cpp.html">C++ API support</a></td>
        <td colspan="2" align="center"><a href="guide/OpenXLA.html">OpenXLA</a></td>
        <td colspan="2" align="center"><a href="guide/keras3_support.html">Keras 3</a></td>
      </tr>
  </tbody>
    <thead>
      <tr>
        <th colspan="12">Developer Guide</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td colspan="3" align="center"><a href="design/extension_design.html">Extension design</a></td>
	  <td colspan="3" align="center"><a href="design/directory_structure.html">Directory structure</a></td>
	  <td colspan="3" align="center"><a href="design/optimization/README.html">Optimizations design</a></td>
          <td colspan="3" align="center"><a href="design/how_to_write_custom_op.html">Custom Op</a></td>
      </tr>
  </tbody>
</table></section>
<section id="highlights">
<h2>Highlights<a class="headerlink" href="#highlights" title="Link to this heading"></a></h2>
<ul>
<li><p>Environment variables &amp; Python API</p>
<p>Generally, the default configuration of Intel® Extension for TensorFlow* provides good performance without any code changes.
Intel® Extension for TensorFlow* also provides simple frontend Python APIs and utilities for advanced users to get more optimized performance with only minor code changes for different kinds of application scenarios. Typically, you only need to add two or three clauses to the original code.</p>
</li>
<li><p>Next Pluggable Device (NPD)</p>
<p>The Next Pluggable Device (NPD) represents an advanced generation of TensorFlow plugin mechanisms. It not only facilitates a seamless integration of new accelerator plugins for registering devices with TensorFlow without requiring modifications to the TensorFlow codebase, but it also serves as a conduit to OpenXLA via its PJRT plugin. This innovative approach significantly streamlines the process of extending TensorFlow’s capabilities with new hardware accelerators, enhancing both efficiency and flexibility.</p>
</li>
<li><p>Advanced auto mixed precision (AMP)</p>
<p>Low precision data types <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> and<code class="docutils literal notranslate"> <span class="pre">float16</span></code> are natively supported by the <code class="docutils literal notranslate"><span class="pre">3rd</span> <span class="pre">Generation</span> <span class="pre">Xeon®</span> <span class="pre">Scalable</span> <span class="pre">Processors</span></code>, codenamed <a class="reference external" href="https://ark.intel.com/content/www/us/en/ark/products/series/204098/3rd-generation-intel-xeon-scalable-processors.html">Cooper Lake</a>,  with <code class="docutils literal notranslate"><span class="pre">AVX512</span></code> instruction set and the Intel® Data Center GPU, which further boosts performance and uses less memory. The lower-precision data types supported by Advanced Auto Mixed Precision (AMP) are fully enabled in Intel® Extension for TensorFlow*.</p>
</li>
<li><p>Graph optimization</p>
<p>Intel® Extension for TensorFlow* provides graph optimization to fuse specific operator patterns to a new single operator for better performance, such as <code class="docutils literal notranslate"><span class="pre">Conv2D+ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">Linear+ReLU</span></code>.  The benefits of the fusions are delivered to users in a transparent fashion.</p>
</li>
<li><p>CPU Thread Pool</p>
<p>Intel® Extension for TensorFlow* uses OMP thread pool by default since it has better performance and scaling for most cases. For workloads with large inter-op concurrency, you can switch to use Eigen thread pool (default in TensorFlow) by setting the environment variable <code class="docutils literal notranslate"><span class="pre">ITEX_OMP_THREADPOOL=0</span></code>.</p>
</li>
<li><p>Operator optimization</p>
<p>Intel® Extension for TensorFlow* also optimizes operators and implements several customized operators for a performance boost. The <code class="docutils literal notranslate"><span class="pre">itex.ops</span></code> namespace is used to extend TensorFlow public APIs implementation for better performance.</p>
</li>
<li><p>GPU profiler</p>
<p>Intel® Extension for TensorFlow* provides support for TensorFlow <a class="reference external" href="https://www.tensorflow.org/guide/profiler">Profiler</a>. To enable the profiler, define three environment variables ( <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">ZE_ENABLE_TRACING_LAYER=1</span></code>, <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">UseCyclesPerSecondTimer=1</span></code>, <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">ENABLE_TF_PROFILER=1</span></code>)</p>
</li>
<li><p>INT8 quantization</p>
<p>Intel® Extension for TensorFlow* co-works with <a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor</a> to provide compatible TensorFlow INT8 quantization solution support with equivalent user experience.</p>
</li>
<li><p>XPUAutoShard on GPU [Experimental]</p>
<p>Intel® Extension for TensorFlow* provides XPUAutoShard feature to automatically shard the input data and the TensorFlow graph, placing these data/graph shards on GPU devices to maximize the hardware usage.</p>
</li>
<li><p>OpenXLA</p>
<p>Intel® Extension for TensorFlow* adopts a uniform Device API PJRT as the supported device plugin mechanism to implement Intel GPU backend for OpenXLA support on TensorFlow frontend.</p>
</li>
<li><p>Keras 3
Keras 3 with TensorFlow comes with a significant enhancement - the Just-In-Time (JIT) compilation is enabled by default. This feature leverages the XLA (Accelerated Linear Algebra) compiler to optimize TensorFlow computations. See <a href="guide/keras3_support.html">Keras 3</a> to avoid possible performance issues and error.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022-2025 Intel Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7fe965c93500> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>