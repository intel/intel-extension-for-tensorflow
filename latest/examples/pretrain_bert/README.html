<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Accelerate BERT-Large Pretraining on Intel GPU &mdash; Intel® Extension for TensorFlow* 0.1.dev1+g44c85da documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=439db15d" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-tensorflow"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel® Extension for TensorFlow*
          </a>
            <div class="version">
              <a href="../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../get_started.html">Intel® Extension for TensorFlow*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/guide/infrastructure.html">Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/guide/features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/guide/performance.html">Performance Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/install/installation_guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../README.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/guide/practice_guide.html">Practice Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/guide/FAQ.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/community/releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/community/contributing.html">Contributing guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-tensorflow">GitHub Repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel® Extension for TensorFlow*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Accelerate BERT-Large Pretraining on Intel GPU</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/examples/pretrain_bert/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="accelerate-bert-large-pretraining-on-intel-gpu">
<h1>Accelerate BERT-Large Pretraining on Intel GPU<a class="headerlink" href="#accelerate-bert-large-pretraining-on-intel-gpu" title="Link to this heading"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>Intel® Extension for TensorFlow* is compatible with stock TensorFlow*.
This example shows BERT-Large Pretraining.</p>
<p>Install the Intel® Extension for TensorFlow* in legacy running environment, Tensorflow will execute the Training on Intel GPU.</p>
</section>
<section id="hardware-requirements">
<h2>Hardware Requirements<a class="headerlink" href="#hardware-requirements" title="Link to this heading"></a></h2>
<p>Verified Hardware Platforms:</p>
<ul class="simple">
<li><p>Intel® Data Center GPU Max Series</p></li>
</ul>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading"></a></h2>
<section id="model-code-change">
<h3>Model Code change<a class="headerlink" href="#model-code-change" title="Link to this heading"></a></h3>
<p>We set up BERT-Large pretraining based on nvidia-bert. We optimized nvidia-bert, for example, using custom kernels, fusing some ops to reduce op number, and adding bf16 mode for the model.</p>
<p>To get better performance, instead of installing official nvidia-bert, you can clone nvidia-bert, apply the patch, then install it as shown here:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">NVIDIA</span><span class="o">/</span><span class="n">DeepLearningExamples</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">DeepLearningExamples</span><span class="o">/</span><span class="n">TensorFlow2</span><span class="o">/</span><span class="n">LanguageModeling</span><span class="o">/</span><span class="n">BERT</span>
<span class="n">cp</span> <span class="o">../../../../</span><span class="n">patch</span> <span class="o">.</span> <span class="c1"># When applying this patch, please move it to the above BERT dir first.</span>
<span class="n">git</span> <span class="n">am</span> <span class="o">--</span><span class="n">signoff</span> <span class="o">&lt;</span> <span class="n">patch</span>  <span class="c1"># It&#39;s ok when meeting warning about whitespace errors.</span>
</pre></div>
</div>
</section>
<section id="prepare-for-gpu">
<h3>Prepare for GPU<a class="headerlink" href="#prepare-for-gpu" title="Link to this heading"></a></h3>
<p>Refer to <a class="reference external" href="../common_guide_running.html#prepare">Prepare</a>.</p>
</section>
<section id="setup-running-environment">
<h3>Setup Running Environment<a class="headerlink" href="#setup-running-environment" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Setup for GPU</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./pip_set_env.sh
</pre></div>
</div>
<p>If you use conda env, you can install packages in <code class="docutils literal notranslate"><span class="pre">./pip_set_env.sh</span></code> manually.</p>
</section>
<section id="enable-running-environment">
<h3>Enable Running Environment<a class="headerlink" href="#enable-running-environment" title="Link to this heading"></a></h3>
<p>Enable oneAPI running environment (only for GPU) and virtual running environment.</p>
<ul class="simple">
<li><p>For GPU, refer to <a class="reference external" href="../common_guide_running.html#running">Running</a></p></li>
</ul>
</section>
<section id="prepare-dataset">
<h3>Prepare Dataset<a class="headerlink" href="#prepare-dataset" title="Link to this heading"></a></h3>
<p>Nvidia-bert repository provides scripts to download, verify, and extract the SQuAD dataset and pretrained weights for fine-tuning as well as Wikipedia and BookCorpus dataset for pre-training.</p>
<p>You can run below scripts to download datasets for fine-tuning and pretraining. Assume current_dir is <code class="docutils literal notranslate"><span class="pre">examples/pretrain_bert/DeepLearningExamples/TensorFlow2/LanguageModeling/BERT</span></code>. And you should modify the environment varible <code class="docutils literal notranslate"><span class="pre">BERT_PREP_WORKING_DIR</span></code> in <code class="docutils literal notranslate"><span class="pre">data/create_datasets_from_start.sh</span></code> to the absolute/relative path of <code class="docutils literal notranslate"><span class="pre">examples/pretrain_bert/DeepLearningExamples/TensorFlow2/LanguageModeling/BERT/data</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bash</span> <span class="n">data</span><span class="o">/</span><span class="n">create_datasets_from_start</span><span class="o">.</span><span class="n">sh</span> <span class="nb">all</span> <span class="n">wiki_only</span>
</pre></div>
</div>
<p>For more details about downloading and processing the dataset, you can reference <a class="reference external" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow2/LanguageModeling/BERT#quick-start-guide">downloading</a> and <a class="reference external" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow2/LanguageModeling/BERT#getting-the-data">processing</a> part. After downloading and processing, the datasets are supposed in the following locations by default</p>
<ul class="simple">
<li><p>SQuAD v1.1 - <code class="docutils literal notranslate"><span class="pre">data/download/squad/v1.1</span></code></p></li>
<li><p>SQuAD v2.0 - <code class="docutils literal notranslate"><span class="pre">data/download/squad/v2.0</span></code></p></li>
<li><p>BERT-Large - <code class="docutils literal notranslate"><span class="pre">data/download/google_pretrained_weights/uncased_L-24_H-1024_A-16</span></code></p></li>
<li><p>BERT-Base - <code class="docutils literal notranslate"><span class="pre">data/download/google_pretrained_weights/uncased_L-12_H-768_A-12</span></code></p></li>
<li><p>Wikipedia TFRecords - <code class="docutils literal notranslate"><span class="pre">data/tfrecords/wikicorpus_en</span></code></p></li>
</ul>
</section>
</section>
<section id="execute-the-example">
<h2>Execute the Example<a class="headerlink" href="#execute-the-example" title="Link to this heading"></a></h2>
<p>Bert pretraining is very time-consuming, as nvidia-bert repository says, training BERT-Large from scratch on 16 V100 using FP16 datatype takes around 4.5 days. So Here we only provide single-tile pretraining scripts within a day to show performance.</p>
<section id="id1">
<h3>Prerequisites<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<p>We use <code class="docutils literal notranslate"><span class="pre">intel-optimization-for-horovod</span></code> to implement efficient multi-GPU training with OneCCL. If you want to use multi-GPU, please replace <code class="docutils literal notranslate"><span class="pre">horovod</span></code> with <code class="docutils literal notranslate"><span class="pre">intel-optimization-for-horovod</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">uninstall</span> <span class="n">horovod</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">intel</span><span class="o">-</span><span class="n">optimization</span><span class="o">-</span><span class="k">for</span><span class="o">-</span><span class="n">horovod</span>
</pre></div>
</div>
</section>
<section id="pretraining-command">
<h3>Pretraining Command<a class="headerlink" href="#pretraining-command" title="Link to this heading"></a></h3>
<p>Assume current_dir is <code class="docutils literal notranslate"><span class="pre">examples/pretrain_bert/DeepLearningExamples/TensorFlow2/LanguageModeling/BERT</span></code>. Please set DATA_DIR to your real data dir.</p>
<ul class="simple">
<li><p>BFloat16 DataType</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DATATYPE</span><span class="o">=</span><span class="n">bf16</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Float32 DataType</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DATATYPE</span><span class="o">=</span><span class="n">fp32</span>
</pre></div>
</div>
<ul class="simple">
<li><p>TF32 DataType</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">ITEX_FP32_MATH_MODE</span><span class="o">=</span><span class="n">TF32</span>
<span class="n">DATATYPE</span><span class="o">=</span><span class="n">fp32</span>
</pre></div>
</div>
<p><strong>Final Scripts</strong></p>
<ul class="simple">
<li><p>We use <a class="reference external" href="https://arxiv.org/pdf/1904.00962.pdf">LAMB</a> as the optimizer and pretraining has two phases. The maximum sequence length of phase1 and phase2 is 128 and 512, respectively. For the whole process of pretraining, you can use scripts in <a class="reference external" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow2/LanguageModeling/BERT#training-process">nvidia-bert</a>.</p></li>
<li><p>BFloat16 DataType</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export TF_USE_LEGACY_KERAS=1  # use Keras 2 for tf2.16

TRAIN_BATCH_SIZE_PHASE1=312
TRAIN_BATCH_SIZE_PHASE2=40
EVAL_BATCH_SIZE=8
LEARNING_RATE_PHASE1=8.12e-4
LEARNING_RATE_PHASE2=5e-4
DATATYPE=$DATATYPE
USE_XLA=false
NUM_GPUS=1
WARMUP_STEPS_PHASE1=810
WARMUP_STEPS_PHASE2=81
TRAIN_STEPS=2600
SAVE_CHECKPOINT_STEPS=100
NUM_ACCUMULATION_STEPS_PHASE1=32
NUM_ACCUMULATION_STEPS_PHASE2=96
BERT_MODEL=large

GBS1=$(expr $TRAIN_BATCH_SIZE_PHASE1 \* $NUM_GPUS \* $NUM_ACCUMULATION_STEPS_PHASE1)
GBS2=$(expr $TRAIN_BATCH_SIZE_PHASE2 \* $NUM_GPUS \* $NUM_ACCUMULATION_STEPS_PHASE2)

PRETRAIN_RESULT_DIR=./results/tf_bert_pretraining_lamb_${BERT_MODEL}_${$DATATYPE}_gbs1_${GBS1}_gbs2_${GBS2}
DATA_DIR=$DATA_DIR

bash scripts/run_pretraining_lamb.sh \
    $TRAIN_BATCH_SIZE_PHASE1 \
    $TRAIN_BATCH_SIZE_PHASE2 \
    $EVAL_BATCH_SIZE \
    $LEARNING_RATE_PHASE1 \
    $LEARNING_RATE_PHASE2 \
    $DATATYPE \
    $USE_XLA \
    $NUM_GPUS \
    $WARMUP_STEPS_PHASE1 \
    $WARMUP_STEPS_PHASE2 \
    $TRAIN_STEPS \
    $SAVE_CHECKPOINT_STEPS \
    $NUM_ACCUMULATION_STEPS_PHASE1 \
    $NUM_ACCUMULATION_STEPS_PHASE2 \
    $BERT_MODEL \
    $DATA_DIR \
    $PRETRAIN_RESULT_DIR \
    |&amp; tee pretrain_lamb.log
</pre></div>
</div>
<ul class="simple">
<li><p>Float32/TF32 DataType
To avoid Out-Of-Memory, please use below hyperparameters instead of above bf16 ones.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TRAIN_BATCH_SIZE_PHASE1</span><span class="o">=</span><span class="mi">60</span>
<span class="n">TRAIN_BATCH_SIZE_PHASE2</span><span class="o">=</span><span class="mi">10</span>
<span class="n">LEARNING_RATE_PHASE1</span><span class="o">=</span><span class="mf">7.5e-4</span>
<span class="n">LEARNING_RATE_PHASE2</span><span class="o">=</span><span class="mf">5e-4</span>
<span class="n">NUM_ACCUMULATION_STEPS_PHASE1</span><span class="o">=</span><span class="mi">64</span>
<span class="n">NUM_ACCUMULATION_STEPS_PHASE2</span><span class="o">=</span><span class="mi">192</span>
</pre></div>
</div>
<p><strong>Note</strong>: If you want to run on more cards and even multi-node, you could adjust the parameter <code class="docutils literal notranslate"><span class="pre">NUM_GPUS</span></code>.</p>
</section>
<section id="finetune-command">
<h3>Finetune Command<a class="headerlink" href="#finetune-command" title="Link to this heading"></a></h3>
<p>Assume current_dir is <code class="docutils literal notranslate"><span class="pre">examples/pretrain_bert/DeepLearningExamples/TensorFlow2/LanguageModeling/BERT</span></code>. <code class="docutils literal notranslate"><span class="pre">$PRETRAIN_RESULT_DIR</span></code> is from previous pretraining. It will finetune using this pretraining checkpoint.</p>
<ul class="simple">
<li><p>BFloat16 DataType</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DATATYPE</span><span class="o">=</span><span class="n">bf16</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Float32 DataType</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DATATYPE</span><span class="o">=</span><span class="n">fp32</span>
</pre></div>
</div>
<ul class="simple">
<li><p>TF32 DataType</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">ITEX_FP32_MATH_MODE</span><span class="o">=</span><span class="n">TF32</span>
<span class="n">DATATYPE</span><span class="o">=</span><span class="n">fp32</span>
</pre></div>
</div>
<p><strong>Final Scripts</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export TF_USE_LEGACY_KERAS=1  # use Keras 2 for tf2.16

NUM_GPUS=1
BATCH_SIZE_PER_GPU=76
LEARNING_RATE_PER_GPU=3e-5
DATATYPE=$DATATYPE
USE_XLA=false
SQUAD_VERSION=1.1
EPOCHS=2
USE_MYTRAIN=true
BERT_MODEL=large
PRETRAIN_PATH=$PRETRAIN_RESULT_DIR/phase_2/pretrained/bert_model.ckpt-1
DATA_DIR=$DATA_DIR
RESULT_DIR=./results/tf_bert_finetune_${BERT_MODEL}_${$DATATYPE}
bash scripts/run_squad.sh \
    $NUM_GPUS \
    $BATCH_SIZE_PER_GPU \
    $LEARNING_RATE_PER_GPU \
    $DATATYPE \
    $USE_XLA \
    $BERT_MODEL \
    $SQUAD_VERSION \
    $EPOCHS \
    $USE_MYTRAIN \
    $PRETRAIN_PATH \
    $DATA_DIR \
    $RESULT_DIR \
    |&amp; tee finetune.log
</pre></div>
</div>
<p><strong>Note</strong>: If you want to run on more cards and even multi-node, you could adjust the parameter <code class="docutils literal notranslate"><span class="pre">NUM_GPUS</span></code>.</p>
<ul class="simple">
<li><p>If you meet Out-Of-Memory error, you can reduce batch_size and use below hyperparameters.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">BATCH_SIZE_PER_GPU</span><span class="o">=</span><span class="mi">12</span>
<span class="n">LEARNING_RATE_PER_GPU</span><span class="o">=</span><span class="mf">5e-6</span>
</pre></div>
</div>
</section>
</section>
<section id="convergence">
<h2>Convergence<a class="headerlink" href="#convergence" title="Link to this heading"></a></h2>
<p>If you want to run Bert-Large pretraining to convergence, we provide a set of bfloat16 distributed training hyperparameters verified on 4 cards(8 tiles) and 12 cards(24 tiles) PVC.</p>
<section id="id2">
<h3>Prerequisites<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<p>We use <code class="docutils literal notranslate"><span class="pre">intel-optimization-for-horovod</span></code> to implement efficient multi-GPU training with OneCCL. If you want to use multi-GPU, please replace <code class="docutils literal notranslate"><span class="pre">horovod</span></code> with <code class="docutils literal notranslate"><span class="pre">intel-optimization-for-horovod</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">uninstall</span> <span class="n">horovod</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">intel</span><span class="o">-</span><span class="n">optimization</span><span class="o">-</span><span class="k">for</span><span class="o">-</span><span class="n">horovod</span>
</pre></div>
</div>
</section>
<section id="commands">
<h3>Commands<a class="headerlink" href="#commands" title="Link to this heading"></a></h3>
<p>Assume current_dir is <code class="docutils literal notranslate"><span class="pre">examples/pretrain_bert/DeepLearningExamples/TensorFlow2/LanguageModeling/BERT</span></code></p>
<ul class="simple">
<li><p>Pretraining</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export TF_USE_LEGACY_KERAS=1  # use Keras 2 for tf2.16

TRAIN_BATCH_SIZE_PHASE1=312
TRAIN_BATCH_SIZE_PHASE2=40
EVAL_BATCH_SIZE=8
LEARNING_RATE_PHASE1=0.004888
LEARNING_RATE_PHASE2=0.004
DATATYPE=bf16
USE_XLA=false
NUM_GPUS=8
WARMUP_STEPS_PHASE1=2000
WARMUP_STEPS_PHASE2=200
TRAIN_STEPS=6416
SAVE_CHECKPOINT_STEPS=100
NUM_ACCUMULATION_STEPS_PHASE1=256
NUM_ACCUMULATION_STEPS_PHASE2=768
BERT_MODEL=large

LEARNING_RATE_PHASE1=$(echo $LEARNING_RATE_PHASE1 $NUM_GPUS | awk &#39;{printf (&quot;%0.7f&quot;, $1/$2)}&#39;)
LEARNING_RATE_PHASE2=$(echo $LEARNING_RATE_PHASE2 $NUM_GPUS | awk &#39;{printf (&quot;%0.7f&quot;, $1/$2)}&#39;)
NUM_ACCUMULATION_STEPS_PHASE1=$(echo $NUM_ACCUMULATION_STEPS_PHASE1 $NUM_GPUS | awk &#39;{print int($1/$2)}&#39;)
NUM_ACCUMULATION_STEPS_PHASE2=$(echo $NUM_ACCUMULATION_STEPS_PHASE2 $NUM_GPUS | awk &#39;{print int($1/$2)}&#39;)
GBS1=$(expr $TRAIN_BATCH_SIZE_PHASE1 \* $NUM_GPUS \* $NUM_ACCUMULATION_STEPS_PHASE1)
GBS2=$(expr $TRAIN_BATCH_SIZE_PHASE2 \* $NUM_GPUS \* $NUM_ACCUMULATION_STEPS_PHASE2)

PRETRAIN_RESULT_DIR=./results/tf_bert_pretraining_lamb_${BERT_MODEL}_${DATATYPE}_gbs1_${GBS1}_gbs2_${GBS2}
DATA_DIR=$DATA_DIR

bash scripts/run_pretraining_lamb.sh \
    $TRAIN_BATCH_SIZE_PHASE1 \
    $TRAIN_BATCH_SIZE_PHASE2 \
    $EVAL_BATCH_SIZE \
    $LEARNING_RATE_PHASE1 \
    $LEARNING_RATE_PHASE2 \
    $DATATYPE \
    $USE_XLA \
    $NUM_GPUS \
    $WARMUP_STEPS_PHASE1 \
    $WARMUP_STEPS_PHASE2 \
    $TRAIN_STEPS \
    $SAVE_CHECKPOINT_STEPS \
    $NUM_ACCUMULATION_STEPS_PHASE1 \
    $NUM_ACCUMULATION_STEPS_PHASE2 \
    $BERT_MODEL \
    $DATA_DIR \
    $PRETRAIN_RESULT_DIR \
    |&amp; tee pretrain_lamb.log
</pre></div>
</div>
<p><strong>Note</strong>: If you want to run on more cards and even multi-node, you could adjust the parameter <code class="docutils literal notranslate"><span class="pre">NUM_GPUS</span></code>.</p>
<ul class="simple">
<li><p>Finetune</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export TF_USE_LEGACY_KERAS=1  # use Keras 2 for tf2.16

NUM_GPUS=8
BATCH_SIZE_PER_GPU=12
LEARNING_RATE_PER_GPU=5e-6
DATATYPE=bf16
USE_XLA=false
SQUAD_VERSION=1.1
EPOCHS=2
USE_MYTRAIN=true
BERT_MODEL=large

PRETRAIN_PATH=$PRETRAIN_RESULT_DIR/phase_2/pretrained/bert_model.ckpt-1
DATA_DIR=$DATA_DIR
RESULT_DIR=./results/tf_bert_finetune_${BERT_MODEL}_${DATATYPE}

bash scripts/run_squad.sh \
    $NUM_GPUS \
    $BATCH_SIZE_PER_GPU \
    $LEARNING_RATE_PER_GPU \
    $DATATYPE \
    $USE_XLA \
    $BERT_MODEL \
    $SQUAD_VERSION \
    $EPOCHS \
    $USE_MYTRAIN \
    $PRETRAIN_PATH \
    $DATA_DIR \
    $RESULT_DIR \
    |&amp; tee finetune.log
</pre></div>
</div>
<p><strong>Note</strong>: If you want to run on more cards and even multi-node, you could adjust the parameter <code class="docutils literal notranslate"><span class="pre">NUM_GPUS</span></code>.</p>
</section>
<section id="results">
<h3>Results<a class="headerlink" href="#results" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Pretraining Phase1</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1015</span> <span class="mi">21</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">32.105446</span> <span class="mi">22471978829632</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">83</span><span class="p">]</span> <span class="n">Training</span> <span class="n">Summary</span><span class="p">:</span> 
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">{</span><span class="s1">&#39;total_training_steps&#39;</span><span class="p">:</span> <span class="mi">5774</span><span class="p">,</span> <span class="s1">&#39;train_loss&#39;</span><span class="p">:</span> <span class="n">xxxxx</span><span class="p">}</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1015</span> <span class="mi">21</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">32.190052</span> <span class="mi">22471978829632</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">595</span><span class="p">]</span> <span class="o">-----------------------------</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1015</span> <span class="mi">21</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">32.190136</span> <span class="mi">22471978829632</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">596</span><span class="p">]</span>   <span class="n">Batch</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">312</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1015</span> <span class="mi">21</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">32.190169</span> <span class="mi">22471978829632</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">597</span><span class="p">]</span>   <span class="n">Num</span> <span class="n">steps</span> <span class="o">=</span> <span class="mi">5774</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1015</span> <span class="mi">21</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">32.190193</span> <span class="mi">22471978829632</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">598</span><span class="p">]</span>   <span class="n">LR</span> <span class="o">=</span> <span class="mf">0.000611</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1015</span> <span class="mi">21</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">32.190217</span> <span class="mi">22471978829632</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">600</span><span class="p">]</span> <span class="n">Multi</span><span class="o">-</span><span class="n">GPU</span> <span class="n">training</span> <span class="k">with</span> <span class="n">TF</span> <span class="n">Horovod</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1015</span> <span class="mi">21</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">32.190249</span> <span class="mi">22471978829632</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">601</span><span class="p">]</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">=</span> <span class="mi">8</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1015</span> <span class="mi">21</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">32.190273</span> <span class="mi">22471978829632</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">602</span><span class="p">]</span> <span class="n">Total</span> <span class="n">Training</span> <span class="n">Time</span> <span class="o">=</span> <span class="n">xxxxx</span> <span class="k">for</span> <span class="n">Sequences</span> <span class="o">=</span> <span class="mi">461180928</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1015</span> <span class="mi">21</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">32.190343</span> <span class="mi">22471978829632</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">604</span><span class="p">]</span> <span class="n">Throughput</span> <span class="n">Average</span> <span class="p">(</span><span class="n">sequences</span><span class="o">/</span><span class="n">sec</span><span class="p">)</span> <span class="k">with</span> <span class="n">overhead</span> <span class="o">=</span> <span class="n">xxxxx</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1015</span> <span class="mi">21</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">32.190536</span> <span class="mi">22471978829632</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">606</span><span class="p">]</span> <span class="n">Throughput</span> <span class="n">Average</span> <span class="p">(</span><span class="n">sequences</span><span class="o">/</span><span class="n">sec</span><span class="p">)</span> <span class="o">=</span> <span class="n">xxxxx</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1015</span> <span class="mi">21</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">32.190559</span> <span class="mi">22471978829632</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">607</span><span class="p">]</span> <span class="o">-----------------------------</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">decayed_learning_rate_at_crossover_point</span> <span class="o">=</span> <span class="mf">4.888000e-03</span><span class="p">,</span> <span class="n">adjusted_init_lr</span> <span class="o">=</span> <span class="mf">4.888000e-03</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">DLL</span> <span class="mi">2023</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">15</span> <span class="mi">21</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">32.190587</span> <span class="o">-</span>  <span class="n">throughput_train</span> <span class="p">:</span> <span class="n">xxxxx</span> <span class="n">sequences</span><span class="o">/</span><span class="n">s</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">DLL</span> <span class="mi">2023</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">15</span> <span class="mi">21</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">32.190668</span> <span class="o">-</span>  <span class="n">total_loss</span> <span class="p">:</span> <span class="n">xxxxx</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Pretraining Phase2</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1021</span> <span class="mi">03</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mf">52.227250</span> <span class="mi">23092487755584</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">83</span><span class="p">]</span> <span class="n">Training</span> <span class="n">Summary</span><span class="p">:</span> 
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">{</span><span class="s1">&#39;total_training_steps&#39;</span><span class="p">:</span> <span class="mi">1666</span><span class="p">,</span> <span class="s1">&#39;train_loss&#39;</span><span class="p">:</span> <span class="n">xxxxx</span><span class="p">}</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1021</span> <span class="mi">03</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mf">52.314010</span> <span class="mi">23092487755584</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">595</span><span class="p">]</span> <span class="o">-----------------------------</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1021</span> <span class="mi">03</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mf">52.314064</span> <span class="mi">23092487755584</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">596</span><span class="p">]</span>   <span class="n">Batch</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">40</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1021</span> <span class="mi">03</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mf">52.314090</span> <span class="mi">23092487755584</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">597</span><span class="p">]</span>   <span class="n">Num</span> <span class="n">steps</span> <span class="o">=</span> <span class="mi">1666</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1021</span> <span class="mi">03</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mf">52.314113</span> <span class="mi">23092487755584</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">598</span><span class="p">]</span>   <span class="n">LR</span> <span class="o">=</span> <span class="mf">0.0005</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1021</span> <span class="mi">03</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mf">52.314135</span> <span class="mi">23092487755584</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">600</span><span class="p">]</span> <span class="n">Multi</span><span class="o">-</span><span class="n">GPU</span> <span class="n">training</span> <span class="k">with</span> <span class="n">TF</span> <span class="n">Horovod</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1021</span> <span class="mi">03</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mf">52.314165</span> <span class="mi">23092487755584</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">601</span><span class="p">]</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">=</span> <span class="mi">8</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1021</span> <span class="mi">03</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mf">52.314197</span> <span class="mi">23092487755584</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">602</span><span class="p">]</span> <span class="n">Total</span> <span class="n">Training</span> <span class="n">Time</span> <span class="o">=</span> <span class="n">xxxxx</span> <span class="k">for</span> <span class="n">Sequences</span> <span class="o">=</span> <span class="mi">51179520</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1021</span> <span class="mi">03</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mf">52.314260</span> <span class="mi">23092487755584</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">604</span><span class="p">]</span> <span class="n">Throughput</span> <span class="n">Average</span> <span class="p">(</span><span class="n">sequences</span><span class="o">/</span><span class="n">sec</span><span class="p">)</span> <span class="k">with</span> <span class="n">overhead</span> <span class="o">=</span> <span class="n">xxxxx</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1021</span> <span class="mi">03</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mf">52.314455</span> <span class="mi">23092487755584</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">606</span><span class="p">]</span> <span class="n">Throughput</span> <span class="n">Average</span> <span class="p">(</span><span class="n">sequences</span><span class="o">/</span><span class="n">sec</span><span class="p">)</span> <span class="o">=</span> <span class="n">xxxxx</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1021</span> <span class="mi">03</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mf">52.314478</span> <span class="mi">23092487755584</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">607</span><span class="p">]</span> <span class="o">-----------------------------</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">decayed_learning_rate_at_crossover_point</span> <span class="o">=</span> <span class="mf">4.000000e-03</span><span class="p">,</span> <span class="n">adjusted_init_lr</span> <span class="o">=</span> <span class="mf">4.000000e-03</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">DLL</span> <span class="mi">2023</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">21</span> <span class="mi">03</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mf">52.314505</span> <span class="o">-</span>  <span class="n">throughput_train</span> <span class="p">:</span> <span class="n">xxxxx</span> <span class="n">sequences</span><span class="o">/</span><span class="n">s</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">DLL</span> <span class="mi">2023</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">21</span> <span class="mi">03</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mf">52.314589</span> <span class="o">-</span>  <span class="n">total_loss</span> <span class="p">:</span> <span class="n">xxxxx</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Finetune</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1022</span> <span class="mi">18</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">51.037595</span> <span class="mi">23450678511424</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">83</span><span class="p">]</span> <span class="n">Training</span> <span class="n">Summary</span><span class="p">:</span> 
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">{</span><span class="s1">&#39;total_training_steps&#39;</span><span class="p">:</span> <span class="mi">1846</span><span class="p">,</span> <span class="s1">&#39;train_loss&#39;</span><span class="p">:</span> <span class="n">xxxxx</span><span class="p">}</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1022</span> <span class="mi">18</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">51.072701</span> <span class="mi">23450678511424</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">595</span><span class="p">]</span> <span class="o">-----------------------------</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1022</span> <span class="mi">18</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">51.072752</span> <span class="mi">23450678511424</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">596</span><span class="p">]</span>   <span class="n">Batch</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">12</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1022</span> <span class="mi">18</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">51.072779</span> <span class="mi">23450678511424</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">597</span><span class="p">]</span>   <span class="n">Num</span> <span class="n">steps</span> <span class="o">=</span> <span class="mi">1846</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1022</span> <span class="mi">18</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">51.072805</span> <span class="mi">23450678511424</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">598</span><span class="p">]</span>   <span class="n">LR</span> <span class="o">=</span> <span class="mf">5e-06</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1022</span> <span class="mi">18</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">51.072829</span> <span class="mi">23450678511424</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">600</span><span class="p">]</span> <span class="n">Multi</span><span class="o">-</span><span class="n">GPU</span> <span class="n">training</span> <span class="k">with</span> <span class="n">TF</span> <span class="n">Horovod</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1022</span> <span class="mi">18</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">51.072861</span> <span class="mi">23450678511424</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">601</span><span class="p">]</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">=</span> <span class="mi">8</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1022</span> <span class="mi">18</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">51.072884</span> <span class="mi">23450678511424</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">602</span><span class="p">]</span> <span class="n">Total</span> <span class="n">Training</span> <span class="n">Time</span> <span class="o">=</span> <span class="n">xxxxx</span> <span class="k">for</span> <span class="n">Sequences</span> <span class="o">=</span> <span class="mi">177216</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1022</span> <span class="mi">18</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">51.072928</span> <span class="mi">23450678511424</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">604</span><span class="p">]</span> <span class="n">Throughput</span> <span class="n">Average</span> <span class="p">(</span><span class="n">sequences</span><span class="o">/</span><span class="n">sec</span><span class="p">)</span> <span class="k">with</span> <span class="n">overhead</span> <span class="o">=</span> <span class="n">xxxxx</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1022</span> <span class="mi">18</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">51.073072</span> <span class="mi">23450678511424</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">606</span><span class="p">]</span> <span class="n">Throughput</span> <span class="n">Average</span> <span class="p">(</span><span class="n">sequences</span><span class="o">/</span><span class="n">sec</span><span class="p">)</span> <span class="o">=</span> <span class="n">xxxxx</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">I1022</span> <span class="mi">18</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">51.073094</span> <span class="mi">23450678511424</span> <span class="n">model_training_utils</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">607</span><span class="p">]</span> <span class="o">-----------------------------</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Inference after finetune</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">DLL</span> <span class="mi">2023</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">22</span> <span class="mi">18</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">51.073121</span> <span class="o">-</span>  <span class="n">throughput_train</span> <span class="p">:</span> <span class="n">xxxxx</span> <span class="n">sequences</span><span class="o">/</span><span class="n">s</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">DLL</span> <span class="mi">2023</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">22</span> <span class="mi">18</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mf">51.073183</span> <span class="o">-</span>  <span class="n">total_loss</span> <span class="p">:</span> <span class="n">xxxxx</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">DLL</span> <span class="mi">2023</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">22</span> <span class="mi">18</span><span class="p">:</span><span class="mi">30</span><span class="p">:</span><span class="mf">21.122414</span> <span class="o">-</span>  <span class="n">f1</span> <span class="p">:</span> <span class="n">xxxxx</span> <span class="kc">None</span>
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">DLL</span> <span class="mi">2023</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">22</span> <span class="mi">18</span><span class="p">:</span><span class="mi">30</span><span class="p">:</span><span class="mf">21.122509</span> <span class="o">-</span>  <span class="n">exact_match</span> <span class="p">:</span> <span class="n">xxxxx</span> 
<span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="sa">b</span><span class="s1">&#39;{&quot;exact_match&quot;: xxxxx, &quot;f1&quot;: xxxxx}</span><span class="se">\n</span><span class="s1">&#39;</span>
</pre></div>
</div>
</section>
</section>
<section id="faq">
<h2>FAQ<a class="headerlink" href="#faq" title="Link to this heading"></a></h2>
<ol class="simple">
<li><p>If you get the following error log, refer to <a class="reference external" href="#Enable-Running-Environment">Enable Running Environment</a> to Enable oneAPI running environment.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">framework</span><span class="o">.</span><span class="n">errors_impl</span><span class="o">.</span><span class="n">NotFoundError</span><span class="p">:</span> <span class="n">libmkl_sycl</span><span class="o">.</span><span class="n">so</span><span class="mf">.2</span><span class="p">:</span> <span class="n">cannot</span> <span class="nb">open</span> <span class="n">shared</span> <span class="nb">object</span> <span class="n">file</span><span class="p">:</span> <span class="n">No</span> <span class="n">such</span> <span class="n">file</span> <span class="ow">or</span> <span class="n">directory</span>
</pre></div>
</div>
<ol class="simple">
<li><p>If you get the following error log, please uninstall current horovod using <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">uninstall</span> <span class="pre">horovod</span></code>. And reinstall horovod using <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">--no-cache-dir</span> <span class="pre">horovod</span></code>.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>horovod.common.exceptions.HorovodVersionMismatchError: Framework tensorflow installed with version 2.15.0 but found version 2.14.1.
             This can result in unexpected behavior including runtime errors.
             Reinstall Horovod using `pip install --no-cache-dir` to build with the new version.
</pre></div>
</div>
<ol class="simple">
<li><p>If you get Out-Of-Memory error log using above pretraining scripts, you can reduce batch_size to avoid it.</p></li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022-2024 Intel Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f4148488610> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>